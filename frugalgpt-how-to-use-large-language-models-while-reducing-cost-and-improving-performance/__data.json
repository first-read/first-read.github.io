{"type":"data","nodes":[null,{"type":"data","data":[{"content":1},"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>LLM의 활용도가 높아져감에 따라 LLM API 서비스들이 많이 생겨나고 있다.\n이런 LLM API 들은 가격이나 성능이 다양한데, 가장 성능이 높은 LLM API만\n사용하면 그 비용이 많이 들게 되고 가격이 저렴한 API를 사용하면 성능이\n떨어지게 된다.\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-chen2023frugalgpt\">[1]\u003C/a>에선 성능을\n떨어뜨리지 않으면서도 가격은 저렴하도록 LLM API를 사용하는 방법을\n제시한다.\u003C/p>\n\u003Ch2 id=\"scope-and-problem-statement\">Scope and Problem Statement\u003C/h2>\n\u003Ch3 id=\"natural-language-query-answering\">Natural language query\nanswering\u003C/h3>\n\u003Cp>이 논문은 natural language query answering task에 집중하여 논의를\n진행하였다. 이 task의 목표는 natural language query distribution \u003Cspan class=\"math inline\">\\(\\mathcal{Q}\\)\u003C/span>에서 샘플링한 query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>에 대한 답을 내는것이다. News\nclassification, reading comprehension, commonsense reasoning 등 다양한\n문제들이 QA 문제로 표현될 수 있기 때문에 다양한 상황에 적용될 수\n있다.\u003C/p>\n\u003Ch3 id=\"llm-marketplace\">LLM marketplace\u003C/h3>\n\u003Cp>이 논문에선 LLM API를 함수 \u003Cspan class=\"math inline\">\\(f_i(\\cdot):\n\\mathcal{P}\\mapsto \\mathcal{A}\\)\u003C/span>로 표현하고, \u003Cspan class=\"math inline\">\\(K\\)\u003C/span>개의 서로 다른 LLM API들의 집합 \u003Cspan class=\"math inline\">\\(\\{f_i(\\cdot)\\}_{i=1}^{K}\\)\u003C/span>을 LLM\nmarket이라한다. \u003Cspan class=\"math inline\">\\(\\mathcal{P}\\)\u003C/span>는\nprompt space이고, \u003Cspan class=\"math inline\">\\(\\mathcal{A}\\)\u003C/span>는\nanswer distribution이다. LLM API를 쓰기 위해선 먼저 query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>를 prompt \u003Cspan class=\"math inline\">\\(p\\)\u003C/span>로 변환해야한다는 점을 주의하자. 이\nprompt \u003Cspan class=\"math inline\">\\(p\\)\u003C/span>는 API 함수를 거쳐 answer\n\u003Cspan class=\"math inline\">\\(a\\)\u003C/span>를 생성한다.\u003C/p>\n\u003Cp>LLM API의 cost(비용)는 보통 다음 세가지로 구성돼있다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>prompt의 길이에 비례하는 비용\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>생성된 answer의 길이에 비례하는 비용\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>query당 고정 비용\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>따라서 cost는 다음과 같이 정의된다. \u003Cspan>\u003Cspan class=\"math display\">\\[c_i(p) \\triangleq \\tilde{c}_{i,2}\\|f_i(p)\\|+\n\\tilde{c}_{i,1} \\|p\\| + \\tilde{c}_{i,0}\\qquad{(1)}\\]\u003C/span>\u003C/span> 이때,\n\u003Cspan class=\"math inline\">\\(\\tilde{c}_{i,j},j=0,1,2\\)\u003C/span>는\nconstant이다.\u003C/p>\n\u003Ch3 id=\"problem-statement-budget-aware-llm-api-usage\">Problem statement:\nbudget-aware LLM API usage\u003C/h3>\n\u003Cp>이 논문의 목표는 \u003Cem>적절한 방법을 이용해 제한된 budget에서 최대\n성능을 내도록 LLM API들을 쓰는것\u003C/em>이다. Formal하게는 다음과 같다.\n\u003Cspan>\u003Cspan class=\"math display\">\\[\\max_{s\\in \\mathcal{S}}\n\\mathbb{E}_{(q,a)\\in\\mathcal{Q}\\times \\mathcal{A}}[r(a,\\hat{a}(s,q))]\n  \\,\\,\\text{\\textbf{s.t.}}\\,\\,\n  \\mathbb{E}_{(q,a)\\in\\mathcal{Q}\\times \\mathcal{A}}[c(s,q)]\\leq\nb\\qquad{(2)}\\]\u003C/span>\u003C/span> 이때, \u003Cspan class=\"math inline\">\\(s\\)\u003C/span>는 strategy(API를 쓰는 방법), \u003Cspan class=\"math inline\">\\(a\\)\u003C/span>는 query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>에 대한 correct answer, \u003Cspan class=\"math inline\">\\(\\hat{a}(s,q)\\)\u003C/span>는 strategy \u003Cspan class=\"math inline\">\\(s\\)\u003C/span>를 사용하여 생성된 answer, \u003Cspan class=\"math inline\">\\(c(s,q)\\)\u003C/span>는 strategy \u003Cspan class=\"math inline\">\\(s\\)\u003C/span>를 사용하여 query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>를 처리하는데 드는 cost이며, \u003Cspan class=\"math inline\">\\(b\\)\u003C/span>는 budget이다. 함수 \u003Cspan class=\"math inline\">\\(r(\\cdot,\\cdot)\\)\u003C/span>은 generated answer와\ncorrect answer의 유사도를 측정하는 reward function이다.\u003C/p>\n\u003Ch2 id=\"how-to-use-llms-affordably-and-accurately\">How to Use LLMs\nAffordably and Accurately\u003C/h2>\n\u003Cfigure id=\"fig:Examples\">\n\u003Cimg src=\"figures/frugalgpt-how-to-use-large-language-models-while-reducing-cost-and-improving-performance/methodexample.jpg\" style=\"width:100.0%\"/>\n\u003Cfigcaption>Figure 1: Illustrations of cost-saving strategies. (a)\nPrompt selection uses a subset of in-context examples as the prompt to\nreduce the size of the prompt. (b) Query concatenation aggregates\nmultiple queries to share prompts. (c) Completion cache stores and\nreuses an LLM API’s response when a similar query is asked. (d) Model\nfine-tuning uses expensive LLMs’ responses to fine-tune cheap LLMs. (e)\nLLM cascade employs different LLM APIs for different queries. (Source:\nFig. 2 in \u003Ca href=\"#ref-chen2023frugalgpt\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>\u003Ca href=\"#ref-chen2023frugalgpt\">[1]\u003C/a>에선 크게\n세가지 방법을 제시한다.\u003C/p>\n\u003Ch3 id=\"prompt-adaptation\">Prompt adaptation\u003C/h3>\n\u003Cp>Prompt adaptation은 prompt 길이에 따라 비용이 증가하니 prompt 길이를\n줄여 비용을 아끼는 방법이다. 본 논문에선 prompt adaptation을 위해 두가지\n방법을 제시한다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cem>Prompt selection\u003C/em>: prompt를 짧게 만들어서 비용을 줄이는\n방법이다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(a)에서 보듯이 모든 예제를 prompt로\n사용하는것이 아니라 일부 subset만 사용하는 등의 방법으로 비용을 줄인다.\n어떤 subset을 선택하는지가 관건이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cem>Query concatenation\u003C/em>: 여러 query를 하나의 prompt로\n사용하는 방법이다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(b)에서\n보듯이 같은 prompt를 써서 여러 질문을 하면 prompt 중복에 따라 비용이\n들게되니 prompt를 공유하는 query들을 묶어서 하나의 query로 만들어서\n사용한다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"llm-approximation\">LLM approximation\u003C/h3>\n\u003Cp>LLM approximation은 LLM의 response를 적절한 비용의 다른 방법을 써서\n근사하는 방법이다. 본 논문에선 다음 두가지 방법을 제시한다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cem>Completion cache\u003C/em>: LLM의 response를 저장해두고, 비슷한\nquery가 들어오면 저장된 response를 사용하는 방법이다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(c)에 이를 적용한 예시가\n나와있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cem>Model fine-tuning(Distillation)\u003C/em>: 비싼 LLM의 response를\n사용하여 저렴한 LLM을 fine-tuning하는 방법이다. 이를 통해 저렴한\n비용으로 비슷한 성능을 낼 수 있다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(d)에 이를\n적용한 예시가 나와있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"llm-cascade\">LLM cascade\u003C/h3>\n\u003Cp>LLM들이 각각 다른 성능과 가격을 가지고 있으니, 이를 적절히 조합하여\n사용하는 방법이다. 이 논문에서 제시된 방법은 LLM을 성능순대로\nsequential하게 배치하고, 응답이 적절했으면 그대로 사용하고 그렇지 않다면\n다음 LLM으로 진행해나가는 방법이다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(e)에 이를\n적용한 예시가 나와있다. Cascade의 핵심요소는 두가지이다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cem>Generation scoring function\u003C/em>: Query와 \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>와 LLM의 답 \u003Cspan class=\"math inline\">\\(a\\)\u003C/span>로부터 얼마나 reliable한지 측정하는\n함수이다. Formal하게 \u003Cspan class=\"math inline\">\\(g(\\cdot,\\cdot):\\mathcal{Q} \\times\n\\mathcal{A}\\mapsto [0,1]\\)\u003C/span>로 표현된다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cem>LLM router\u003C/em>: LLM router는 LLM market에서 \u003Cspan class=\"math inline\">\\(m\\)\u003C/span>개의 LLM API를 선택한다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>Cascade의 동작은 구체적으로 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cspan class=\"math inline\">\\(m\\)\u003C/span>개의 LLM API를 선택한다.\n선택된 \u003Cspan class=\"math inline\">\\(m\\)\u003C/span>개의 LLM의 인덱스를 \u003Cspan class=\"math inline\">\\(\\pmb L \\in [K]^m\\)\u003C/span>로 표현하자.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>가 들어오면, \u003Cspan class=\"math inline\">\\(i\\)\u003C/span>번째 API를 선택하여 query에 대한 답\n\u003Cspan class=\"math inline\">\\(f_{L_i}(q)\\)\u003C/span>을 생성한다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cspan class=\"math inline\">\\(g(q,f_{L_i}(q))\\)\u003C/span>를 계산하여\n\u003Cspan class=\"math inline\">\\(\\pmb \\tau_i\\)\u003C/span>보다 높으면 그대로\n사용하고, 그렇지 않으면 다음 API를 선택한다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>Cascade는 optimization problem으로 표현될 수 있는데, 이 optimization\nproblem을 그대로 풀기엔 계산량이 너무 많으므로 small answer\ndisagreement를 가지는 LLM들은 무시하고, objective는 작은 sample을 돌려본\n결과로 근사하여 풀어낸다.\u003C/p>\n\u003Ch3 id=\"compositions\">Compositions\u003C/h3>\n\u003Cp>위의 방법들을 조합하여 사용하는것도 cost reduction에 도움이 된다.\u003C/p>\n\u003Ch2 id=\"llm-cascade-reduces-cost-and-improves-accuracy\">LLM Cascade\nReduces Cost and Improves Accuracy\u003C/h2>\n\u003Cfigure id=\"fig:Tradeoffs\">\n\u003Cimg src=\"figures/frugalgpt-how-to-use-large-language-models-while-reducing-cost-and-improving-performance/tradeoffs_full.jpg\" style=\"width:90.0%\"/>\n\u003Cfigcaption>Figure 2: Accuracy and cost tradeoffs achieved by FrugalGPT.\nOverall, FrugalGPT often achieves the same performance of the best\nindividual LLM API (e.g., GPT-4) with orders of magnitudes smaller cost.\nWhen incurring the same cost, FrugalGPT can improves the accuracy by up\nto 5%. Examples of LLM cascade for each dataset are shown on the right.\n(Source: Fig. 5 in \u003Ca href=\"#ref-chen2023frugalgpt\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>저자들이 LLM cascade의 방법으로 선보인 FrugalGPT의 주요 실험 결과는\nFig. \u003Ca data-reference=\"fig:Tradeoffs\" data-reference-type=\"ref\" href=\"#fig:Tradeoffs\">2\u003C/a>에 나와있다.\u003C/p>\n\u003Cp>실험에서 싼 LLM이라도 비싼 LLM보다 성능이 높을 수 있음을 알 수 있다.\n저자들은 이를 각 dataset에 대해 측정해 얼마나 performance를 더 높일 수\n있는지(상한)를 계산하고 이를 \u003Cem>MPI\u003C/em>(maxium performance\nimprovement)라고 정의하였다.\u003C/p>\n\u003Cp>또 Fig. \u003Ca data-reference=\"fig:Tradeoffs\" data-reference-type=\"ref\" href=\"#fig:Tradeoffs\">2\u003C/a>에서 LLM API의 cost 순위는 dataset에\n따라 가변적임을 주목하자. 이는 LLM API가 각각 다른 pricing 정책을 가지고\n있기 때문으로 보인다.\u003C/p>\n\u003Ch2 id=\"limitations\">Limitations\u003C/h2>\n\u003Cp>저자들이 밝힌 한계점은 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>LLM cascade를 학습하기 위해 labeled example이 필요하다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>학습의 training example은 test example과 같은 distribution에서\n나와야한다. 즉, 실제로 마주할 문제의 distribution과 비슷한\ndistribution에서 학습을 해야한다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM cascade를 학습하는데에도 비용이 든다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>LLM API를 적절히 사용하면 성능을 높이면서도 비용을 줄일 수\n있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>그 방법으로는 prompt adaptation, LLM approximation, LLM cascade가\n있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM cascade는 LLM API를 성능순대로 sequential하게 배치하고,\n응답이 적절했으면 그대로 사용하고 그렇지 않다면 다음 LLM으로\n진행해나가는 방법이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM cascade는 비용을 크게 줄이면서도 성능은 최고 성능의 단일\nAPI와 비슷하거나 더 높도록 할 수 있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-chen2023frugalgpt\" role=\"listitem\">[1] \nChen, Lingjiao, Matei Zaharia, and James Zou. 2023. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2305.05176\">FrugalGPT: How to Use Large\nLanguage Models While Reducing Cost and Improving\nPerformance\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"],"uses":{}}]}
