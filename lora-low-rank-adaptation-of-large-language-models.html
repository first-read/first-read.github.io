<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
	<link rel="manifest" href="./favicon/site.webmanifest">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<script>
		MathJax = {
			loader: { load: ['[tex]/textmacros'] },
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				packages: { '[+]': ['textmacros'] },
			},
		};
	</script>
	<script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js">
		</script>
	
		<link href="./_app/immutable/assets/0.08c9bd5d.css" rel="stylesheet">
		<link href="./_app/immutable/assets/3.15287e86.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.c64402c8.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.e108d1fd.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/singletons.6898e7bd.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.8c09aa58.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.38a092e6.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.cefdd44a.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/8.175d8244.js"><title>First Read - LoRA: Low-Rank Adaptation of Large Language Models</title><!-- HEAD_svelte-v2f3dh_START --><!-- HEAD_svelte-v2f3dh_END -->
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">  <div class="nav svelte-5baz1e" data-svelte-h="svelte-89nan6"><a href="/">Posts</a></div>  <div class="content svelte-lwo4ch"><div class="title svelte-lwo4ch" data-svelte-h="svelte-uzdok0"><h1 class="svelte-lwo4ch">First Read</h1> <h2 class="svelte-lwo4ch">LoRA: Low-Rank Adaptation of Large Language Models</h2> <div class="date svelte-lwo4ch">2023-08-25</div></div> <!-- HTML_TAG_START --><html><body><h2 id="overview">Overview</h2>
<p>현재 언어모델의 패러다임은 pre-trained language model을 task에 맞춰
fine-tuning하여 사용하는 것이다. 각 task에 맞춰 처음부터 모델을
학습시키는 것보다 훨씬 적은 비용으로 높은 성능을 얻을 수 있기 때문이다.
그러나 <a href="#ref-hu2021lora">[1]</a> 이전까지의 fine-tuning
방법들은 비용이 많이 들거나, inference latency가 증가하거나,
baseline보다 성능이 떨어지기도 한다던지의 문제가 있었다.</p>
<p><a href="#ref-hu2021lora">[1]</a>은
이런 문제를 해결하는 새로운 fine-tuning 방법(LoRA)을 제시한
논문이다.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>LORA는 언어모델뿐 아니라 어떤 모델에도 적용이 가능하지만 <a href="#ref-hu2021lora">[1]</a>에서는
간단히 언어모델에 국한하였다.</p>
<p><span class="math inline">\(\Phi\)</span>로 parameterized된
pre-trained autoregressive language model <span class="math inline">\(P_\Phi(y|x)\)</span>가 주어졌다고 하자. Task를
training dataset <span class="math inline">\(\mathbf{Z} =  \{(x_i,
y_i)\}_{i=1,..,N}\)</span> (<span class="math inline">\(x_i\)</span>와
<span class="math inline">\(y_i\)</span>는 토큰 sequence)로 표현한다면
full fine-tuning은 다음과 같이 나타낼 수 있다.</p>
<p><span><span class="math display">\[\max_{\Phi}
\sum_{(x,y)\in\mathbf{Z}} \sum_{t=1}^{|y|}  \text{log}
\left(  P_{\Phi}(y_{t} | x, y_{&lt;t})
\right)\qquad{(1)}\]</span></span></p>
<p>LoRA는 parameter-efficient한 방법으로 <span class="math inline">\(|\Theta| \ll |\Phi_0|\)</span> 인 파라미터 <span class="math inline">\(\Theta\)</span>만 신경쓰면 된다. 즉, <span class="math inline">\(\Phi_0\)</span>을 pre-trained weight이라 할 때
다음과 같이 나타낼 수 있다.</p>
<p><span><span class="math display">\[\begin{aligned}
  \max_{\Theta}
\sum_{(x,y)\in\mathbf{Z}}  \sum_{t=1}^{|y|}  \log\left({p_{\Phi_0+\Delta\Phi(\Theta)}(y_{t}
| x, y_{&lt;t}})\right)
\end{aligned}\qquad{(2)}\]</span></span></p>
<h2 id="existing-solutions">Existing Solutions</h2>
<p>기존의 방식들은 다음과 같은 단점이 있다.</p>
<ol>
<li><p><strong>Adapter</strong></p>
<ol>
<li><p>Adapter layer의 extra computation을 bypass할 방법이 없다. 즉,
inference latency가 증가한다.</p></li>
</ol></li>
<li><p><strong>Prefix tuning</strong></p>
<ol>
<li><p>최적화가 어렵다.</p></li>
<li><p>Sequence length의 희생이 따른다.</p></li>
</ol></li>
</ol>
<h2 id="lora">LoRA</h2>
<h3 id="motivation">Motivation</h3>
<p>LoRA는 weight의 update가 low "intrinsic rank"를 가지고 있다는 가정을
기반으로 한다.</p>
<p>Pre-trained weight matrix <span class="math inline">\(W_0\in
\mathbb{R}^{d\times k}\)</span>에 대해 그 update를 low-rank
decomposition을 이용해 나타내면 다음과 같다. <span><span class="math display">\[W_0+\Delta W=W_0+BA \left(B\in
\mathbb{R}^{d\times r}, A\in \mathbb{R}^{r\times
k}\right)\qquad{(3)}\]</span></span> 이때 rank <span class="math inline">\(r \ll \min(d,k)\)</span>이다. 학습동안 <span class="math inline">\(W_0\)</span>는 frozen되어 있고, <span class="math inline">\(A\)</span>와 <span class="math inline">\(B\)</span>만 학습된다. 이에 맞춰 바뀐 forward
pass는 다음과 같다.</p>
<p><span><span class="math display">\[h = W_0 x + \Delta W x = W_0 x +
BA x\qquad{(4)}\]</span></span></p>
<p>Fig. <a data-reference="fig:figure1" data-reference-type="ref" href="#fig:figure1">1</a>이 이를 나타내고 있다. <a href="#ref-hu2021lora">[1]</a>에선 A를
random Gaussian initialization로, B는 0으로 초기화시켰다. <span class="math inline">\(\Delta W x\)</span>는 상수 <span class="math inline">\(\alpha\)</span>에 대해 <span class="math inline">\(\frac{\alpha}{r}\)</span>로 scale하였다.</p>
<figure id="fig:figure1">
<img src="figures/lora-low-rank-adaptation-of-large-language-models/figure1.jpg" style="width:40.0%"/>
<figcaption>Figure 1: Reparametrization of LoRA. (Image source: Fig. 1
in <a href="#ref-hu2021lora">[1]</a>)</figcaption>
</figure>
<p>LoRA는 다음과 같은 장점을 가진다.</p>
<ol>
<li><p><strong>Generalization of Full Fine-tuning</strong> LoRA rank
<span class="math inline">\(r\)</span>의 설정에 따라 full fine-tuning에
근접할 수도 있다(다양한 fine-tuning이 가능하다).</p></li>
<li><p><strong>No Additional Inference Latency</strong> 모델의 배포때
더해진 weight, 즉, <span class="math inline">\(W = W_0 + BA\)</span>로
사용하면 추가 latency가 없다. 원래의 weight으로 돌리고 싶을땐 단순히
다시 빼주면 된다.</p></li>
<li><p><strong>Modularity</strong> LoRA는 각 downstream task에 맞춰 만들
수 있다. 즉, 각각의 task에 맞춰 모듈식으로 갈아끼워가며 사용할 수
있다.</p></li>
</ol>
<p>단점으로는 더해진 형태로 weight을 쓰게 되면 동시에 여러 LoRA를 사용할
수 없다는 점 등이 있다.</p>
<h2 id="empirical-experiments">Empirical Experiments</h2>
<figure id="fig:lora_wikisql">
<img src="figures/lora-low-rank-adaptation-of-large-language-models/LoRA_wikisql.jpg" style="width:100.0%"/>
<figcaption>Figure 2: GPT-3 175B validation accuracy vs. number of
trainable parameters of several adaptation methods on WikiSQL and
MNLI-matched. LoRA exhibits better scalability and task performance.
(Image source: Fig. 2 in <a href="#ref-hu2021lora">[1]</a>)</figcaption>
</figure>
<p>성능은 대부분의 결과에서 full fine-tuning에 근접하거나 더 뛰어나며
다른 방법들을 앞지르는 모습을 보여준다. Fig. <a data-reference="fig:lora_wikisql" data-reference-type="ref" href="#fig:lora_wikisql">2</a>에
GPT-3 175B에 대한 대략적인 실험결과가 나타나 있다. 조심해야할것은 Fig.
<a data-reference="fig:lora_wikisql" data-reference-type="ref" href="#fig:lora_wikisql">2</a>에 나와있듯이 trainable
parameter를 증가시키는 것이 꼭 성능을 향상시키는 것은 아니라는
점이다.</p>
<h2 id="quick-recap">Quick Recap</h2>
<ol>
<li><p><span class="math inline">\(W_0+\Delta W=W_0+BA\)</span>로
fine-tuning을 할 수 있다.</p></li>
<li><p>Inference latency가 없게 만들 수 있고, 모듈식 사용이
가능하다.</p></li>
<li><p>Memory와 Storage를 절약할 수 있다.</p></li>
<li><p>성능은 대개 full fine-tuning에 비견될만큼 뛰어나다.</p></li>
</ol>
<div class="references csl-bib-body hanging-indent" id="refs" role="list" style="margin-bottom: 2rem"><h2 style="margin-top: 4rem">References</h2>
<div class="csl-entry" id="ref-hu2021lora" role="listitem">[1] 
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“<a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of
Large Language Models</a>.”</span>
</div>
</div>
</body></html><!-- HTML_TAG_END --> </div> 
			
			<script>
				{
					__sveltekit_1a01rcl = {
						base: new URL(".", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{content:"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>현재 언어모델의 패러다임은 pre-trained language model을 task에 맞춰\nfine-tuning하여 사용하는 것이다. 각 task에 맞춰 처음부터 모델을\n학습시키는 것보다 훨씬 적은 비용으로 높은 성능을 얻을 수 있기 때문이다.\n그러나 \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a> 이전까지의 fine-tuning\n방법들은 비용이 많이 들거나, inference latency가 증가하거나,\nbaseline보다 성능이 떨어지기도 한다던지의 문제가 있었다.\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>은\n이런 문제를 해결하는 새로운 fine-tuning 방법(LoRA)을 제시한\n논문이다.\u003C/p>\n\u003Ch2 id=\"problem-statement\">Problem Statement\u003C/h2>\n\u003Cp>LORA는 언어모델뿐 아니라 어떤 모델에도 적용이 가능하지만 \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>에서는\n간단히 언어모델에 국한하였다.\u003C/p>\n\u003Cp>\u003Cspan class=\"math inline\">\\(\\Phi\\)\u003C/span>로 parameterized된\npre-trained autoregressive language model \u003Cspan class=\"math inline\">\\(P_\\Phi(y|x)\\)\u003C/span>가 주어졌다고 하자. Task를\ntraining dataset \u003Cspan class=\"math inline\">\\(\\mathbf{Z} =  \\{(x_i,\ny_i)\\}_{i=1,..,N}\\)\u003C/span> (\u003Cspan class=\"math inline\">\\(x_i\\)\u003C/span>와\n\u003Cspan class=\"math inline\">\\(y_i\\)\u003C/span>는 토큰 sequence)로 표현한다면\nfull fine-tuning은 다음과 같이 나타낼 수 있다.\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[\\max_{\\Phi}\n\\sum_{(x,y)\\in\\mathbf{Z}} \\sum_{t=1}^{|y|}  \\text{log}\n\\left(  P_{\\Phi}(y_{t} | x, y_{&lt;t})\n\\right)\\qquad{(1)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>LoRA는 parameter-efficient한 방법으로 \u003Cspan class=\"math inline\">\\(|\\Theta| \\ll |\\Phi_0|\\)\u003C/span> 인 파라미터 \u003Cspan class=\"math inline\">\\(\\Theta\\)\u003C/span>만 신경쓰면 된다. 즉, \u003Cspan class=\"math inline\">\\(\\Phi_0\\)\u003C/span>을 pre-trained weight이라 할 때\n다음과 같이 나타낼 수 있다.\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[\\begin{aligned}\n  \\max_{\\Theta}\n\\sum_{(x,y)\\in\\mathbf{Z}}  \\sum_{t=1}^{|y|}  \\log\\left({p_{\\Phi_0+\\Delta\\Phi(\\Theta)}(y_{t}\n| x, y_{&lt;t}})\\right)\n\\end{aligned}\\qquad{(2)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Ch2 id=\"existing-solutions\">Existing Solutions\u003C/h2>\n\u003Cp>기존의 방식들은 다음과 같은 단점이 있다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Adapter\u003C/strong>\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>Adapter layer의 extra computation을 bypass할 방법이 없다. 즉,\ninference latency가 증가한다.\u003C/p>\u003C/li>\n\u003C/ol>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Prefix tuning\u003C/strong>\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>최적화가 어렵다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Sequence length의 희생이 따른다.\u003C/p>\u003C/li>\n\u003C/ol>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"lora\">LoRA\u003C/h2>\n\u003Ch3 id=\"motivation\">Motivation\u003C/h3>\n\u003Cp>LoRA는 weight의 update가 low \"intrinsic rank\"를 가지고 있다는 가정을\n기반으로 한다.\u003C/p>\n\u003Cp>Pre-trained weight matrix \u003Cspan class=\"math inline\">\\(W_0\\in\n\\mathbb{R}^{d\\times k}\\)\u003C/span>에 대해 그 update를 low-rank\ndecomposition을 이용해 나타내면 다음과 같다. \u003Cspan>\u003Cspan class=\"math display\">\\[W_0+\\Delta W=W_0+BA \\left(B\\in\n\\mathbb{R}^{d\\times r}, A\\in \\mathbb{R}^{r\\times\nk}\\right)\\qquad{(3)}\\]\u003C/span>\u003C/span> 이때 rank \u003Cspan class=\"math inline\">\\(r \\ll \\min(d,k)\\)\u003C/span>이다. 학습동안 \u003Cspan class=\"math inline\">\\(W_0\\)\u003C/span>는 frozen되어 있고, \u003Cspan class=\"math inline\">\\(A\\)\u003C/span>와 \u003Cspan class=\"math inline\">\\(B\\)\u003C/span>만 학습된다. 이에 맞춰 바뀐 forward\npass는 다음과 같다.\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[h = W_0 x + \\Delta W x = W_0 x +\nBA x\\qquad{(4)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>Fig. \u003Ca data-reference=\"fig:figure1\" data-reference-type=\"ref\" href=\"#fig:figure1\">1\u003C/a>이 이를 나타내고 있다. \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>에선 A를\nrandom Gaussian initialization로, B는 0으로 초기화시켰다. \u003Cspan class=\"math inline\">\\(\\Delta W x\\)\u003C/span>는 상수 \u003Cspan class=\"math inline\">\\(\\alpha\\)\u003C/span>에 대해 \u003Cspan class=\"math inline\">\\(\\frac{\\alpha}{r}\\)\u003C/span>로 scale하였다.\u003C/p>\n\u003Cfigure id=\"fig:figure1\">\n\u003Cimg src=\"figures/lora-low-rank-adaptation-of-large-language-models/figure1.jpg\" style=\"width:40.0%\"/>\n\u003Cfigcaption>Figure 1: Reparametrization of LoRA. (Image source: Fig. 1\nin \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>LoRA는 다음과 같은 장점을 가진다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Generalization of Full Fine-tuning\u003C/strong> LoRA rank\n\u003Cspan class=\"math inline\">\\(r\\)\u003C/span>의 설정에 따라 full fine-tuning에\n근접할 수도 있다(다양한 fine-tuning이 가능하다).\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>No Additional Inference Latency\u003C/strong> 모델의 배포때\n더해진 weight, 즉, \u003Cspan class=\"math inline\">\\(W = W_0 + BA\\)\u003C/span>로\n사용하면 추가 latency가 없다. 원래의 weight으로 돌리고 싶을땐 단순히\n다시 빼주면 된다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Modularity\u003C/strong> LoRA는 각 downstream task에 맞춰 만들\n수 있다. 즉, 각각의 task에 맞춰 모듈식으로 갈아끼워가며 사용할 수\n있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>단점으로는 더해진 형태로 weight을 쓰게 되면 동시에 여러 LoRA를 사용할\n수 없다는 점 등이 있다.\u003C/p>\n\u003Ch2 id=\"empirical-experiments\">Empirical Experiments\u003C/h2>\n\u003Cfigure id=\"fig:lora_wikisql\">\n\u003Cimg src=\"figures/lora-low-rank-adaptation-of-large-language-models/LoRA_wikisql.jpg\" style=\"width:100.0%\"/>\n\u003Cfigcaption>Figure 2: GPT-3 175B validation accuracy vs. number of\ntrainable parameters of several adaptation methods on WikiSQL and\nMNLI-matched. LoRA exhibits better scalability and task performance.\n(Image source: Fig. 2 in \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>성능은 대부분의 결과에서 full fine-tuning에 근접하거나 더 뛰어나며\n다른 방법들을 앞지르는 모습을 보여준다. Fig. \u003Ca data-reference=\"fig:lora_wikisql\" data-reference-type=\"ref\" href=\"#fig:lora_wikisql\">2\u003C/a>에\nGPT-3 175B에 대한 대략적인 실험결과가 나타나 있다. 조심해야할것은 Fig.\n\u003Ca data-reference=\"fig:lora_wikisql\" data-reference-type=\"ref\" href=\"#fig:lora_wikisql\">2\u003C/a>에 나와있듯이 trainable\nparameter를 증가시키는 것이 꼭 성능을 향상시키는 것은 아니라는\n점이다.\u003C/p>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cspan class=\"math inline\">\\(W_0+\\Delta W=W_0+BA\\)\u003C/span>로\nfine-tuning을 할 수 있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Inference latency가 없게 만들 수 있고, 모듈식 사용이\n가능하다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Memory와 Storage를 절약할 수 있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>성능은 대개 full fine-tuning에 비견될만큼 뛰어나다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-hu2021lora\" role=\"listitem\">[1] \nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen. 2021. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of\nLarge Language Models\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"},"uses":{}}];

					Promise.all([
						import("./_app/immutable/entry/start.c64402c8.js"),
						import("./_app/immutable/entry/app.8c09aa58.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 8],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>