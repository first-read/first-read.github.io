<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
	<link rel="manifest" href="./favicon/site.webmanifest">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<script>
		MathJax = {
			loader: { load: ['[tex]/textmacros'] },
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				packages: { '[+]': ['textmacros'] },
			},
		};
	</script>
	<script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js">
		</script>
	
		<link href="./_app/immutable/assets/0.08c9bd5d.css" rel="stylesheet">
		<link href="./_app/immutable/assets/3.15287e86.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.1162bf57.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.e108d1fd.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/singletons.caa5cb96.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.c39bda2d.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.38a092e6.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.3e7173ef.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/8.bfc6dc69.js"><title>First Read - LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</title><!-- HEAD_svelte-1qhwsnh_START --><!-- HEAD_svelte-1qhwsnh_END -->
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">  <div class="nav svelte-5baz1e" data-svelte-h="svelte-89nan6"><a href="/">Posts</a></div>  <div class="content svelte-lwo4ch"><div class="title svelte-lwo4ch" data-svelte-h="svelte-18zpg9f"><h1 class="svelte-lwo4ch">First Read</h1> <h2 class="svelte-lwo4ch">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</h2> <div class="date svelte-lwo4ch">2023-09-09</div></div> <!-- HTML_TAG_START --><html><body><h2 id="overview">Overview</h2>
<p>LLM이 발전하며 여러 곳에서 널리 쓰이고 있지만, 그에따라 inference에
드는 memory와 computation 비용 역시 증가하고 있다.이를 해결하기 위해
parameter를 줄이는 방법으로 low-bit-precision을 통한 quantization 방법이
쓰이고 있는데, <a href="#ref-dettmers2022llmint8">[1]</a>
이전까지의 방법들은 대부분 작은모델에서, 그것도 성능을 희생해야하는
경우가 대부분이였다.</p>
<p><a href="#ref-dettmers2022llmint8">[1]</a>에선 이를
해결하여 성능저하없이 scaling 가능한 Int8 quantization 방법인
<strong>LLM.int8()</strong>을 제시하였다.</p>
<p>LLM.int8()은 크게 두 부분으로 구성돼있다. Vector-wise quantization과
mixed-precision decomposition이 그것이다.</p>
<h2 id="int8-matrix-multiplication-at-scale">Int8 Matrix Multiplication
at Scale</h2>
<p>단순히 matrix 전체에 대해 Absmax나 zeropoint 등의 quantization 방법을
단일로 쓰는것은 크게 효과적이지 않다. 그 이유는 matrix의 값 중 outlier에
해당하는 값들이 전체 precision을 떨어뜨리기 때문이다. 따라서 다른 값들에
크게 영향을 주지 않게 고립시키는 것이 중요하므로 이를 해결하기 위해
row-wise quantization 등 여러 방법이 나오게 되었다. 특히 이 중
vector-wise 방법은 quantization이 이뤄진 부분끼리 matrix 연산이 된다는
점에서 더 precision을 보존할 수 있게 도와준다. 이것이 이 논문에서
vector-wise를 채택한 이유이다.</p>
<p>그러나 큰 모델에선(<a href="#ref-dettmers2022llmint8">[1]</a>에선 6.7B
언저리쯤이라고 밝혔다) vector-wise만으로는 충분하지 않은 경우가 많은데,
이를 해결하는 방법이 두번째 부분인 mixed-precision decomposition이다.
이는 outlier 부분을 계산할때는 16bit로 계산하고 나머지 부분은 Int8
quantization을 하여 결과를 합치는 방식으로, outlier가 전체에서 차지하는
부분이 작아(<span class="math inline">\(\approx\)</span>0.1%) memory와
computation은 기존의 방법과 거의 동일하게 아낄 수 있다.</p>
<p>LLM.int8()은 이 두 방법과 absmax를 결합한 quantization 방법이다.</p>
<h3 id="vector-wise-quantization">Vector-wise Quantization</h3>
<p>Vector-wise 방식은 matrix의 곱이 vector와 vector간 내적들로
이뤄졌다는점에서 착안해 각 vector내에서 quantization을 하는 방법이다.
즉, 두 매트릭스 <span class="math inline">\(\mathbf{A},
\mathbf{B}\)</span>에 대해 그 곱 <span class="math inline">\(\mathbf{AB}\)</span>를 구할때 <span class="math inline">\(\mathbf{A}\)</span>에선 row-wise로, <span class="math inline">\(\mathbf{B}\)</span>에선 column-wise로
quantization을 하면 된다. Dequantize를 위해선 다시 scaling constant로
나눠 주면 된다. 구체적으론 다음과 같다.</p>
<p>Hidden state <span class="math inline">\(\mathbf{X}_{f16}\in
\mathbb{R}^{s\times h}\)</span>와 weight matrix <span class="math inline">\(\mathbf{W}_{f16}\in \mathbb{R}^{h\times
o}\)</span>가 주어졌을때, 각각의 scaling constant vector가 <span class="math inline">\(\mathbf{c}_x \in \mathbb{R}^s\)</span>와 <span class="math inline">\(\mathbf{c}_w \in \mathbb{R}^o\)</span>라고 하자.
이때 computation 결과 <span class="math inline">\(\mathbf{C}_{f_{16}}\)</span>은 다음과 같다.</p>
<p><span><span class="math display">\[\mathbf{C}_{f_{16}} \approx
\frac{1}{\mathbf{c}_{x_{f16}}\otimes \mathbf{c}_{w_{f16}}}
\mathbf{C}_{i32} = \mathbf{S}\cdot\mathbf{C}_{i32}=
\mathbf{S}\cdot\mathbf{X}_{i8}\mathbf{W}_{i8} = \mathbf{S}\cdot
Q(\mathbf{X}_{f16})\phantom{.}
Q(\mathbf{W}_{f16}),\qquad{(1)}\]</span></span></p>
<p><span class="math inline">\(\otimes\)</span>는 벡터의 외적을
나타낸다.</p>
<h3 id="mixed-precision-decomposition">Mixed-precision
Decomposition</h3>
<p><a href="#ref-dettmers2022llmint8">[1]</a>에서
진행한 실험에 따르면, outlier는 주로 column 단위로 존재한다. 그런데
vector-wise quantization은 hidden state의 row에 대해 quantization을
하므로 이것만으론 효과적으로 quantization을 해낼 수 없다. 다행히
저자들의 실험에 따르면 대부분의 경우 outlier는 전체의 0.1% 정도만
차지하므로 이 부분의 precision을 분리해 계산하면 전체적으로는 큰
성능저하 없이 quantization을 할 수 있다. 구체적으론 다음과 같다.</p>
<p>Input matrix <span class="math inline">\(\mathbf{X}_{f16}\in
\mathbb{R}^{s\times h}\)</span>와 weight matrix <span class="math inline">\(\mathbf{W}_{f16}\in \mathbb{R}^{h\times
o}\)</span>에 대해 set <span class="math inline">\(O\)</span>가 다음과
같이 주어졌다고 하자. <span><span class="math display">\[O = \{ i\in
\mathbb{Z} | 0 \leq i &lt; h, \max_{j} \left| \mathbf{X}_{f16}[j][i]
\right| \geq \alpha \}\qquad{(2)}\]</span></span> (이때 <span class="math inline">\(\alpha\)</span>는 outlier의 기준이 되는
threshold로, 저자들은 실험을 통해 <span class="math inline">\(\alpha=6.0\)</span>이면 성능의 저하가 없다고
보고하였다.)</p>
<p>아인슈타인 노테이션을 이용하면 mixed-precision decomposition을 다음과
같이 표현할 수 있다.</p>
<p><span><span class="math display">\[\mathbf{C}_{f16} \approx
\sum\limits_{h\in O} \mathbf{X}_{f16}^h \mathbf{W}_{f16}^h +
\mathbf{S}_{f16} \cdot  \sum\limits_{h\not\in O} \mathbf{X}_{i8}^h
\mathbf{W}_{i8}^h\qquad{(3)}\]</span></span> 이때 <span class="math inline">\(\mathbf{S}_{f16}\)</span>는 denormalization
term이다.</p>
<h3 id="experimental-setup">Experimental Setup</h3>
<p>저자들은 LLM.int8()이 scaling에 얼마나 robust한지 실험을 진행하였다.
주로 quantization 방법간 비교를 위한 perplexity 측정과 zeroshot acc
측정을 통해 16bit와 비교하였다.</p>
<h3 id="main-results">Main Results</h3>
<p>실험 결과 단일 absmax나 zeropoint 방법은 scaling에 실패하였으며
LLM.int8()은 scaling에도 robust함을 보여주었다. 추가적으로 runtime
실험도 진행하였는데, 6.7B보다 작으면 overhead가 있긴 하지만 더 큰
사이즈의 모델에선 더 빠른 성능을 보여주었다. 작은 모델에선 굳이
quantization을 쓸 필요가 없다는 점을 상기한다면 이는 꽤 고무적인 결과라
볼 수 있다.</p>
<h2 id="emergent-large-magnitude-features-in-transformers-at-scale">Emergent
Large Magnitude Features in Transformers at Scale</h2>
<p>저자들은 LLM.int8()의 기반이 되었던 outlier에 대한 분석이 어떻게
진행되었는지를 보여주었다.</p>
<h3 id="finding-outlier-features">Finding Outlier Features</h3>
<p>저자들은 outlier를 찾기 위해 다음 두가지 목적을 정하였다.</p>
<ol>
<li><p>결과를 이해할 수 있고 복잡하지 않아야 함.</p></li>
<li><p>중요한 (체계적이거나 확률적인)패턴을 포착할 수 있어야
함.</p></li>
</ol>
<p>이를 바탕으로 저자들이 제시한 구체적인 outlier의 조건은 다음과
같다.</p>
<ol>
<li><p>Feature(column)의 적어도 한 값의 magnitude가 6 이상이여야
한다.</p></li>
<li><p>이 column과 같은 인덱스를 가진 다른 layer의 column들의 25% 이상이
해당 조건을 만족해야 한다.</p></li>
<li><p>모든 row의 6%이상에서 해당 조건을 만족해야 한다.</p></li>
</ol>
<p>해당 threshold는 실험을 통해 perplexity에 미치는 영향이 없는 것으로
판단되는 값으로 정한것이다. layer나 row의 비율은 가장 작은 모델 기준 단
하나의 outlier만 존재할때의 threshold이다.</p>
<p>Outlier는 사이즈가 큰 모델의 경우 systematic하게(대부분 layer에서
나오거나 아예 안나오거나) 존재하였으며, 작은 모델에선 probabilistic(어떤
layer들에서만 가끔씩)하게 존재 하였다.</p>
<h3 id="measuring-the-effect-of-outlier-features">Measuring the Effect
of Outlier Features</h3>
<p>저자들은 outlier의 영향을 측정하기 위한 실험도 진행하였다. 실험은 각
layer별로 outlier를 제거(0을 assign)하였을때와 아닐때의 softmax
probability의 top-1을 비교하였다. 이떄, cascading error(누적 에러)를
방지하기 위해 각 layer를 고립(실제 forward는 다시 원래값을 넣어서 진행)
시켜서 진행하였다. 대조군으론 non-outlier에 대해 같은 방법으로
진행하였다.</p>
<p>주요결과는 다음과 같다.</p>
<ol>
<li><p>Affection은 기본 quantization이 실패할때쯤(6 6.7B쯤)과 비슷할때
emergent하게 나타난다. 이는 기존의 quantization 방법이 scaling에
실패하는 이유가 outlier와 연관이 있을 수 있음을 보여준다.</p></li>
<li><p>perplexity와는 지수함수 관계에 있다. 이는 emergence가 단순히
model size가 아니라 여러 복잡한 요소들과 연관된(data quality, training
data 사용량 등) perplexity와 연관이 있음을 보여준다.</p></li>
<li><p>모든 layer에서 outlier가 나오게 되면 magnitude의 중간값도 크게
증가한다. 이렇게 큰 값은 quantization이 실패하는 주요 이유 중
하나이다.</p></li>
<li><p>N(outliers)가 perplexity와는 monotonic하고 model size와는
non-monotonic하게 연관이 있다. 이는 perplexity가 phase shift에 더 주요할
수도 있음을 보여준다.</p></li>
<li><p>Outlier를 제거시 top1이 40%에서 20%로 급감하고, perplexity는
6 10배 상승함을 보여주었다. 대조군에선 top1이 고작 0.02% 0.3%정도
감소하고 perplexity가 0.1% 상승하였다. 이는 outlier의 영향이 큼을
보여준다.</p></li>
</ol>
<h3 id="interpretation-of-quantization-performance">Interpretation of
Quantization Performance</h3>
<p>Outlier는 asymmetric하게 주로 한쪽으로 쏠린 경우가 많다(모두 양수거나
모두 음수거나). 이는 absmax보다 zeropoint가 더 효과적이였던 이유를
설명해준다. 하지만 zeropoint 방법 역시 accumulated quantization error에
의해 scaling에 실패하였다.</p>
<p>LLM.int8()을 쓸땐 absmax 방법을 택하나 zeropoint 방법을 택하나 큰
차이가 없었다. 이는 outlier가 아닌 나머지 부분이 symmetric하게 분포되어
있다고 해석할 수 있다. 하지만 row-wise보단 vector-wise 때의 LLM.int8()의
성능이 더 좋은것으로 보아 vector-wise의 장점은 그대로 보존된다고 볼 수
있다.</p>
<h2 id="limitations">Limitations</h2>
<p>이 논문의 한계점은 다음과 같다.</p>
<ol>
<li><p>FP8에선 수행하지 않았다.</p></li>
<li><p>175B보다 큰 모델에선 수행하지 않았다.</p></li>
<li><p>Attention function에 대해선 수행하지 않았다.</p></li>
<li><p>Training이나 fine-tuning에 대해선 수행하지 않았다.</p></li>
</ol>
<h2 id="quick-recap">Quick Recap</h2>
<ol>
<li><p>LLM.int8()은 vector-wise quantization과 mixed-precision
decomposition을 결합한 방법이다.</p></li>
<li><p>LLM.int8()은 (175B까진)scaling에 robust하다.</p></li>
<li><p>Outlier는 emergent하게 나타나며 큰 모델에선 주로 systematic하게,
작은 모델에선 probabilistic하게 나타난다.</p></li>
<li><p>Outlier의 emergence는 model size보다 perplexity와 연관이
있다.</p></li>
</ol>
<div class="references csl-bib-body hanging-indent" id="refs" role="list" style="margin-bottom: 2rem"><h2 style="margin-top: 4rem">References</h2>
<div class="csl-entry" id="ref-dettmers2022llmint8" role="listitem">[1] 
Dettmers, Tim, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022.
<span>“<a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-Bit
Matrix Multiplication for Transformers at Scale</a>.”</span>
</div>
</div>
</body></html><!-- HTML_TAG_END --> </div> 
			
			<script>
				{
					__sveltekit_rqxlb1 = {
						base: new URL(".", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{content:"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>LLM이 발전하며 여러 곳에서 널리 쓰이고 있지만, 그에따라 inference에\n드는 memory와 computation 비용 역시 증가하고 있다.이를 해결하기 위해\nparameter를 줄이는 방법으로 low-bit-precision을 통한 quantization 방법이\n쓰이고 있는데, \u003Ca href=\"#ref-dettmers2022llmint8\">[1]\u003C/a>\n이전까지의 방법들은 대부분 작은모델에서, 그것도 성능을 희생해야하는\n경우가 대부분이였다.\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-dettmers2022llmint8\">[1]\u003C/a>에선 이를\n해결하여 성능저하없이 scaling 가능한 Int8 quantization 방법인\n\u003Cstrong>LLM.int8()\u003C/strong>을 제시하였다.\u003C/p>\n\u003Cp>LLM.int8()은 크게 두 부분으로 구성돼있다. Vector-wise quantization과\nmixed-precision decomposition이 그것이다.\u003C/p>\n\u003Ch2 id=\"int8-matrix-multiplication-at-scale\">Int8 Matrix Multiplication\nat Scale\u003C/h2>\n\u003Cp>단순히 matrix 전체에 대해 Absmax나 zeropoint 등의 quantization 방법을\n단일로 쓰는것은 크게 효과적이지 않다. 그 이유는 matrix의 값 중 outlier에\n해당하는 값들이 전체 precision을 떨어뜨리기 때문이다. 따라서 다른 값들에\n크게 영향을 주지 않게 고립시키는 것이 중요하므로 이를 해결하기 위해\nrow-wise quantization 등 여러 방법이 나오게 되었다. 특히 이 중\nvector-wise 방법은 quantization이 이뤄진 부분끼리 matrix 연산이 된다는\n점에서 더 precision을 보존할 수 있게 도와준다. 이것이 이 논문에서\nvector-wise를 채택한 이유이다.\u003C/p>\n\u003Cp>그러나 큰 모델에선(\u003Ca href=\"#ref-dettmers2022llmint8\">[1]\u003C/a>에선 6.7B\n언저리쯤이라고 밝혔다) vector-wise만으로는 충분하지 않은 경우가 많은데,\n이를 해결하는 방법이 두번째 부분인 mixed-precision decomposition이다.\n이는 outlier 부분을 계산할때는 16bit로 계산하고 나머지 부분은 Int8\nquantization을 하여 결과를 합치는 방식으로, outlier가 전체에서 차지하는\n부분이 작아(\u003Cspan class=\"math inline\">\\(\\approx\\)\u003C/span>0.1%) memory와\ncomputation은 기존의 방법과 거의 동일하게 아낄 수 있다.\u003C/p>\n\u003Cp>LLM.int8()은 이 두 방법과 absmax를 결합한 quantization 방법이다.\u003C/p>\n\u003Ch3 id=\"vector-wise-quantization\">Vector-wise Quantization\u003C/h3>\n\u003Cp>Vector-wise 방식은 matrix의 곱이 vector와 vector간 내적들로\n이뤄졌다는점에서 착안해 각 vector내에서 quantization을 하는 방법이다.\n즉, 두 매트릭스 \u003Cspan class=\"math inline\">\\(\\mathbf{A},\n\\mathbf{B}\\)\u003C/span>에 대해 그 곱 \u003Cspan class=\"math inline\">\\(\\mathbf{AB}\\)\u003C/span>를 구할때 \u003Cspan class=\"math inline\">\\(\\mathbf{A}\\)\u003C/span>에선 row-wise로, \u003Cspan class=\"math inline\">\\(\\mathbf{B}\\)\u003C/span>에선 column-wise로\nquantization을 하면 된다. Dequantize를 위해선 다시 scaling constant로\n나눠 주면 된다. 구체적으론 다음과 같다.\u003C/p>\n\u003Cp>Hidden state \u003Cspan class=\"math inline\">\\(\\mathbf{X}_{f16}\\in\n\\mathbb{R}^{s\\times h}\\)\u003C/span>와 weight matrix \u003Cspan class=\"math inline\">\\(\\mathbf{W}_{f16}\\in \\mathbb{R}^{h\\times\no}\\)\u003C/span>가 주어졌을때, 각각의 scaling constant vector가 \u003Cspan class=\"math inline\">\\(\\mathbf{c}_x \\in \\mathbb{R}^s\\)\u003C/span>와 \u003Cspan class=\"math inline\">\\(\\mathbf{c}_w \\in \\mathbb{R}^o\\)\u003C/span>라고 하자.\n이때 computation 결과 \u003Cspan class=\"math inline\">\\(\\mathbf{C}_{f_{16}}\\)\u003C/span>은 다음과 같다.\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[\\mathbf{C}_{f_{16}} \\approx\n\\frac{1}{\\mathbf{c}_{x_{f16}}\\otimes \\mathbf{c}_{w_{f16}}}\n\\mathbf{C}_{i32} = \\mathbf{S}\\cdot\\mathbf{C}_{i32}=\n\\mathbf{S}\\cdot\\mathbf{X}_{i8}\\mathbf{W}_{i8} = \\mathbf{S}\\cdot\nQ(\\mathbf{X}_{f16})\\phantom{.}\nQ(\\mathbf{W}_{f16}),\\qquad{(1)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>\u003Cspan class=\"math inline\">\\(\\otimes\\)\u003C/span>는 벡터의 외적을\n나타낸다.\u003C/p>\n\u003Ch3 id=\"mixed-precision-decomposition\">Mixed-precision\nDecomposition\u003C/h3>\n\u003Cp>\u003Ca href=\"#ref-dettmers2022llmint8\">[1]\u003C/a>에서\n진행한 실험에 따르면, outlier는 주로 column 단위로 존재한다. 그런데\nvector-wise quantization은 hidden state의 row에 대해 quantization을\n하므로 이것만으론 효과적으로 quantization을 해낼 수 없다. 다행히\n저자들의 실험에 따르면 대부분의 경우 outlier는 전체의 0.1% 정도만\n차지하므로 이 부분의 precision을 분리해 계산하면 전체적으로는 큰\n성능저하 없이 quantization을 할 수 있다. 구체적으론 다음과 같다.\u003C/p>\n\u003Cp>Input matrix \u003Cspan class=\"math inline\">\\(\\mathbf{X}_{f16}\\in\n\\mathbb{R}^{s\\times h}\\)\u003C/span>와 weight matrix \u003Cspan class=\"math inline\">\\(\\mathbf{W}_{f16}\\in \\mathbb{R}^{h\\times\no}\\)\u003C/span>에 대해 set \u003Cspan class=\"math inline\">\\(O\\)\u003C/span>가 다음과\n같이 주어졌다고 하자. \u003Cspan>\u003Cspan class=\"math display\">\\[O = \\{ i\\in\n\\mathbb{Z} | 0 \\leq i &lt; h, \\max_{j} \\left| \\mathbf{X}_{f16}[j][i]\n\\right| \\geq \\alpha \\}\\qquad{(2)}\\]\u003C/span>\u003C/span> (이때 \u003Cspan class=\"math inline\">\\(\\alpha\\)\u003C/span>는 outlier의 기준이 되는\nthreshold로, 저자들은 실험을 통해 \u003Cspan class=\"math inline\">\\(\\alpha=6.0\\)\u003C/span>이면 성능의 저하가 없다고\n보고하였다.)\u003C/p>\n\u003Cp>아인슈타인 노테이션을 이용하면 mixed-precision decomposition을 다음과\n같이 표현할 수 있다.\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[\\mathbf{C}_{f16} \\approx\n\\sum\\limits_{h\\in O} \\mathbf{X}_{f16}^h \\mathbf{W}_{f16}^h +\n\\mathbf{S}_{f16} \\cdot  \\sum\\limits_{h\\not\\in O} \\mathbf{X}_{i8}^h\n\\mathbf{W}_{i8}^h\\qquad{(3)}\\]\u003C/span>\u003C/span> 이때 \u003Cspan class=\"math inline\">\\(\\mathbf{S}_{f16}\\)\u003C/span>는 denormalization\nterm이다.\u003C/p>\n\u003Ch3 id=\"experimental-setup\">Experimental Setup\u003C/h3>\n\u003Cp>저자들은 LLM.int8()이 scaling에 얼마나 robust한지 실험을 진행하였다.\n주로 quantization 방법간 비교를 위한 perplexity 측정과 zeroshot acc\n측정을 통해 16bit와 비교하였다.\u003C/p>\n\u003Ch3 id=\"main-results\">Main Results\u003C/h3>\n\u003Cp>실험 결과 단일 absmax나 zeropoint 방법은 scaling에 실패하였으며\nLLM.int8()은 scaling에도 robust함을 보여주었다. 추가적으로 runtime\n실험도 진행하였는데, 6.7B보다 작으면 overhead가 있긴 하지만 더 큰\n사이즈의 모델에선 더 빠른 성능을 보여주었다. 작은 모델에선 굳이\nquantization을 쓸 필요가 없다는 점을 상기한다면 이는 꽤 고무적인 결과라\n볼 수 있다.\u003C/p>\n\u003Ch2 id=\"emergent-large-magnitude-features-in-transformers-at-scale\">Emergent\nLarge Magnitude Features in Transformers at Scale\u003C/h2>\n\u003Cp>저자들은 LLM.int8()의 기반이 되었던 outlier에 대한 분석이 어떻게\n진행되었는지를 보여주었다.\u003C/p>\n\u003Ch3 id=\"finding-outlier-features\">Finding Outlier Features\u003C/h3>\n\u003Cp>저자들은 outlier를 찾기 위해 다음 두가지 목적을 정하였다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>결과를 이해할 수 있고 복잡하지 않아야 함.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>중요한 (체계적이거나 확률적인)패턴을 포착할 수 있어야\n함.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>이를 바탕으로 저자들이 제시한 구체적인 outlier의 조건은 다음과\n같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>Feature(column)의 적어도 한 값의 magnitude가 6 이상이여야\n한다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>이 column과 같은 인덱스를 가진 다른 layer의 column들의 25% 이상이\n해당 조건을 만족해야 한다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>모든 row의 6%이상에서 해당 조건을 만족해야 한다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>해당 threshold는 실험을 통해 perplexity에 미치는 영향이 없는 것으로\n판단되는 값으로 정한것이다. layer나 row의 비율은 가장 작은 모델 기준 단\n하나의 outlier만 존재할때의 threshold이다.\u003C/p>\n\u003Cp>Outlier는 사이즈가 큰 모델의 경우 systematic하게(대부분 layer에서\n나오거나 아예 안나오거나) 존재하였으며, 작은 모델에선 probabilistic(어떤\nlayer들에서만 가끔씩)하게 존재 하였다.\u003C/p>\n\u003Ch3 id=\"measuring-the-effect-of-outlier-features\">Measuring the Effect\nof Outlier Features\u003C/h3>\n\u003Cp>저자들은 outlier의 영향을 측정하기 위한 실험도 진행하였다. 실험은 각\nlayer별로 outlier를 제거(0을 assign)하였을때와 아닐때의 softmax\nprobability의 top-1을 비교하였다. 이떄, cascading error(누적 에러)를\n방지하기 위해 각 layer를 고립(실제 forward는 다시 원래값을 넣어서 진행)\n시켜서 진행하였다. 대조군으론 non-outlier에 대해 같은 방법으로\n진행하였다.\u003C/p>\n\u003Cp>주요결과는 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>Affection은 기본 quantization이 실패할때쯤(6 6.7B쯤)과 비슷할때\nemergent하게 나타난다. 이는 기존의 quantization 방법이 scaling에\n실패하는 이유가 outlier와 연관이 있을 수 있음을 보여준다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>perplexity와는 지수함수 관계에 있다. 이는 emergence가 단순히\nmodel size가 아니라 여러 복잡한 요소들과 연관된(data quality, training\ndata 사용량 등) perplexity와 연관이 있음을 보여준다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>모든 layer에서 outlier가 나오게 되면 magnitude의 중간값도 크게\n증가한다. 이렇게 큰 값은 quantization이 실패하는 주요 이유 중\n하나이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>N(outliers)가 perplexity와는 monotonic하고 model size와는\nnon-monotonic하게 연관이 있다. 이는 perplexity가 phase shift에 더 주요할\n수도 있음을 보여준다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Outlier를 제거시 top1이 40%에서 20%로 급감하고, perplexity는\n6 10배 상승함을 보여주었다. 대조군에선 top1이 고작 0.02% 0.3%정도\n감소하고 perplexity가 0.1% 상승하였다. 이는 outlier의 영향이 큼을\n보여준다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"interpretation-of-quantization-performance\">Interpretation of\nQuantization Performance\u003C/h3>\n\u003Cp>Outlier는 asymmetric하게 주로 한쪽으로 쏠린 경우가 많다(모두 양수거나\n모두 음수거나). 이는 absmax보다 zeropoint가 더 효과적이였던 이유를\n설명해준다. 하지만 zeropoint 방법 역시 accumulated quantization error에\n의해 scaling에 실패하였다.\u003C/p>\n\u003Cp>LLM.int8()을 쓸땐 absmax 방법을 택하나 zeropoint 방법을 택하나 큰\n차이가 없었다. 이는 outlier가 아닌 나머지 부분이 symmetric하게 분포되어\n있다고 해석할 수 있다. 하지만 row-wise보단 vector-wise 때의 LLM.int8()의\n성능이 더 좋은것으로 보아 vector-wise의 장점은 그대로 보존된다고 볼 수\n있다.\u003C/p>\n\u003Ch2 id=\"limitations\">Limitations\u003C/h2>\n\u003Cp>이 논문의 한계점은 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>FP8에선 수행하지 않았다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>175B보다 큰 모델에선 수행하지 않았다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Attention function에 대해선 수행하지 않았다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Training이나 fine-tuning에 대해선 수행하지 않았다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>LLM.int8()은 vector-wise quantization과 mixed-precision\ndecomposition을 결합한 방법이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM.int8()은 (175B까진)scaling에 robust하다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Outlier는 emergent하게 나타나며 큰 모델에선 주로 systematic하게,\n작은 모델에선 probabilistic하게 나타난다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Outlier의 emergence는 model size보다 perplexity와 연관이\n있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-dettmers2022llmint8\" role=\"listitem\">[1] \nDettmers, Tim, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022.\n\u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2208.07339\">LLM.int8(): 8-Bit\nMatrix Multiplication for Transformers at Scale\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"},"uses":{}}];

					Promise.all([
						import("./_app/immutable/entry/start.1162bf57.js"),
						import("./_app/immutable/entry/app.c39bda2d.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 8],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>