<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
	<link rel="manifest" href="./favicon/site.webmanifest">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<script>
		MathJax = {
			loader: { load: ['[tex]/textmacros'] },
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				packages: { '[+]': ['textmacros'] },
			},
		};
	</script>
	<script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js">
		</script>
	
		<link href="./_app/immutable/assets/0.08c9bd5d.css" rel="stylesheet">
		<link href="./_app/immutable/assets/3.15287e86.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.c92e3770.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.e108d1fd.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/singletons.6b72bec5.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.c101b71d.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.38a092e6.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.31eac6a9.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/3.74adf8a2.js"><title>First Read - Axiomatic Attribution for Deep Networks</title><!-- HEAD_svelte-1v2ofvz_START --><!-- HEAD_svelte-1v2ofvz_END -->
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">  <div class="nav svelte-5baz1e" data-svelte-h="svelte-89nan6"><a href="/">Posts</a></div>  <div class="content svelte-lwo4ch"><div class="title svelte-lwo4ch" data-svelte-h="svelte-khm6mp"><h1 class="svelte-lwo4ch">First Read</h1> <h2 class="svelte-lwo4ch">Axiomatic Attribution for Deep Networks</h2> <div class="date svelte-lwo4ch">2023-09-01</div></div> <!-- HTML_TAG_START --><html><body><h2 id="overview">Overview</h2>
<p>Deep network는 그 특성상 왜 그런 결과가 나왔는지를 설명하기 어렵다.
한 예로, 이미지 분류 문제같은 경우 “왜 이 이미지가 고양이로
분류되었는가?”라는 질문에 답하긴 쉽지 않다. 이러한 문제를 해결하기 위해
각 input이 output에 얼마나 영향을 미쳤는지를 계산하는 attribution
method가 여럿 제안되었다.</p>
<p>하지만 이런 attribution technique을 만들때의 힘든점 중 하나는
실험적으로 평가하기가 어렵다는 것이다. 예를 들어, attribution 계산 결과
예상과 꽤 차이나는 결과가 나왔다고 하자. 그렇다면 이는 attribution
method에 문제가 있는것인가? 아니면 모델의 잘못된 동작에서 비롯된
error인가? 이는 실험적 결과만 가지고는 쉽게 판단할 수 없는 문제이다.</p>
<p><a href="#ref-sundararajan2017axiomatic">[2]</a>에선
이러한 문제를 해결하기 위해 axiomatic한 접근방법을 취했다. 즉,
attribution method가 가져야할(desirable) 성질들을 정의하고 이를 만족하는
method를 찾아내는 방법을 택한것이다. 저자들은 이러한 성질들을 만족하는
새 attribution method인 <strong>integrated gradients</strong>를 제안하고
실제로 유용하다는 것을 보여주었다.</p>
<h2 id="the-definition-of-attribution">The definition of
Attribution</h2>
<p>저자들은 우선 deep network의 attribution이 무엇인지 정의하였다.</p>
<div class="definition">
<p><strong>Definition 1</strong>. <em>Formally, suppose we have a
function <span class="math inline">\(F: \mathbb{R}^n \rightarrow
[0,1]\)</span> that represents a deep network, and an input <span class="math inline">\(x = (x_1,\ldots,x_n) \in \mathbb{R}^n\)</span>. An
attribution of the prediction at input <span class="math inline">\(x\)</span> relative to a baseline input <span class="math inline">\(x'\)</span> is a vector <span class="math inline">\(A_F(x, x') = (a_1,\ldots,a_n) \in
\mathbb{R}^n\)</span> where <span class="math inline">\(a_i\)</span> is
the <em>contribution</em> of <span class="math inline">\(x_i\)</span> to
the prediction <span class="math inline">\(F(x)\)</span>. (Source:
Definition 1 in <a href="#ref-sundararajan2017axiomatic">[2]</a>)</em></p>
</div>
<p>이때 정의를 보면 baseline이 필요함을 알 수 있다. 이는 실제 우리가
인과관계를 추론할때를 생각해보면 그 필요성을 알 수 있는데, 인과관계
추론에선 흔히 그 원인(으로 추정되는것)이 있을때와 없을때를 비교해
생각하기 때문이다. Baseline으론 따라서 signal이 없음을 의미하는 값을
쓰는것이 자연스럽다(예를 들어, 이미지 분류 문제에서는 검은색
이미지).</p>
<h2 id="two-fundamental-axioms">Two Fundamental Axioms</h2>
<p>저자들은 attribution method가 만족해야할 두가지 axiom으로 다음을
제시하였다.</p>
<div class="axiom">
<p><strong>Axiom 1</strong>. <em><strong>Sensitivity(a)</strong>:
Input과 baseline의 한 feature가 다를때, 두 prediction 또한 다르다면 그
feature의 attribution은 0이 아니다.</em></p>
</div>
<p>이 axiom은 간단히 말해 어떤 feature가 prediction에 영향을 준다면
attribution 또한 0이 아니어야 한다는 말을 하고 있다.</p>
<p>Sensitivity(a)를 만족하지 않는 예로써 gradient를 생각해볼 수 있다.
예를 들어, attribution method로 gradient를 택하고 <span class="math inline">\(f(x) = 1 - \text{ReLU}(1 - x)\)</span>, baseline이
<span class="math inline">\(x = 0\)</span>인 경우를 보자. 이때, input이
<span class="math inline">\(x = 2\)</span>라면 함수값은 1로 baseline의
함수값 0과 다르다. 그러나 <span class="math inline">\(x =
2\)</span>에서의 gradient는 0이므로 attribution이 0이 되어 axiom을
만족하지 않는다. 즉, gradient는 attribution method로 적절하지 않다.</p>
<div class="axiom">
<p><strong>Axiom 2</strong>. <em><strong>Implementation
Invariance</strong>: Attribution method는 functionally equivalent<a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>한 두 network에 대해 같은 attribution
값을 보장해야 한다.</em></p>
</div>
<p>이 axiom은 attribution method가 network의 구현에 상관없이 동작해야
한다는 것을 의미한다. Attribution method가 구현에 의존하게 된다면, 같은
함수를 다른 방식으로 구현한 두 network에 대해 같은 attribution을 보장할
수 없게 된다. 이는 attribution method로는 바람직하지 않은 성질이다.</p>
<h2 id="path-methods">Path Methods</h2>
<p>저자들은 위 axiom에 추가로 유용한 성질들을 제시하였다.</p>
<div class="axiom">
<p><strong>Axiom 3</strong>. <em><strong>Completeness</strong>: 모든
feature에 대한 attribution의 합은 input <span class="math inline">\(x\)</span>에 대한 output과 baseline <span class="math inline">\(x'\)</span>에 대한 output의 차이와 같다. 즉,
<span><span class="math display">\[\sum_{i=1}^n \text{Attr}_i(x) = F(x)
- F(x')\qquad{(1)}\]</span></span></em></p>
</div>
<p>이는 attribution method의 sanity check에 도움을 준다.</p>
<div class="axiom">
<p><strong>Axiom 4</strong>. <em><strong>Sensitivity(b)</strong>: Deep
network 로 구현된 함수가 (수학적으로) 어떤 변수에 의존하지 않는다면, 그
변수의 attribution은 0이다.</em></p>
</div>
<p>이는 sensitivity(a)를 보완하는 성격의 axiom으로 sensitivity(a)의
역처럼 생각해볼 수 있다.</p>
<div class="axiom">
<p><strong>Axiom 5</strong>. <em><strong>Linearity</strong>: 두
네트워크가 각각 두 함수 <span class="math inline">\(f_1\)</span>과 <span class="math inline">\(f_2\)</span>로 표현된다 하자. 이때, 그
선형결합으로 표현되는 새 network <span class="math inline">\(f_3 =
\alpha f_1 + \beta f_2\)</span>의 attribution 역시 이와 동일한 계수를
갖는 <span class="math inline">\(f_1\)</span>과 <span class="math inline">\(f_2\)</span>의 attribution의
선형결합이다.</em></p>
</div>
<p><a href="#ref-Friedman">[1]</a>의
Theorem 1에 의하면, Implementation Invariance, Sensitivity(b),
Linearity, Completeness를 만족하는 attribution method는 <strong>path
method</strong>밖에 없다. 여기서 path method란 path integrated
gradients를 기반으로하는 attribution methods를 의미한다. Path integrated
gradients는 다음과 같이 정의된다.</p>
<div class="definition">
<p><strong>Definition 2</strong>. <em>Formally, let <span class="math inline">\(\gamma= (\gamma_1, \ldots, \gamma_n): [0,1]
    \rightarrow \mathbb{R}^n\)</span> be a smooth function specifying a
path in <span class="math inline">\(\mathbb{R}^n\)</span> from the
baseline <span class="math inline">\(x'\)</span> to the input <span class="math inline">\(x\)</span>, i.e., <span class="math inline">\(\gamma(0) = x'\)</span> and <span class="math inline">\(\gamma(1) = x\)</span>.</em></p>
<p><em>Given a path function <span class="math inline">\(\gamma\)</span>, <em>path integrated
gradients</em> are obtained by integrating the gradients along the path
<span class="math inline">\(\gamma(\alpha)\)</span> for <span class="math inline">\(\alpha \in [0,1]\)</span>. Formally, path
integrated gradients along the <span class="math inline">\(i^{th}\)</span> dimension for an input <span class="math inline">\(x\)</span> is defined as follows. <span><span class="math display">\[\text{PathIntegratedGrads}^{\gamma}_i(x) ::=
\int_{\alpha=0}^{1} \tfrac{\partial F(\gamma(\alpha))}{\partial
\gamma_i(\alpha)  }~\tfrac{\partial \gamma_i(\alpha)}{\partial
\alpha}  ~d\alpha\qquad{(2)}\]</span></span> where <span class="math inline">\(\tfrac{\partial F(x)}{\partial x_i}\)</span> is
the gradient<a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> of <span class="math inline">\(F\)</span> along the <span class="math inline">\(i^{th}\)</span> dimension at <span class="math inline">\(x\)</span>. (Source: Equation 2 in <a href="#ref-sundararajan2017axiomatic">[2]</a>)</em></p>
</div>
<p>Path integrated gradients는 식에서 알 수 있듯이 <span class="math inline">\(\tfrac{\partial F(\gamma(\alpha))}{\partial
\alpha}\)</span>를 chain rule을 적용하여 각 항별로 나눠 적분한것이다.
즉, <span class="math inline">\(\alpha\)</span>의 변화에 따라 각
feature가 최종 output의 변화에 얼마나 영향을 주었는지를 적분을 통해
알아본다고 생각할 수 있다.</p>
<h2 id="integrated-gradients">Integrated Gradients</h2>
<p>마지막으로 유용한 성질로 Symmetry-Preserving이란 것이 있다.</p>
<p>Attribution method가 <strong>Symmetry-Preserving</strong>하다는 것은
input이 모델의 모든 symmetric<a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> variables에 대해 동일
값을 가지고, baseline 역시 모든 symmetric variables에 대해 모두 동일값을
가질때, 이 symmetric variables에 대한 attribution은 같아야 한다는것을
말한다.</p>
<p>즉, 동일한 role로 작용하는 변수들에 대해 같은 attribution을 가져야
한다는 것이다.</p>
<p>이제 위의 모든 성질을 만족시키는<a class="footnote-ref" href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> attribution method로
저자들은 path method의 straightline case인 integrated gradients를
제안하였다. 즉, integrated gradients는 <span class="math inline">\(\alpha \in [0,1]\)</span>에 대해 <span class="math inline">\(\gamma(\alpha) = x' + \alpha
\times(x-x')\)</span>인 경우의 path method이다.<a class="footnote-ref" href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>저자는 integrated gradients가 symmetry-preserving 성질을 만족하는
유일한 path method 임을 보였다.</p>
<div class="theorem">
<p><strong>Theorem 1</strong>. <em>Integrated gradients는
symmetry-preserving 성질을 만족하는 유일한 path method이다.</em></p>
</div>
<h3 id="implementation">Implementation</h3>
<figure id="fig:intgrad-finalgrad">
<p><img alt="image" src="figures/axiomatic-attribution-for-deep-networks/img0.jpg" style="width:50.0%"/> <img alt="image" src="figures/axiomatic-attribution-for-deep-networks/img1.jpg" style="width:50.0%"/> <img alt="image" src="figures/axiomatic-attribution-for-deep-networks/img2.jpg" style="width:50.0%"/> <img alt="image" src="figures/axiomatic-attribution-for-deep-networks/img3.jpg" style="width:50.0%"/> <img alt="image" src="figures/axiomatic-attribution-for-deep-networks/img4.jpg" style="width:50.0%"/> <img alt="image" src="figures/axiomatic-attribution-for-deep-networks/img5.jpg" style="width:50.0%"/> <img alt="image" src="figures/axiomatic-attribution-for-deep-networks/img6.jpg" style="width:50.0%"/> <img alt="image" src="figures/axiomatic-attribution-for-deep-networks/img7.jpg" style="width:50.0%"/></p>
<figcaption>Figure 1: <strong>Comparing integrated gradients with
gradients at the image.</strong> Left-to-right: original input image,
label and softmax score for the highest scoring class, visualization of
integrated gradients, visualization of gradients*image. Notice that the
visualizations obtained from integrated gradients are better at
reflecting distinctive features of the image. (Source: Fig. 2 in <a href="#ref-sundararajan2017axiomatic">[2]</a>)
</figcaption>
</figure>
<p>Integrated gradients는 Riemman approximation을 이용해 쉽게 구현할 수
있다(<a href="#ref-sundararajan2017axiomatic">[2]</a>의
Equation 3 참고). 실제 <a href="https://github.com/ankurtaly/Integrated-Gradients/blob/master/IntegratedGradients/integrated_gradients.py">Official
github implementation</a>은 trapezoidal rule 역시 적용하여 근사하였다.
구현시 리만 근사의 step으로 <span class="math inline">\(m\)</span>을
정해야 하는데, 이는 적당한 <span class="math inline">\(m\)</span>을
넣어보고 attribution값을 <span class="math inline">\(F(x) -
F(x')\)</span>와 비교해보면서 적절한 <span class="math inline">\(m\)</span>을 찾아내면 된다(Completeness
axiom).</p>
<p>Fig. <a data-reference="fig:intgrad-finalgrad" data-reference-type="ref" href="#fig:intgrad-finalgrad">1</a>은 이미지 분류 모델에
integrated gradients와 gradient를 적용해 비교한 결과이다. Integrated
gradients는 실제 어떤 픽셀이 모델의 예측에 영향을 주었는지를 상대적으로
잘 보여주고 있다.</p>
<h2 id="quick-recap">Quick Recap</h2>
<ol>
<li><p>Attribution method가 만족해야할 두 성질 Sensitivity(a),
Implementation Invariance를 정의하였다.</p></li>
<li><p>그 외 유용한 성질로 Completeness, Sensitivity(b), Linearity,
Symmetry-Preserving을 정의하였다.</p></li>
<li><p>Path method는 Implementation Invariance, Sensitivity(b),
Linearity, Completeness를 만족하는 유일한 method이다.</p></li>
<li><p>Integrated gradients는 path method의 straightline case로, 위의
모든 성질을 만족하는 유일한 attribution method이다.</p></li>
<li><p>Integrated gradients는 Riemman approximation을 이용해 쉽게 구현할
수 있다. 이때 Completeness를 이용하면 적절한 step size를 찾을 수
있다.</p></li>
</ol>
<div class="references csl-bib-body hanging-indent" id="refs" role="list" style="margin-bottom: 2rem"><h2 style="margin-top: 4rem">References</h2>
<div class="csl-entry" id="ref-Friedman" role="listitem">[1] 
Friedman, Eric J. 2004. <span>“Paths and Consistency in Additive Cost
Sharing.”</span> <em>International Journal of Game Theory</em>.
</div>
<div class="csl-entry" id="ref-sundararajan2017axiomatic" role="listitem">[2] 
Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. <span>“<a href="https://arxiv.org/abs/1703.01365">Axiomatic Attribution for Deep
Networks</a>.”</span>
</div>
</div>
<aside class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p><em>두 network의 output이 모든 input에 대해 같다면 그
구현에 상관없이 <em>functionally equivalent</em>하다고 한다.</em><a class="footnote-back" href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><em>ML분야에선 gradient와 미분, 편미분 등을 혼용해서
쓰는 경우가 많다.</em><a class="footnote-back" href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>모든 <span class="math inline">\(x, y\)</span>가 함수
<span class="math inline">\(F\)</span>에 대해 <span class="math inline">\(F(x, y) = F(y, x)\)</span>를 만족시킨다면 <span class="math inline">\(x\)</span>와 <span class="math inline">\(y\)</span>를 symmetric하다고 한다.<a class="footnote-back" href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="math inline">\(F: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> 가 almost everywhere에서 differentiable할 때<a class="footnote-back" href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>논문의 Equation 1에선 <span class="math inline">\(x_i\)</span>를 혼용하여 써놓았다. <span class="math inline">\(\partial x_i\)</span>는 정확히는 <span class="math inline">\(\partial (x'_i + \alpha \times (x_i -
x'_i))\)</span>를 의미한다. <a href="https://github.com/ankurtaly/Integrated-Gradients/blob/master/IntegratedGradients/integrated_gradients.py">Official
github implementation</a>을 참고.<a class="footnote-back" href="#fnref5" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</body></html><!-- HTML_TAG_END --> </div> 
			
			<script>
				{
					__sveltekit_1pflgxc = {
						base: new URL(".", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{content:"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>Deep network는 그 특성상 왜 그런 결과가 나왔는지를 설명하기 어렵다.\n한 예로, 이미지 분류 문제같은 경우 “왜 이 이미지가 고양이로\n분류되었는가?”라는 질문에 답하긴 쉽지 않다. 이러한 문제를 해결하기 위해\n각 input이 output에 얼마나 영향을 미쳤는지를 계산하는 attribution\nmethod가 여럿 제안되었다.\u003C/p>\n\u003Cp>하지만 이런 attribution technique을 만들때의 힘든점 중 하나는\n실험적으로 평가하기가 어렵다는 것이다. 예를 들어, attribution 계산 결과\n예상과 꽤 차이나는 결과가 나왔다고 하자. 그렇다면 이는 attribution\nmethod에 문제가 있는것인가? 아니면 모델의 잘못된 동작에서 비롯된\nerror인가? 이는 실험적 결과만 가지고는 쉽게 판단할 수 없는 문제이다.\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-sundararajan2017axiomatic\">[2]\u003C/a>에선\n이러한 문제를 해결하기 위해 axiomatic한 접근방법을 취했다. 즉,\nattribution method가 가져야할(desirable) 성질들을 정의하고 이를 만족하는\nmethod를 찾아내는 방법을 택한것이다. 저자들은 이러한 성질들을 만족하는\n새 attribution method인 \u003Cstrong>integrated gradients\u003C/strong>를 제안하고\n실제로 유용하다는 것을 보여주었다.\u003C/p>\n\u003Ch2 id=\"the-definition-of-attribution\">The definition of\nAttribution\u003C/h2>\n\u003Cp>저자들은 우선 deep network의 attribution이 무엇인지 정의하였다.\u003C/p>\n\u003Cdiv class=\"definition\">\n\u003Cp>\u003Cstrong>Definition 1\u003C/strong>. \u003Cem>Formally, suppose we have a\nfunction \u003Cspan class=\"math inline\">\\(F: \\mathbb{R}^n \\rightarrow\n[0,1]\\)\u003C/span> that represents a deep network, and an input \u003Cspan class=\"math inline\">\\(x = (x_1,\\ldots,x_n) \\in \\mathbb{R}^n\\)\u003C/span>. An\nattribution of the prediction at input \u003Cspan class=\"math inline\">\\(x\\)\u003C/span> relative to a baseline input \u003Cspan class=\"math inline\">\\(x'\\)\u003C/span> is a vector \u003Cspan class=\"math inline\">\\(A_F(x, x') = (a_1,\\ldots,a_n) \\in\n\\mathbb{R}^n\\)\u003C/span> where \u003Cspan class=\"math inline\">\\(a_i\\)\u003C/span> is\nthe \u003Cem>contribution\u003C/em> of \u003Cspan class=\"math inline\">\\(x_i\\)\u003C/span> to\nthe prediction \u003Cspan class=\"math inline\">\\(F(x)\\)\u003C/span>. (Source:\nDefinition 1 in \u003Ca href=\"#ref-sundararajan2017axiomatic\">[2]\u003C/a>)\u003C/em>\u003C/p>\n\u003C/div>\n\u003Cp>이때 정의를 보면 baseline이 필요함을 알 수 있다. 이는 실제 우리가\n인과관계를 추론할때를 생각해보면 그 필요성을 알 수 있는데, 인과관계\n추론에선 흔히 그 원인(으로 추정되는것)이 있을때와 없을때를 비교해\n생각하기 때문이다. Baseline으론 따라서 signal이 없음을 의미하는 값을\n쓰는것이 자연스럽다(예를 들어, 이미지 분류 문제에서는 검은색\n이미지).\u003C/p>\n\u003Ch2 id=\"two-fundamental-axioms\">Two Fundamental Axioms\u003C/h2>\n\u003Cp>저자들은 attribution method가 만족해야할 두가지 axiom으로 다음을\n제시하였다.\u003C/p>\n\u003Cdiv class=\"axiom\">\n\u003Cp>\u003Cstrong>Axiom 1\u003C/strong>. \u003Cem>\u003Cstrong>Sensitivity(a)\u003C/strong>:\nInput과 baseline의 한 feature가 다를때, 두 prediction 또한 다르다면 그\nfeature의 attribution은 0이 아니다.\u003C/em>\u003C/p>\n\u003C/div>\n\u003Cp>이 axiom은 간단히 말해 어떤 feature가 prediction에 영향을 준다면\nattribution 또한 0이 아니어야 한다는 말을 하고 있다.\u003C/p>\n\u003Cp>Sensitivity(a)를 만족하지 않는 예로써 gradient를 생각해볼 수 있다.\n예를 들어, attribution method로 gradient를 택하고 \u003Cspan class=\"math inline\">\\(f(x) = 1 - \\text{ReLU}(1 - x)\\)\u003C/span>, baseline이\n\u003Cspan class=\"math inline\">\\(x = 0\\)\u003C/span>인 경우를 보자. 이때, input이\n\u003Cspan class=\"math inline\">\\(x = 2\\)\u003C/span>라면 함수값은 1로 baseline의\n함수값 0과 다르다. 그러나 \u003Cspan class=\"math inline\">\\(x =\n2\\)\u003C/span>에서의 gradient는 0이므로 attribution이 0이 되어 axiom을\n만족하지 않는다. 즉, gradient는 attribution method로 적절하지 않다.\u003C/p>\n\u003Cdiv class=\"axiom\">\n\u003Cp>\u003Cstrong>Axiom 2\u003C/strong>. \u003Cem>\u003Cstrong>Implementation\nInvariance\u003C/strong>: Attribution method는 functionally equivalent\u003Ca class=\"footnote-ref\" href=\"#fn1\" id=\"fnref1\" role=\"doc-noteref\">\u003Csup>1\u003C/sup>\u003C/a>한 두 network에 대해 같은 attribution\n값을 보장해야 한다.\u003C/em>\u003C/p>\n\u003C/div>\n\u003Cp>이 axiom은 attribution method가 network의 구현에 상관없이 동작해야\n한다는 것을 의미한다. Attribution method가 구현에 의존하게 된다면, 같은\n함수를 다른 방식으로 구현한 두 network에 대해 같은 attribution을 보장할\n수 없게 된다. 이는 attribution method로는 바람직하지 않은 성질이다.\u003C/p>\n\u003Ch2 id=\"path-methods\">Path Methods\u003C/h2>\n\u003Cp>저자들은 위 axiom에 추가로 유용한 성질들을 제시하였다.\u003C/p>\n\u003Cdiv class=\"axiom\">\n\u003Cp>\u003Cstrong>Axiom 3\u003C/strong>. \u003Cem>\u003Cstrong>Completeness\u003C/strong>: 모든\nfeature에 대한 attribution의 합은 input \u003Cspan class=\"math inline\">\\(x\\)\u003C/span>에 대한 output과 baseline \u003Cspan class=\"math inline\">\\(x'\\)\u003C/span>에 대한 output의 차이와 같다. 즉,\n\u003Cspan>\u003Cspan class=\"math display\">\\[\\sum_{i=1}^n \\text{Attr}_i(x) = F(x)\n- F(x')\\qquad{(1)}\\]\u003C/span>\u003C/span>\u003C/em>\u003C/p>\n\u003C/div>\n\u003Cp>이는 attribution method의 sanity check에 도움을 준다.\u003C/p>\n\u003Cdiv class=\"axiom\">\n\u003Cp>\u003Cstrong>Axiom 4\u003C/strong>. \u003Cem>\u003Cstrong>Sensitivity(b)\u003C/strong>: Deep\nnetwork 로 구현된 함수가 (수학적으로) 어떤 변수에 의존하지 않는다면, 그\n변수의 attribution은 0이다.\u003C/em>\u003C/p>\n\u003C/div>\n\u003Cp>이는 sensitivity(a)를 보완하는 성격의 axiom으로 sensitivity(a)의\n역처럼 생각해볼 수 있다.\u003C/p>\n\u003Cdiv class=\"axiom\">\n\u003Cp>\u003Cstrong>Axiom 5\u003C/strong>. \u003Cem>\u003Cstrong>Linearity\u003C/strong>: 두\n네트워크가 각각 두 함수 \u003Cspan class=\"math inline\">\\(f_1\\)\u003C/span>과 \u003Cspan class=\"math inline\">\\(f_2\\)\u003C/span>로 표현된다 하자. 이때, 그\n선형결합으로 표현되는 새 network \u003Cspan class=\"math inline\">\\(f_3 =\n\\alpha f_1 + \\beta f_2\\)\u003C/span>의 attribution 역시 이와 동일한 계수를\n갖는 \u003Cspan class=\"math inline\">\\(f_1\\)\u003C/span>과 \u003Cspan class=\"math inline\">\\(f_2\\)\u003C/span>의 attribution의\n선형결합이다.\u003C/em>\u003C/p>\n\u003C/div>\n\u003Cp>\u003Ca href=\"#ref-Friedman\">[1]\u003C/a>의\nTheorem 1에 의하면, Implementation Invariance, Sensitivity(b),\nLinearity, Completeness를 만족하는 attribution method는 \u003Cstrong>path\nmethod\u003C/strong>밖에 없다. 여기서 path method란 path integrated\ngradients를 기반으로하는 attribution methods를 의미한다. Path integrated\ngradients는 다음과 같이 정의된다.\u003C/p>\n\u003Cdiv class=\"definition\">\n\u003Cp>\u003Cstrong>Definition 2\u003C/strong>. \u003Cem>Formally, let \u003Cspan class=\"math inline\">\\(\\gamma= (\\gamma_1, \\ldots, \\gamma_n): [0,1]\n    \\rightarrow \\mathbb{R}^n\\)\u003C/span> be a smooth function specifying a\npath in \u003Cspan class=\"math inline\">\\(\\mathbb{R}^n\\)\u003C/span> from the\nbaseline \u003Cspan class=\"math inline\">\\(x'\\)\u003C/span> to the input \u003Cspan class=\"math inline\">\\(x\\)\u003C/span>, i.e., \u003Cspan class=\"math inline\">\\(\\gamma(0) = x'\\)\u003C/span> and \u003Cspan class=\"math inline\">\\(\\gamma(1) = x\\)\u003C/span>.\u003C/em>\u003C/p>\n\u003Cp>\u003Cem>Given a path function \u003Cspan class=\"math inline\">\\(\\gamma\\)\u003C/span>, \u003Cem>path integrated\ngradients\u003C/em> are obtained by integrating the gradients along the path\n\u003Cspan class=\"math inline\">\\(\\gamma(\\alpha)\\)\u003C/span> for \u003Cspan class=\"math inline\">\\(\\alpha \\in [0,1]\\)\u003C/span>. Formally, path\nintegrated gradients along the \u003Cspan class=\"math inline\">\\(i^{th}\\)\u003C/span> dimension for an input \u003Cspan class=\"math inline\">\\(x\\)\u003C/span> is defined as follows. \u003Cspan>\u003Cspan class=\"math display\">\\[\\text{PathIntegratedGrads}^{\\gamma}_i(x) ::=\n\\int_{\\alpha=0}^{1} \\tfrac{\\partial F(\\gamma(\\alpha))}{\\partial\n\\gamma_i(\\alpha)  }~\\tfrac{\\partial \\gamma_i(\\alpha)}{\\partial\n\\alpha}  ~d\\alpha\\qquad{(2)}\\]\u003C/span>\u003C/span> where \u003Cspan class=\"math inline\">\\(\\tfrac{\\partial F(x)}{\\partial x_i}\\)\u003C/span> is\nthe gradient\u003Ca class=\"footnote-ref\" href=\"#fn2\" id=\"fnref2\" role=\"doc-noteref\">\u003Csup>2\u003C/sup>\u003C/a> of \u003Cspan class=\"math inline\">\\(F\\)\u003C/span> along the \u003Cspan class=\"math inline\">\\(i^{th}\\)\u003C/span> dimension at \u003Cspan class=\"math inline\">\\(x\\)\u003C/span>. (Source: Equation 2 in \u003Ca href=\"#ref-sundararajan2017axiomatic\">[2]\u003C/a>)\u003C/em>\u003C/p>\n\u003C/div>\n\u003Cp>Path integrated gradients는 식에서 알 수 있듯이 \u003Cspan class=\"math inline\">\\(\\tfrac{\\partial F(\\gamma(\\alpha))}{\\partial\n\\alpha}\\)\u003C/span>를 chain rule을 적용하여 각 항별로 나눠 적분한것이다.\n즉, \u003Cspan class=\"math inline\">\\(\\alpha\\)\u003C/span>의 변화에 따라 각\nfeature가 최종 output의 변화에 얼마나 영향을 주었는지를 적분을 통해\n알아본다고 생각할 수 있다.\u003C/p>\n\u003Ch2 id=\"integrated-gradients\">Integrated Gradients\u003C/h2>\n\u003Cp>마지막으로 유용한 성질로 Symmetry-Preserving이란 것이 있다.\u003C/p>\n\u003Cp>Attribution method가 \u003Cstrong>Symmetry-Preserving\u003C/strong>하다는 것은\ninput이 모델의 모든 symmetric\u003Ca class=\"footnote-ref\" href=\"#fn3\" id=\"fnref3\" role=\"doc-noteref\">\u003Csup>3\u003C/sup>\u003C/a> variables에 대해 동일\n값을 가지고, baseline 역시 모든 symmetric variables에 대해 모두 동일값을\n가질때, 이 symmetric variables에 대한 attribution은 같아야 한다는것을\n말한다.\u003C/p>\n\u003Cp>즉, 동일한 role로 작용하는 변수들에 대해 같은 attribution을 가져야\n한다는 것이다.\u003C/p>\n\u003Cp>이제 위의 모든 성질을 만족시키는\u003Ca class=\"footnote-ref\" href=\"#fn4\" id=\"fnref4\" role=\"doc-noteref\">\u003Csup>4\u003C/sup>\u003C/a> attribution method로\n저자들은 path method의 straightline case인 integrated gradients를\n제안하였다. 즉, integrated gradients는 \u003Cspan class=\"math inline\">\\(\\alpha \\in [0,1]\\)\u003C/span>에 대해 \u003Cspan class=\"math inline\">\\(\\gamma(\\alpha) = x' + \\alpha\n\\times(x-x')\\)\u003C/span>인 경우의 path method이다.\u003Ca class=\"footnote-ref\" href=\"#fn5\" id=\"fnref5\" role=\"doc-noteref\">\u003Csup>5\u003C/sup>\u003C/a>\u003C/p>\n\u003Cp>저자는 integrated gradients가 symmetry-preserving 성질을 만족하는\n유일한 path method 임을 보였다.\u003C/p>\n\u003Cdiv class=\"theorem\">\n\u003Cp>\u003Cstrong>Theorem 1\u003C/strong>. \u003Cem>Integrated gradients는\nsymmetry-preserving 성질을 만족하는 유일한 path method이다.\u003C/em>\u003C/p>\n\u003C/div>\n\u003Ch3 id=\"implementation\">Implementation\u003C/h3>\n\u003Cfigure id=\"fig:intgrad-finalgrad\">\n\u003Cp>\u003Cimg alt=\"image\" src=\"figures/axiomatic-attribution-for-deep-networks/img0.jpg\" style=\"width:50.0%\"/> \u003Cimg alt=\"image\" src=\"figures/axiomatic-attribution-for-deep-networks/img1.jpg\" style=\"width:50.0%\"/> \u003Cimg alt=\"image\" src=\"figures/axiomatic-attribution-for-deep-networks/img2.jpg\" style=\"width:50.0%\"/> \u003Cimg alt=\"image\" src=\"figures/axiomatic-attribution-for-deep-networks/img3.jpg\" style=\"width:50.0%\"/> \u003Cimg alt=\"image\" src=\"figures/axiomatic-attribution-for-deep-networks/img4.jpg\" style=\"width:50.0%\"/> \u003Cimg alt=\"image\" src=\"figures/axiomatic-attribution-for-deep-networks/img5.jpg\" style=\"width:50.0%\"/> \u003Cimg alt=\"image\" src=\"figures/axiomatic-attribution-for-deep-networks/img6.jpg\" style=\"width:50.0%\"/> \u003Cimg alt=\"image\" src=\"figures/axiomatic-attribution-for-deep-networks/img7.jpg\" style=\"width:50.0%\"/>\u003C/p>\n\u003Cfigcaption>Figure 1: \u003Cstrong>Comparing integrated gradients with\ngradients at the image.\u003C/strong> Left-to-right: original input image,\nlabel and softmax score for the highest scoring class, visualization of\nintegrated gradients, visualization of gradients*image. Notice that the\nvisualizations obtained from integrated gradients are better at\nreflecting distinctive features of the image. (Source: Fig. 2 in \u003Ca href=\"#ref-sundararajan2017axiomatic\">[2]\u003C/a>)\n\u003C/figcaption>\n\u003C/figure>\n\u003Cp>Integrated gradients는 Riemman approximation을 이용해 쉽게 구현할 수\n있다(\u003Ca href=\"#ref-sundararajan2017axiomatic\">[2]\u003C/a>의\nEquation 3 참고). 실제 \u003Ca href=\"https://github.com/ankurtaly/Integrated-Gradients/blob/master/IntegratedGradients/integrated_gradients.py\">Official\ngithub implementation\u003C/a>은 trapezoidal rule 역시 적용하여 근사하였다.\n구현시 리만 근사의 step으로 \u003Cspan class=\"math inline\">\\(m\\)\u003C/span>을\n정해야 하는데, 이는 적당한 \u003Cspan class=\"math inline\">\\(m\\)\u003C/span>을\n넣어보고 attribution값을 \u003Cspan class=\"math inline\">\\(F(x) -\nF(x')\\)\u003C/span>와 비교해보면서 적절한 \u003Cspan class=\"math inline\">\\(m\\)\u003C/span>을 찾아내면 된다(Completeness\naxiom).\u003C/p>\n\u003Cp>Fig. \u003Ca data-reference=\"fig:intgrad-finalgrad\" data-reference-type=\"ref\" href=\"#fig:intgrad-finalgrad\">1\u003C/a>은 이미지 분류 모델에\nintegrated gradients와 gradient를 적용해 비교한 결과이다. Integrated\ngradients는 실제 어떤 픽셀이 모델의 예측에 영향을 주었는지를 상대적으로\n잘 보여주고 있다.\u003C/p>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>Attribution method가 만족해야할 두 성질 Sensitivity(a),\nImplementation Invariance를 정의하였다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>그 외 유용한 성질로 Completeness, Sensitivity(b), Linearity,\nSymmetry-Preserving을 정의하였다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Path method는 Implementation Invariance, Sensitivity(b),\nLinearity, Completeness를 만족하는 유일한 method이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Integrated gradients는 path method의 straightline case로, 위의\n모든 성질을 만족하는 유일한 attribution method이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Integrated gradients는 Riemman approximation을 이용해 쉽게 구현할\n수 있다. 이때 Completeness를 이용하면 적절한 step size를 찾을 수\n있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-Friedman\" role=\"listitem\">[1] \nFriedman, Eric J. 2004. \u003Cspan>“Paths and Consistency in Additive Cost\nSharing.”\u003C/span> \u003Cem>International Journal of Game Theory\u003C/em>.\n\u003C/div>\n\u003Cdiv class=\"csl-entry\" id=\"ref-sundararajan2017axiomatic\" role=\"listitem\">[2] \nSundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/1703.01365\">Axiomatic Attribution for Deep\nNetworks\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003Caside class=\"footnotes footnotes-end-of-document\" id=\"footnotes\" role=\"doc-endnotes\">\n\u003Chr/>\n\u003Col>\n\u003Cli id=\"fn1\">\u003Cp>\u003Cem>두 network의 output이 모든 input에 대해 같다면 그\n구현에 상관없이 \u003Cem>functionally equivalent\u003C/em>하다고 한다.\u003C/em>\u003Ca class=\"footnote-back\" href=\"#fnref1\" role=\"doc-backlink\">↩︎\u003C/a>\u003C/p>\u003C/li>\n\u003Cli id=\"fn2\">\u003Cp>\u003Cem>ML분야에선 gradient와 미분, 편미분 등을 혼용해서\n쓰는 경우가 많다.\u003C/em>\u003Ca class=\"footnote-back\" href=\"#fnref2\" role=\"doc-backlink\">↩︎\u003C/a>\u003C/p>\u003C/li>\n\u003Cli id=\"fn3\">\u003Cp>모든 \u003Cspan class=\"math inline\">\\(x, y\\)\u003C/span>가 함수\n\u003Cspan class=\"math inline\">\\(F\\)\u003C/span>에 대해 \u003Cspan class=\"math inline\">\\(F(x, y) = F(y, x)\\)\u003C/span>를 만족시킨다면 \u003Cspan class=\"math inline\">\\(x\\)\u003C/span>와 \u003Cspan class=\"math inline\">\\(y\\)\u003C/span>를 symmetric하다고 한다.\u003Ca class=\"footnote-back\" href=\"#fnref3\" role=\"doc-backlink\">↩︎\u003C/a>\u003C/p>\u003C/li>\n\u003Cli id=\"fn4\">\u003Cp>\u003Cspan class=\"math inline\">\\(F: \\mathbb{R}^n \\rightarrow\n\\mathbb{R}\\)\u003C/span> 가 almost everywhere에서 differentiable할 때\u003Ca class=\"footnote-back\" href=\"#fnref4\" role=\"doc-backlink\">↩︎\u003C/a>\u003C/p>\u003C/li>\n\u003Cli id=\"fn5\">\u003Cp>논문의 Equation 1에선 \u003Cspan class=\"math inline\">\\(x_i\\)\u003C/span>를 혼용하여 써놓았다. \u003Cspan class=\"math inline\">\\(\\partial x_i\\)\u003C/span>는 정확히는 \u003Cspan class=\"math inline\">\\(\\partial (x'_i + \\alpha \\times (x_i -\nx'_i))\\)\u003C/span>를 의미한다. \u003Ca href=\"https://github.com/ankurtaly/Integrated-Gradients/blob/master/IntegratedGradients/integrated_gradients.py\">Official\ngithub implementation\u003C/a>을 참고.\u003Ca class=\"footnote-back\" href=\"#fnref5\" role=\"doc-backlink\">↩︎\u003C/a>\u003C/p>\u003C/li>\n\u003C/ol>\n\u003C/aside>\n\u003C/body>\u003C/html>"},"uses":{}}];

					Promise.all([
						import("./_app/immutable/entry/start.c92e3770.js"),
						import("./_app/immutable/entry/app.c101b71d.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>