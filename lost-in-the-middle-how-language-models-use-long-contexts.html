<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
	<link rel="manifest" href="./favicon/site.webmanifest">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<script>
		MathJax = {
			loader: { load: ['[tex]/textmacros'] },
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				packages: { '[+]': ['textmacros'] },
			},
		};
	</script>
	<script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js">
		</script>
	
		<link href="./_app/immutable/assets/0.08c9bd5d.css" rel="stylesheet">
		<link href="./_app/immutable/assets/3.15287e86.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.63a02e9c.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.e108d1fd.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/singletons.36d626ee.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.b8702bb8.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.38a092e6.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.0396fef1.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/8.baa1eb43.js"><title>First Read - Lost in the Middle: How Language Models Use Long Contexts</title><!-- HEAD_svelte-t15so9_START --><!-- HEAD_svelte-t15so9_END -->
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">  <div class="nav svelte-5baz1e" data-svelte-h="svelte-89nan6"><a href="/">Posts</a></div>  <div class="content svelte-lwo4ch"><div class="title svelte-lwo4ch" data-svelte-h="svelte-mka0s"><h1 class="svelte-lwo4ch">First Read</h1> <h2 class="svelte-lwo4ch">Lost in the Middle: How Language Models Use Long Contexts</h2> <div class="date svelte-lwo4ch">2023-08-16</div></div> <!-- HTML_TAG_START --><html><body><h2 id="overview">Overview</h2>
<p>언어모델의 input context 길이는 그 모델이 얼마나 긴 input을 한번에
처리할 수 있는지를 보여준다. 이 길이가 짧다면 여러 제약이 생기기
마련인데(예를 들어, 문서 요약 작업 등에서 context 길이를 초과하는 문서는
처리를 할 수 없어 요약을 할 수 없다. 이를 처리하기 위해선 별도의 우회
작업을 해야 한다.) 최근의 여러 언어모델들은 이런 제약을 극복하기 위해
context 길이를 늘리는 추세이다.<br/>
그런데 이렇게 context 길이가 늘어나면 늘어나는대로 모든 context를 처리할
수 있는걸까? Context가 긴 경우, 찾고자 하는 정보가 어느 위치에 있던지
상관없이 모두 동일한 성능을 보일까?<br/>
<a href="#ref-liu-etal:2023:arxiv">[1]</a>은 이를
알아보기 위해 여러 실험을 하여 그 결과를 소개한 논문이다.</p>
<h2 id="experimental-setup">Experimental Setup</h2>
<h3 class="unnumbered" id="tasks">Tasks</h3>
<ol>
<li><p>Multi-Document Question Answering Task</p></li>
<li><p>Key-Value Retrieval Task</p></li>
<li><p>Effect of Model Architecture</p></li>
<li><p>Effect of Query-Aware Contextualization</p></li>
<li><p>Effect of Instruction-Tuning</p></li>
<li><p>Is More Context Is Always Better?</p></li>
</ol>
<p>1과 2는 input context가 늘어감에 따라 accuracy가 어떻게 변하는지,
주어진 정답이 input context의 어디에 위치하는지에 따라 accuracy가 어떻게
변하는지를 보는 실험이다.<br/>
3, 4, 5는 왜 모델이 input context를 전부 활용하지 못하는지 알아보기 위한
추가 실험이다.<br/>
6은 context가 긴게 항상 더 나은지를 알아보기위한 추가 실험이다.</p>
<h3 class="unnumbered" id="models">Models</h3>
<ol>
<li><p>MPT-30B-Instruct, MPT-30B-Base(Instruction-Tuning
실험에서)</p></li>
<li><p>LongChat-13B(16K)</p></li>
<li><p>GPT-3.5-Turbo, GPT-3.5-Turbo(16K)</p></li>
<li><p>Claude-1.3, Claude-1.3(100K)</p></li>
</ol>
<p>1과 2는 Open Model, 3과 4는 Closed Model이다.</p>
<h2 id="multi-document-question-answering">Multi-Document Question
Answering</h2>
<p>Multi-doc QA는 말 그대로 여러 문서를 input으로 주고 정답을 찾아내는
실험이다. 구체적으론, 정답을 포함한 한개의 문서와 정답을 포함하지 않지만
연관성이 있는 <span class="math inline">\(k - 1\)</span>개의 문서를
input으로 주고 실험을 진행한다.</p>
<figure id="fig:qa_results">
<img src="figures/lost-in-the-middle-how-language-models-use-long-contexts/qa.jpeg" style="width:100.0%"/>
<figcaption>Figure 1: The effect of changing the position of relevant
information (document containing the answer) on multi-document question
answering performance. Lower positions are closer to the start of the
input context. Performance is generally highest when relevant
information is positioned at the very start or very end of the context,
and rapidly degrades when models must reason over information in the
middle of their input context. (Image source: Fig. 5 in <a href="#ref-liu-etal:2023:arxiv">[1]</a>)</figcaption>
</figure>
<figure id="fig:qa_context_length_results">
<img src="figures/lost-in-the-middle-how-language-models-use-long-contexts/qa_context_length.jpeg" style="width:40.0%"/>
<figcaption>Figure 2: Language model performance (averaged across
position of relevant information) on the multi-document question
answering task decreases as the input context grows longer.(Image
source: Fig. 6 in <a href="#ref-liu-etal:2023:arxiv">[1]</a>)</figcaption>
</figure>
<p>실험 결과, 정답이 포함된 문서가 input의 첫번째 혹은 끝에 주어졌을때
accuracy가 높은 U자형 그래프가 나타났으며(Fig. <a data-reference="fig:qa_results" data-reference-type="ref" href="#fig:qa_results">1</a>), input
context 길이가 길어짐에 따라 accuracy가 떨어지는 경향을 보였다(Fig. <a data-reference="fig:qa_context_length_results" data-reference-type="ref" href="#fig:qa_context_length_results">2</a>).</p>
<h2 id="key-value-retrieval">Key-Value Retrieval</h2>
<p>Key-Value Retrieval은 랜덤하게 생성된 UUID를 키와 값으로 갖는 JSON
데이터를 input으로 주고, 키를 통해 값을 얻을 수 있는지 확인하는
task이다. 즉, 기본적인것은 Multi-doc QA와 같지만 문서가 아닌 JSON
데이터를 사용한다는 점이 다르다.<br/>
실험 결과, Claude처럼 거의 완벽하게 정답을 맞춘 모델을 제외하면
Multi-doc QA와 비슷한 경향을 보였다. 즉, 위치에 따른 accuracy는 U자형
그래프를, input context 길이에 따른 accuracy는 context가 길어질수록
감소하는 경향을 보였다.</p>
<h2 id="why-do-language-models-struggle-to-use-their-entire-input-context">Why
Do Language Models Struggle To Use Their Entire Input Context?</h2>
<p>저자들은 왜 언어모델이 input context를 전부 활용하지 못하는지
알아보기 위해 추가 실험을 진행했다.</p>
<h3 id="effect-of-model-architecture">Effect of Model Architecture</h3>
<p>실험에 사용된 Open Model은 모두 decoder-only 모델이었으므로
encoder-decoder 모델을 사용해 결과를 비교해보았다. 실험 결과,
encoder-decoder 모델은 input이 training time의 context window 이내라면
정답문서의 위치에 따른 accuracy의 변화가 상대적으로 적었지만 이를
벗어났을 경우 전과 같은 경향(U자형 그래프)을 보였다.</p>
<h3 id="effect-of-query-aware-contextualization">Effect of Query-Aware
Contextualization</h3>
<p>Query-aware contextualization은 query(질문)의 위치에 따라 변화가
있는지 확인을 한것이다. 이전의 실험들은 모두 질문이 prompt의 맨 끝에
있었는데, 이럴 경우 decoder-only 모델들은 문서를 읽으며 질문을 확인할 수
없다.<br/>
Query의 위치를 바꿔가며 실험을 진행한 결과, key-value retrieval
task에서는 큰 성능 향상을 보였지만 multi-doc QA에선 큰 변화가
없었다.</p>
<h3 id="effect-of-instruction-tuning">Effect of Instruction-Tuning</h3>
<p>이전실험에 쓰였던 모델들은 모두 instruction-tuning이 된
모델들이였는데, 저자들은 혹시 이 instruction tuning 과정이 이런 결과를
만들었는지 실험을 하였다. Instruction tuning에선 보통 task
specification이 앞에 위치하는 경우가 많기 때문이다.<br/>
실험 결과, instruction tuning을 하지 않은 모델의 경우도 마찬가지로 U자형
그래프를 보였다.</p>
<h2 id="is-more-context-always-better">Is More Context Always
Better?</h2>
<p>언어 모델들은 context 길이가 길 경우 더 많은 정보를 처리할 수 있지만,
그만큼 비용이 들게 된다. 이 실험은 이런 context 길이에 따른 trade-off
관계를 알 수 있게 해준다.<br/>
실험 결과 context가 길어질수록 향상되는 성능은 미미하지만, 그에 따른
비용은 크게 증가하였다.</p>
<h2 id="quick-recap">Quick Recap</h2>
<ol>
<li><p>Input context 길이가 길어질수록 성능이 떨어지는 경향을
보인다.</p></li>
<li><p>위치에 따른 성능변화는 U자형 그래프를 보인다.</p></li>
<li><p>Context 길이에 따른 trade-off를 고려할때, retrieved context는
re-ranking 혹은 truncation을 통해 사용하는 것이 좋다.</p></li>
</ol>
<div class="references csl-bib-body hanging-indent" id="refs" role="list" style="margin-bottom: 2rem"><h2 style="margin-top: 4rem">References</h2>
<div class="csl-entry" id="ref-liu-etal:2023:arxiv" role="listitem">[1] 
Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
Bevilacqua, Fabio Petroni, and Percy Liang. 2023. <span>“<a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language
Models Use Long Contexts</a>.”</span>
</div>
</div>
</body></html><!-- HTML_TAG_END --> </div> 
			
			<script>
				{
					__sveltekit_mtp1rx = {
						base: new URL(".", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{content:"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>언어모델의 input context 길이는 그 모델이 얼마나 긴 input을 한번에\n처리할 수 있는지를 보여준다. 이 길이가 짧다면 여러 제약이 생기기\n마련인데(예를 들어, 문서 요약 작업 등에서 context 길이를 초과하는 문서는\n처리를 할 수 없어 요약을 할 수 없다. 이를 처리하기 위해선 별도의 우회\n작업을 해야 한다.) 최근의 여러 언어모델들은 이런 제약을 극복하기 위해\ncontext 길이를 늘리는 추세이다.\u003Cbr/>\n그런데 이렇게 context 길이가 늘어나면 늘어나는대로 모든 context를 처리할\n수 있는걸까? Context가 긴 경우, 찾고자 하는 정보가 어느 위치에 있던지\n상관없이 모두 동일한 성능을 보일까?\u003Cbr/>\n\u003Ca href=\"#ref-liu-etal:2023:arxiv\">[1]\u003C/a>은 이를\n알아보기 위해 여러 실험을 하여 그 결과를 소개한 논문이다.\u003C/p>\n\u003Ch2 id=\"experimental-setup\">Experimental Setup\u003C/h2>\n\u003Ch3 class=\"unnumbered\" id=\"tasks\">Tasks\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cp>Multi-Document Question Answering Task\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Key-Value Retrieval Task\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Effect of Model Architecture\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Effect of Query-Aware Contextualization\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Effect of Instruction-Tuning\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Is More Context Is Always Better?\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>1과 2는 input context가 늘어감에 따라 accuracy가 어떻게 변하는지,\n주어진 정답이 input context의 어디에 위치하는지에 따라 accuracy가 어떻게\n변하는지를 보는 실험이다.\u003Cbr/>\n3, 4, 5는 왜 모델이 input context를 전부 활용하지 못하는지 알아보기 위한\n추가 실험이다.\u003Cbr/>\n6은 context가 긴게 항상 더 나은지를 알아보기위한 추가 실험이다.\u003C/p>\n\u003Ch3 class=\"unnumbered\" id=\"models\">Models\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cp>MPT-30B-Instruct, MPT-30B-Base(Instruction-Tuning\n실험에서)\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LongChat-13B(16K)\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>GPT-3.5-Turbo, GPT-3.5-Turbo(16K)\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Claude-1.3, Claude-1.3(100K)\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>1과 2는 Open Model, 3과 4는 Closed Model이다.\u003C/p>\n\u003Ch2 id=\"multi-document-question-answering\">Multi-Document Question\nAnswering\u003C/h2>\n\u003Cp>Multi-doc QA는 말 그대로 여러 문서를 input으로 주고 정답을 찾아내는\n실험이다. 구체적으론, 정답을 포함한 한개의 문서와 정답을 포함하지 않지만\n연관성이 있는 \u003Cspan class=\"math inline\">\\(k - 1\\)\u003C/span>개의 문서를\ninput으로 주고 실험을 진행한다.\u003C/p>\n\u003Cfigure id=\"fig:qa_results\">\n\u003Cimg src=\"figures/lost-in-the-middle-how-language-models-use-long-contexts/qa.jpeg\" style=\"width:100.0%\"/>\n\u003Cfigcaption>Figure 1: The effect of changing the position of relevant\ninformation (document containing the answer) on multi-document question\nanswering performance. Lower positions are closer to the start of the\ninput context. Performance is generally highest when relevant\ninformation is positioned at the very start or very end of the context,\nand rapidly degrades when models must reason over information in the\nmiddle of their input context. (Image source: Fig. 5 in \u003Ca href=\"#ref-liu-etal:2023:arxiv\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cfigure id=\"fig:qa_context_length_results\">\n\u003Cimg src=\"figures/lost-in-the-middle-how-language-models-use-long-contexts/qa_context_length.jpeg\" style=\"width:40.0%\"/>\n\u003Cfigcaption>Figure 2: Language model performance (averaged across\nposition of relevant information) on the multi-document question\nanswering task decreases as the input context grows longer.(Image\nsource: Fig. 6 in \u003Ca href=\"#ref-liu-etal:2023:arxiv\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>실험 결과, 정답이 포함된 문서가 input의 첫번째 혹은 끝에 주어졌을때\naccuracy가 높은 U자형 그래프가 나타났으며(Fig. \u003Ca data-reference=\"fig:qa_results\" data-reference-type=\"ref\" href=\"#fig:qa_results\">1\u003C/a>), input\ncontext 길이가 길어짐에 따라 accuracy가 떨어지는 경향을 보였다(Fig. \u003Ca data-reference=\"fig:qa_context_length_results\" data-reference-type=\"ref\" href=\"#fig:qa_context_length_results\">2\u003C/a>).\u003C/p>\n\u003Ch2 id=\"key-value-retrieval\">Key-Value Retrieval\u003C/h2>\n\u003Cp>Key-Value Retrieval은 랜덤하게 생성된 UUID를 키와 값으로 갖는 JSON\n데이터를 input으로 주고, 키를 통해 값을 얻을 수 있는지 확인하는\ntask이다. 즉, 기본적인것은 Multi-doc QA와 같지만 문서가 아닌 JSON\n데이터를 사용한다는 점이 다르다.\u003Cbr/>\n실험 결과, Claude처럼 거의 완벽하게 정답을 맞춘 모델을 제외하면\nMulti-doc QA와 비슷한 경향을 보였다. 즉, 위치에 따른 accuracy는 U자형\n그래프를, input context 길이에 따른 accuracy는 context가 길어질수록\n감소하는 경향을 보였다.\u003C/p>\n\u003Ch2 id=\"why-do-language-models-struggle-to-use-their-entire-input-context\">Why\nDo Language Models Struggle To Use Their Entire Input Context?\u003C/h2>\n\u003Cp>저자들은 왜 언어모델이 input context를 전부 활용하지 못하는지\n알아보기 위해 추가 실험을 진행했다.\u003C/p>\n\u003Ch3 id=\"effect-of-model-architecture\">Effect of Model Architecture\u003C/h3>\n\u003Cp>실험에 사용된 Open Model은 모두 decoder-only 모델이었으므로\nencoder-decoder 모델을 사용해 결과를 비교해보았다. 실험 결과,\nencoder-decoder 모델은 input이 training time의 context window 이내라면\n정답문서의 위치에 따른 accuracy의 변화가 상대적으로 적었지만 이를\n벗어났을 경우 전과 같은 경향(U자형 그래프)을 보였다.\u003C/p>\n\u003Ch3 id=\"effect-of-query-aware-contextualization\">Effect of Query-Aware\nContextualization\u003C/h3>\n\u003Cp>Query-aware contextualization은 query(질문)의 위치에 따라 변화가\n있는지 확인을 한것이다. 이전의 실험들은 모두 질문이 prompt의 맨 끝에\n있었는데, 이럴 경우 decoder-only 모델들은 문서를 읽으며 질문을 확인할 수\n없다.\u003Cbr/>\nQuery의 위치를 바꿔가며 실험을 진행한 결과, key-value retrieval\ntask에서는 큰 성능 향상을 보였지만 multi-doc QA에선 큰 변화가\n없었다.\u003C/p>\n\u003Ch3 id=\"effect-of-instruction-tuning\">Effect of Instruction-Tuning\u003C/h3>\n\u003Cp>이전실험에 쓰였던 모델들은 모두 instruction-tuning이 된\n모델들이였는데, 저자들은 혹시 이 instruction tuning 과정이 이런 결과를\n만들었는지 실험을 하였다. Instruction tuning에선 보통 task\nspecification이 앞에 위치하는 경우가 많기 때문이다.\u003Cbr/>\n실험 결과, instruction tuning을 하지 않은 모델의 경우도 마찬가지로 U자형\n그래프를 보였다.\u003C/p>\n\u003Ch2 id=\"is-more-context-always-better\">Is More Context Always\nBetter?\u003C/h2>\n\u003Cp>언어 모델들은 context 길이가 길 경우 더 많은 정보를 처리할 수 있지만,\n그만큼 비용이 들게 된다. 이 실험은 이런 context 길이에 따른 trade-off\n관계를 알 수 있게 해준다.\u003Cbr/>\n실험 결과 context가 길어질수록 향상되는 성능은 미미하지만, 그에 따른\n비용은 크게 증가하였다.\u003C/p>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>Input context 길이가 길어질수록 성능이 떨어지는 경향을\n보인다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>위치에 따른 성능변화는 U자형 그래프를 보인다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Context 길이에 따른 trade-off를 고려할때, retrieved context는\nre-ranking 혹은 truncation을 통해 사용하는 것이 좋다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-liu-etal:2023:arxiv\" role=\"listitem\">[1] \nLiu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele\nBevilacqua, Fabio Petroni, and Percy Liang. 2023. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2307.03172\">Lost in the Middle: How Language\nModels Use Long Contexts\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"},"uses":{}}];

					Promise.all([
						import("./_app/immutable/entry/start.63a02e9c.js"),
						import("./_app/immutable/entry/app.b8702bb8.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 8],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>