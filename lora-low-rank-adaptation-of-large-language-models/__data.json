{"type":"data","nodes":[null,{"type":"data","data":[{"content":1},"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>현재 언어모델의 패러다임은 pre-trained language model을 task에 맞춰\nfine-tuning하여 사용하는 것이다. 각 task에 맞춰 처음부터 모델을\n학습시키는 것보다 훨씬 적은 비용으로 높은 성능을 얻을 수 있기 때문이다.\n그러나 \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a> 이전까지의 fine-tuning\n방법들은 비용이 많이 들거나, inference latency가 증가하거나,\nbaseline보다 성능이 떨어지기도 한다던지의 문제가 있었다.\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>은\n이런 문제를 해결하는 새로운 fine-tuning 방법(LoRA)을 제시한\n논문이다.\u003C/p>\n\u003Ch2 id=\"problem-statement\">Problem Statement\u003C/h2>\n\u003Cp>LORA는 언어모델뿐 아니라 어떤 모델에도 적용이 가능하지만 \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>에서는\n간단히 언어모델에 국한하였다.\u003C/p>\n\u003Cp>\u003Cspan class=\"math inline\">\\(\\Phi\\)\u003C/span>로 parameterized된\npre-trained autoregressive language model \u003Cspan class=\"math inline\">\\(P_\\Phi(y|x)\\)\u003C/span>가 주어졌다고 하자. Task를\ntraining dataset \u003Cspan class=\"math inline\">\\(\\mathbf{Z} =  \\{(x_i,\ny_i)\\}_{i=1,..,N}\\)\u003C/span> (\u003Cspan class=\"math inline\">\\(x_i\\)\u003C/span>와\n\u003Cspan class=\"math inline\">\\(y_i\\)\u003C/span>는 토큰 sequence)로 표현한다면\nfull fine-tuning은 다음과 같이 나타낼 수 있다.\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[\\max_{\\Phi}\n\\sum_{(x,y)\\in\\mathbf{Z}} \\sum_{t=1}^{|y|}  \\text{log}\n\\left(  P_{\\Phi}(y_{t} | x, y_{&lt;t})\n\\right)\\qquad{(1)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>LoRA는 parameter-efficient한 방법으로 \u003Cspan class=\"math inline\">\\(|\\Theta| \\ll |\\Phi_0|\\)\u003C/span> 인 파라미터 \u003Cspan class=\"math inline\">\\(\\Theta\\)\u003C/span>만 신경쓰면 된다. 즉, \u003Cspan class=\"math inline\">\\(\\Phi_0\\)\u003C/span>을 pre-trained weight이라 할 때\n다음과 같이 나타낼 수 있다.\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[\\begin{aligned}\n  \\max_{\\Theta}\n\\sum_{(x,y)\\in\\mathbf{Z}}  \\sum_{t=1}^{|y|}  \\log\\left({p_{\\Phi_0+\\Delta\\Phi(\\Theta)}(y_{t}\n| x, y_{&lt;t}})\\right)\n\\end{aligned}\\qquad{(2)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Ch2 id=\"existing-solutions\">Existing Solutions\u003C/h2>\n\u003Cp>기존의 방식들은 다음과 같은 단점이 있다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Adapter\u003C/strong>\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>Adapter layer의 extra computation을 bypass할 방법이 없다. 즉,\ninference latency가 증가한다.\u003C/p>\u003C/li>\n\u003C/ol>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Prefix tuning\u003C/strong>\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>최적화가 어렵다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Sequence length의 희생이 따른다.\u003C/p>\u003C/li>\n\u003C/ol>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"lora\">LoRA\u003C/h2>\n\u003Ch3 id=\"motivation\">Motivation\u003C/h3>\n\u003Cp>LoRA는 weight의 update가 low \"intrinsic rank\"를 가지고 있다는 가정을\n기반으로 한다.\u003C/p>\n\u003Cp>Pre-trained weight matrix \u003Cspan class=\"math inline\">\\(W_0\\in\n\\mathbb{R}^{d\\times k}\\)\u003C/span>에 대해 그 update를 low-rank\ndecomposition을 이용해 나타내면 다음과 같다. \u003Cspan>\u003Cspan class=\"math display\">\\[W_0+\\Delta W=W_0+BA \\left(B\\in\n\\mathbb{R}^{d\\times r}, A\\in \\mathbb{R}^{r\\times\nk}\\right)\\qquad{(3)}\\]\u003C/span>\u003C/span> 이때 rank \u003Cspan class=\"math inline\">\\(r \\ll \\min(d,k)\\)\u003C/span>이다. 학습동안 \u003Cspan class=\"math inline\">\\(W_0\\)\u003C/span>는 frozen되어 있고, \u003Cspan class=\"math inline\">\\(A\\)\u003C/span>와 \u003Cspan class=\"math inline\">\\(B\\)\u003C/span>만 학습된다. 이에 맞춰 바뀐 forward\npass는 다음과 같다.\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[h = W_0 x + \\Delta W x = W_0 x +\nBA x\\qquad{(4)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>Fig. \u003Ca data-reference=\"fig:figure1\" data-reference-type=\"ref\" href=\"#fig:figure1\">1\u003C/a>이 이를 나타내고 있다. \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>에선 A를\nrandom Gaussian initialization로, B는 0으로 초기화시켰다. \u003Cspan class=\"math inline\">\\(\\Delta W x\\)\u003C/span>는 상수 \u003Cspan class=\"math inline\">\\(\\alpha\\)\u003C/span>에 대해 \u003Cspan class=\"math inline\">\\(\\frac{\\alpha}{r}\\)\u003C/span>로 scale하였다.\u003C/p>\n\u003Cfigure id=\"fig:figure1\">\n\u003Cimg src=\"figures/lora-low-rank-adaptation-of-large-language-models/figure1.jpg\" style=\"width:40.0%\"/>\n\u003Cfigcaption>Figure 1: Reparametrization of LoRA. (Image source: Fig. 1\nin \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>LoRA는 다음과 같은 장점을 가진다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Generalization of Full Fine-tuning\u003C/strong> LoRA rank\n\u003Cspan class=\"math inline\">\\(r\\)\u003C/span>의 설정에 따라 full fine-tuning에\n근접할 수도 있다(다양한 fine-tuning이 가능하다).\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>No Additional Inference Latency\u003C/strong> 모델의 배포때\n더해진 weight, 즉, \u003Cspan class=\"math inline\">\\(W = W_0 + BA\\)\u003C/span>로\n사용하면 추가 latency가 없다. 원래의 weight으로 돌리고 싶을땐 단순히\n다시 빼주면 된다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Modularity\u003C/strong> LoRA는 각 downstream task에 맞춰 만들\n수 있다. 즉, 각각의 task에 맞춰 모듈식으로 갈아끼워가며 사용할 수\n있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>단점으로는 더해진 형태로 weight을 쓰게 되면 동시에 여러 LoRA를 사용할\n수 없다는 점 등이 있다.\u003C/p>\n\u003Ch2 id=\"empirical-experiments\">Empirical Experiments\u003C/h2>\n\u003Cfigure id=\"fig:lora_wikisql\">\n\u003Cimg src=\"figures/lora-low-rank-adaptation-of-large-language-models/LoRA_wikisql.jpg\" style=\"width:100.0%\"/>\n\u003Cfigcaption>Figure 2: GPT-3 175B validation accuracy vs. number of\ntrainable parameters of several adaptation methods on WikiSQL and\nMNLI-matched. LoRA exhibits better scalability and task performance.\n(Image source: Fig. 2 in \u003Ca href=\"#ref-hu2021lora\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>성능은 대부분의 결과에서 full fine-tuning에 근접하거나 더 뛰어나며\n다른 방법들을 앞지르는 모습을 보여준다. Fig. \u003Ca data-reference=\"fig:lora_wikisql\" data-reference-type=\"ref\" href=\"#fig:lora_wikisql\">2\u003C/a>에\nGPT-3 175B에 대한 대략적인 실험결과가 나타나 있다. 조심해야할것은 Fig.\n\u003Ca data-reference=\"fig:lora_wikisql\" data-reference-type=\"ref\" href=\"#fig:lora_wikisql\">2\u003C/a>에 나와있듯이 trainable\nparameter를 증가시키는 것이 꼭 성능을 향상시키는 것은 아니라는\n점이다.\u003C/p>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cspan class=\"math inline\">\\(W_0+\\Delta W=W_0+BA\\)\u003C/span>로\nfine-tuning을 할 수 있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Inference latency가 없게 만들 수 있고, 모듈식 사용이\n가능하다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Memory와 Storage를 절약할 수 있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>성능은 대개 full fine-tuning에 비견될만큼 뛰어나다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-hu2021lora\" role=\"listitem\">[1] \nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen. 2021. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of\nLarge Language Models\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"],"uses":{}}]}
