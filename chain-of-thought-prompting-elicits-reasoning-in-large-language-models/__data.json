{"type":"data","nodes":[null,{"type":"data","data":[{"content":1},"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>Language model의 scale을 키우는 것은 여러 방면으로 성능 향상에 도움이\n된다. 그러나 arithmetic, commonsense, symbolic reasoning같은 작업에선\nscale을 키우는것 단독으론 충분하지 않다.\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-wei2023chainofthought\">[1]\u003C/a>은\nChain-of-Thought라는 기법을 제안한다. 이 기법은 prompt를 통해 language\nmodel의 reasoning 성능을 향상시킨다.\u003C/p>\n\u003Ch2 id=\"chain-of-thought-prompting\">Chain-of-Thought Prompting\u003C/h2>\n\u003Cfigure id=\"fig:pull-figure\">\n\u003Cimg src=\"figures/chain-of-thought-prompting-elicits-reasoning-in-large-language-models/new-pull-figure-landscape.jpg\" style=\"width:100.0%\"/>\n\u003Cfigcaption>Figure 1: Chain-of-thought prompting enables large language\nmodels to tackle complex arithmetic, commonsense, and symbolic reasoning\ntasks. Chain-of-thought reasoning processes are highlighted. (Source:\nFig. 1 in \u003Ca href=\"#ref-wei2023chainofthought\">[1]\u003C/a>)\n\u003C/figcaption>\n\u003C/figure>\n\u003Cp>Chain of Thought는 다음 두가지 아이디어에 기반한다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>정답에 대한 근거가 주어지면 reasoning 작업을 수행하는데 도움이\n된다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM은 prompting을 통해 in-context few-shot learning을 수행할 수\n있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>이 아이디어를 통합한것이 chain of thought로, Fig. \u003Ca data-reference=\"fig:pull-figure\" data-reference-type=\"ref\" href=\"#fig:pull-figure\">1\u003C/a>에서 볼 수 있듯이 최종 결과를\n내기전 reasoning step을 밟아갈 수 있도록 prompt를 구성한다.\u003C/p>\n\u003Cp>Chain of Thought의 장점은 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>문제를 여러 step을 거쳐나가며 풀도록 함으로써 computation을 더\n사용할 수 있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>정답에 도달한 과정을 볼 수 있으며 이를 통해 reasoning의 어떤 점이\n틀렸는지 debugging이 가능하다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Arithmetic, commmonsense, symbolic reasoning 등 사람이 언어적으로\n해결할 수 있는 모든 문제에 적용 가능하다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>단지 적절한 크기의 LLM에 prompting을 하는것만으로 적용이\n가능하다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>\u003Ca href=\"#ref-wei2023chainofthought\">[1]\u003C/a>은\nchain of thought를 적용하여 arithmetic, commonsense, symbolic reasoning\n3가지에 대한 실험을 수행하였고 그 결과를 담았다.\u003C/p>\n\u003Ch2 id=\"arithmetic-reasoning\">Arithmetic Reasoning\u003C/h2>\n\u003Ch3 id=\"experimental-setup\">Experimental Setup\u003C/h3>\n\u003Cp>실험 환경은 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Benchmarks\u003C/strong>: GSM8K, SVAMP, ASDiv, AQuA, MAWPS\n다섯가지이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Standard prompting\u003C/strong>: Baseline으로 standard\nfew-shot prompting을 사용한다. 이는 Fig. \u003Ca data-reference=\"fig:pull-figure\" data-reference-type=\"ref\" href=\"#fig:pull-figure\">1\u003C/a>의\n왼쪽과 같이 example을 제공하되 reasoning이 아닌 정답만 바로 제공하는\n식의 prompting이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Chain-of-thought prompting\u003C/strong>: Fig. \u003Ca data-reference=\"fig:pull-figure\" data-reference-type=\"ref\" href=\"#fig:pull-figure\">1\u003C/a>의 오른쪽과 같이 reasoning step을\n거쳐가며 정답을 찾도록 prompting을 구성한다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Language models\u003C/strong>: GPT-3, LaMDA, PaLM, UL2 20B,\nCodex 다섯가지이다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"results\">Results\u003C/h3>\n\u003Cp>주요 결과는 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>Chain-of-thought는 small model에선 효과적이지 않았으며 큰\n모델에서만 효과가 있었다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Chain-of-thought는 GSM8K같이 복잡한 문제일수록 더 효과적이었다.\nMAWPS같이 single step으로 풀 수 있는 문제에서는 이득이 없었다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Chain-of-thought의 성능은 이전까지의 SoTA를 뛰어넘을 정도로\n좋았다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM의 scale을 키우는것은 chain-of-thought를 쓸때 one-step\nmissing이나 semantic understanding error같은 에러를 내는것을 크게\n줄여준다. 이는 Scaling이 왜 효과적인지를 어느정도 설명해준다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"ablation-study\">Ablation Study\u003C/h3>\n\u003Cp>이런 성능향상이 다른것에서 기인하는것은 아닌지 확인하기 위해 ablation\nstudy를 수행하였다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Equation only\u003C/strong>: Chain-of-thought가 equation을\n제공하는것에 의해 성능이 향상되는것은 아닌지 확인하기 위해 equation만\n제공하는 prompt를 구성하였다. GSM8K같은 복잡한 문제에선 성능향상이\n없었지만 one-step problem에선 어느정도 성능이 향상되었다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Variable compute only\u003C/strong>: Computation을 더 많이\n해서 그런것은 아닌지 확인하기위해 dots(...)로 구성된 sequence를 문제를\n풀기위한 equation과 같은 길이만큼 주었다. 이때의 성능은 baseline과 같아\n성능향상이 없는것을 확인할 수 있었다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Chain of thought after answer\u003C/strong>: 혹시 모델이\n단순히 제공된 prompt로 인해 더 연관된 정보를 잘 찾았던 것은 아닌지\n확인하기 위해 순서를 바꿔 chain of thought를 정답 이후에 주었다. 이 역시\nbaseline과 같은 성능을 보여 reasoning의 순서가 중요함을\n보여주었다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"robustness-of-chain-of-thought\">Robustness of Chain of\nThought\u003C/h3>\n\u003Cp>Chain of thought가 prompt에 얼마나 sensitive한지 확인하기위한 실험을\n수행하였다. 여러사람이 각각 chain of thought를 쓰게하고 그 결과를 확인한\n결과, prompt에 따라 어느정도 성능의 차이는 있지만 전부 standard\nbaseline보다 크게 높은 성능을 보여주었다. 즉, annotator에 대해\nrobust하다는 것을 보여주었다. Exemplars, language models 등의 실험에\n대해서도 마찬가지로 robust함을 보여주었다.\u003C/p>\n\u003Ch2 id=\"commonsense-reasoning\">Commonsense Reasoning\u003C/h2>\n\u003Cp>Arithmetic에서와 비슷한 경향을 보여주었다\u003C/p>\n\u003Ch2 id=\"symbolic-reasoning\">Symbolic Reasoning\u003C/h2>\n\u003Cp>역시 비슷한 경향을 보여주었다. 추가로, few-shot exemplar에서 본것보다\n긴 input에 대해서도 inference-time에서 generalization이 가능하다는것을\n보여주었다.\u003C/p>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>Chain-of-thought는 reasoning step을 거쳐가도록 prompting하는\n방식이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Chain-of-thought는 단순 few-shot prompting보다 성능이\n좋다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Chain-of-thought는 single-step 문제 등 간단한 문제에선 효과가\n없을 수 있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Chain-of-thought는 작은 모델에선 효과가 없을 수 있으며 scale이\n커질수록 효과가 있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-wei2023chainofthought\" role=\"listitem\">[1] \nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\nFei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2201.11903\">Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"],"uses":{}}]}
