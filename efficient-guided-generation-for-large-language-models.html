<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
	<link rel="manifest" href="./favicon/site.webmanifest">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<script>
		MathJax = {
			loader: { load: ['[tex]/textmacros'] },
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				packages: { '[+]': ['textmacros'] },
			},
		};
	</script>
	<script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js">
		</script>
	
		<link href="./_app/immutable/assets/0.08c9bd5d.css" rel="stylesheet">
		<link href="./_app/immutable/assets/3.15287e86.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.1162bf57.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.e108d1fd.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/singletons.caa5cb96.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.c39bda2d.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.38a092e6.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.3e7173ef.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/5.fd7d7f74.js"><title>First Read - Efficient Guided Generation for Large Language Models</title><!-- HEAD_svelte-1k12xm0_START --><!-- HEAD_svelte-1k12xm0_END -->
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">  <div class="nav svelte-5baz1e" data-svelte-h="svelte-89nan6"><a href="/">Posts</a></div>  <div class="content svelte-lwo4ch"><div class="title svelte-lwo4ch" data-svelte-h="svelte-151csjy"><h1 class="svelte-lwo4ch">First Read</h1> <h2 class="svelte-lwo4ch">Efficient Guided Generation for Large Language Models</h2> <div class="date svelte-lwo4ch">2023-09-07</div></div> <!-- HTML_TAG_START --><html><body><h2 id="overview">Overview</h2>
<p>언어모델을 이용해 작업을 한다면 문서 요약결과를 JSON으로 출력한다거나
하는 등 우리가 원하는 형식으로 output을 내고싶을 때가 종종 있다. 하지만
이런 output의 형식을 제한하는 것은 단순한 프롬프팅이나 fine-tuning만으로
해결하기가 쉽지 않다. <a href="#ref-willard2023efficient">[1]</a>에선
이를 FSM을 통해 효율적으로 해결하는 방법을 제시한다.</p>
<h2 id="method">Method</h2>
<p>Guided generation의 핵심은 output을 낼 때 vocabulary의 token별 확률을
조정하는 것이다(예를 들어, vocabulary가 ["I’m", "my", "his", "hi"]로
구성돼있을때 "hi" 다음 올 token의 확률이 [0.8, 0.1, 0.05, 0.05]라면 이를
[0, 1, 0, 0] 이런 식으로 조정). 이런 방법은 이전에도 있어왔는데,
이전까지의 방법들은 runtime에서 그때그때마다(token을 output으로
내놓을때마다) 조정을 하는 방법으로 O(N)의 시간복잡도를 가졌다(N은
vocabulary 크기).</p>
<p><a href="#ref-willard2023efficient">[1]</a>에선
이를 FSM을 통해 각 state별 token의 확률을 미리 조정하여 저장하고, 이를
이용해 output을 내놓는 방법을 제시한다. 이를 통해 O(N)의 runtime
시간복잡도를 평균 O(1)로 줄일 수 있다(FSM을 이용한 memoization이라고
보면 됨).</p>
<p>이 방법은 CFG 등으로 확장이 가능해 기존의 여러 parser들과 연계가
가능하다.</p>
<h3 id="llm-sampling-and-guided-generation">LLM Sampling and Guided
Generation</h3>
<p>우선 일반적인 언어모델의 샘플링 방법을 살펴보자. <span class="math inline">\(\mathcal{V}\)</span>가 vocabulary이고 <span class="math inline">\(|\mathcal{V}| = N\)</span> 일때 <span class="math inline">\(t\)</span> 토큰 sequence <span class="math inline">\(S_t = \left(s_1 \dots s_t\right)\)</span>에 대해
<span class="math inline">\(s_i \in \mathcal{V}\)</span>라고 하자. <span class="math inline">\(\mathcal{V}\)</span>는 fixed alphabet으로부터
만들어진 string으로 구성돼있고 <span class="math inline">\(N\)</span>은
보통 <span class="math inline">\(10^4\)</span> 보다 크거나 같다.</p>
<p>다음 토큰 <span class="math inline">\(s_{t+1}\)</span>은 아래와 같은
random variable로 정의된다:</p>
<p><span><span class="math display">\[\begin{aligned}
  \boldsymbol{\alpha} &amp; = \operatorname{LLM}(S_t,
\boldsymbol{\theta})         \\
  s_{t+1}             &amp; \sim
\operatorname{Categorical}({\boldsymbol{\alpha}})
\end{aligned}\qquad{(1)}\]</span></span></p>
<p>이때 <span class="math inline">\(\boldsymbol{\theta}\)</span>는 LLM의
파라미터이고 <span class="math inline">\(\boldsymbol{\alpha} \in
\mathbb{R}^N\)</span>이다.</p>
<p>Algorithm <a data-reference="alg:llm-sequence-sampling" data-reference-type="ref" href="#alg:llm-sequence-sampling">1</a>은
이 샘플링 과정을 나타낸다.</p>
<figure id="alg:llm-sequence-sampling">
<img alt="Algorithm 1" src="figures/efficient-guided-generation-for-large-language-models/_13a1__algorithm1.png" style="width:100.0%"/>
<figcaption><p>Algorithm 1: Basic LLM token sampling(Source: Algorithm 1 in <a href="#ref-willard2023efficient">[1]</a>)</p></figcaption>
</figure>
<p>이제 output logit <span class="math inline">\(\boldsymbol{\alpha}\)</span>를 조정하여 Guided
generation을 하는 방법을 살펴보자. 이는 위의 샘플링 과정에 boolean mask
<span class="math inline">\(m: \mathcal{P}\left(\mathcal{V}\right) \to
\left\{0, 1\right\}^N\)</span> 을 추가하면 된다. 식은 다음과 같다:</p>
<p><span><span class="math display">\[\begin{aligned}
  \boldsymbol{\alpha}         &amp; = \operatorname{LM}(\tilde{S}_t,
\boldsymbol{\theta})                \\
  \tilde{\boldsymbol{\alpha}} &amp; =
\operatorname{m}\left(\tilde{S}_t\right) \odot \boldsymbol{\alpha} \\
  \tilde{s}_{t+1}             &amp; \sim
\operatorname{Categorical}({\tilde{\boldsymbol{\alpha}}})
\end{aligned}\qquad{(2)}\]</span></span></p>
<p>이때 이 마스킹 과정을 runtime에서 해버리면 O(N)의 시간복잡도를 가지게
된다. 이를 효율적으로 하기 위해 <a href="#ref-willard2023efficient">[1]</a>에선
FSM을 이용한다.</p>
<h3 id="iterative-fsm-processing-and-indexing">Iterative FSM Processing
and Indexing</h3>
<p>기본골자는 다음과 같다. 어떤 regex로 표현될 수 있는 문자열이 있다면,
이는 FSM으로 변환이 가능하고, 이 FSM의 각 state별로 다음 transition에
대해 가능한 token의 확률을 조정하여 저장해놓는다(각각의 vocab에 대한
마스킹을 미리 해놓는다). 이를 통해 runtime에서는 각 state별로 마스킹된
logit을 이용해 샘플링을 하면 되고, hash map 등을 이용하면 평균 O(1)의
시간복잡도를 가지게 된다. 다음 알고리즘이 이를 표현한 것이다.</p>
<figure id="alg:fsm-sub-sequences">
<img alt="Algorithm 2" src="figures/efficient-guided-generation-for-large-language-models/_13a1__algorithm2.png" style="width:100.0%"/>
<figcaption><p>Algorithm 2: Find sub-sequences of the FSM <span class="math inline">\(M\)</span> that accept the string <span class="math inline">\(\boldsymbol{v}\)</span> (Source: Algorithm 3 in
<a href="#ref-willard2023efficient">[1]</a>)</p></figcaption>
</figure>
<h3 id="extensions-to-iterative-parsing">Extensions to Iterative
Parsing</h3>
<p>이 방법은 PDA를 써서 CFG 등으로 확장이 가능하며 이를 통해 기존의 여러
parser들과 연계가 가능하다. 기본적인 진행은 이전과 유사하다.</p>
<h3 id="limitations">Limitations</h3>
<p>개인적으로 이 방법이 가지는 한계는 다음과 같다고 생각한다.</p>
<ol>
<li><p>결국 vocab size가 크면 쉽게 적용할만하지 않다.</p></li>
<li><p>언어모델 외부에선(예를 들어, 출력된 결과만으론) 적용이
불가능하다(직접 logit을 조정해야함).</p></li>
<li><p>여전히 원하지 않는 결과를 얻게 될 수도 있다(예를 들어, 어떤
string 값을 갖는 JSON을 생성할때 그 string에서 계속(context를 잊어먹어
다음 단계로 가지않고) 생성이 진행되는 경우, 글자 제한 등으로 이런 경우를
막으려고 한다면 그 제한안에 우리가 원하는 내용이 들어갈지는 확신할 수
없다).</p></li>
<li><p>단순 마스킹은 언어모델이 처음부터 output의 포맷을 고려하여
진행했을때와 확률자체가 다를 수 있다. 예를 들어 마스킹을 한 경우 첫번째
인덱스를 답으로 뱉을 수 있지만, 처음부터 어떤 포맷이될지 언어모델이 미리
알고 진행했다면 두번째 인덱스가 답이 될 수도 있다.</p></li>
</ol>
<p>하지만 이런 한계에도 불구하고 이 방법을 training이나 fine tuning 때
이용하여 학습을 진행한다던지, masking을 언어모델 내부적으로 구현하여
inference때 쓸데 없는 뉴런을 거치지 않게 만들어 cost를 아낀다던지 등의
부가적인 장점 역시 존재한다.</p>
<h2 id="quick-recap">Quick Recap</h2>
<ol>
<li><p>FSM과 memoization같은 방법을 활용해 logit을 masking한다면 평균
O(1)의 시간복잡도로 guided generation을 할 수 있다.</p></li>
<li><p>이 방법은 CFG 등으로 확장이 가능하며 기존의 여러 parser들과
연계가 가능하다.</p></li>
<li><p>언어모델 내부적으로 마스킹을 구현한다면 inference때 쓸데 없는
뉴런을 거치지 않게 만들어 cost를 아끼게 만들 수 있다.</p></li>
</ol>
<div class="references csl-bib-body hanging-indent" id="refs" role="list" style="margin-bottom: 2rem"><h2 style="margin-top: 4rem">References</h2>
<div class="csl-entry" id="ref-willard2023efficient" role="listitem">[1] 
Willard, Brandon T., and Rémi Louf. 2023. <span>“<a href="https://arxiv.org/abs/2307.09702">Efficient Guided Generation for
Large Language Models</a>.”</span>
</div>
</div>
</body></html><!-- HTML_TAG_END --> </div> 
			
			<script>
				{
					__sveltekit_rqxlb1 = {
						base: new URL(".", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{content:"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>언어모델을 이용해 작업을 한다면 문서 요약결과를 JSON으로 출력한다거나\n하는 등 우리가 원하는 형식으로 output을 내고싶을 때가 종종 있다. 하지만\n이런 output의 형식을 제한하는 것은 단순한 프롬프팅이나 fine-tuning만으로\n해결하기가 쉽지 않다. \u003Ca href=\"#ref-willard2023efficient\">[1]\u003C/a>에선\n이를 FSM을 통해 효율적으로 해결하는 방법을 제시한다.\u003C/p>\n\u003Ch2 id=\"method\">Method\u003C/h2>\n\u003Cp>Guided generation의 핵심은 output을 낼 때 vocabulary의 token별 확률을\n조정하는 것이다(예를 들어, vocabulary가 [\"I’m\", \"my\", \"his\", \"hi\"]로\n구성돼있을때 \"hi\" 다음 올 token의 확률이 [0.8, 0.1, 0.05, 0.05]라면 이를\n[0, 1, 0, 0] 이런 식으로 조정). 이런 방법은 이전에도 있어왔는데,\n이전까지의 방법들은 runtime에서 그때그때마다(token을 output으로\n내놓을때마다) 조정을 하는 방법으로 O(N)의 시간복잡도를 가졌다(N은\nvocabulary 크기).\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-willard2023efficient\">[1]\u003C/a>에선\n이를 FSM을 통해 각 state별 token의 확률을 미리 조정하여 저장하고, 이를\n이용해 output을 내놓는 방법을 제시한다. 이를 통해 O(N)의 runtime\n시간복잡도를 평균 O(1)로 줄일 수 있다(FSM을 이용한 memoization이라고\n보면 됨).\u003C/p>\n\u003Cp>이 방법은 CFG 등으로 확장이 가능해 기존의 여러 parser들과 연계가\n가능하다.\u003C/p>\n\u003Ch3 id=\"llm-sampling-and-guided-generation\">LLM Sampling and Guided\nGeneration\u003C/h3>\n\u003Cp>우선 일반적인 언어모델의 샘플링 방법을 살펴보자. \u003Cspan class=\"math inline\">\\(\\mathcal{V}\\)\u003C/span>가 vocabulary이고 \u003Cspan class=\"math inline\">\\(|\\mathcal{V}| = N\\)\u003C/span> 일때 \u003Cspan class=\"math inline\">\\(t\\)\u003C/span> 토큰 sequence \u003Cspan class=\"math inline\">\\(S_t = \\left(s_1 \\dots s_t\\right)\\)\u003C/span>에 대해\n\u003Cspan class=\"math inline\">\\(s_i \\in \\mathcal{V}\\)\u003C/span>라고 하자. \u003Cspan class=\"math inline\">\\(\\mathcal{V}\\)\u003C/span>는 fixed alphabet으로부터\n만들어진 string으로 구성돼있고 \u003Cspan class=\"math inline\">\\(N\\)\u003C/span>은\n보통 \u003Cspan class=\"math inline\">\\(10^4\\)\u003C/span> 보다 크거나 같다.\u003C/p>\n\u003Cp>다음 토큰 \u003Cspan class=\"math inline\">\\(s_{t+1}\\)\u003C/span>은 아래와 같은\nrandom variable로 정의된다:\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[\\begin{aligned}\n  \\boldsymbol{\\alpha} &amp; = \\operatorname{LLM}(S_t,\n\\boldsymbol{\\theta})         \\\\\n  s_{t+1}             &amp; \\sim\n\\operatorname{Categorical}({\\boldsymbol{\\alpha}})\n\\end{aligned}\\qquad{(1)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>이때 \u003Cspan class=\"math inline\">\\(\\boldsymbol{\\theta}\\)\u003C/span>는 LLM의\n파라미터이고 \u003Cspan class=\"math inline\">\\(\\boldsymbol{\\alpha} \\in\n\\mathbb{R}^N\\)\u003C/span>이다.\u003C/p>\n\u003Cp>Algorithm \u003Ca data-reference=\"alg:llm-sequence-sampling\" data-reference-type=\"ref\" href=\"#alg:llm-sequence-sampling\">1\u003C/a>은\n이 샘플링 과정을 나타낸다.\u003C/p>\n\u003Cfigure id=\"alg:llm-sequence-sampling\">\n\u003Cimg alt=\"Algorithm 1\" src=\"figures/efficient-guided-generation-for-large-language-models/_13a1__algorithm1.png\" style=\"width:100.0%\"/>\n\u003Cfigcaption>\u003Cp>Algorithm 1: Basic LLM token sampling(Source: Algorithm 1 in \u003Ca href=\"#ref-willard2023efficient\">[1]\u003C/a>)\u003C/p>\u003C/figcaption>\n\u003C/figure>\n\u003Cp>이제 output logit \u003Cspan class=\"math inline\">\\(\\boldsymbol{\\alpha}\\)\u003C/span>를 조정하여 Guided\ngeneration을 하는 방법을 살펴보자. 이는 위의 샘플링 과정에 boolean mask\n\u003Cspan class=\"math inline\">\\(m: \\mathcal{P}\\left(\\mathcal{V}\\right) \\to\n\\left\\{0, 1\\right\\}^N\\)\u003C/span> 을 추가하면 된다. 식은 다음과 같다:\u003C/p>\n\u003Cp>\u003Cspan>\u003Cspan class=\"math display\">\\[\\begin{aligned}\n  \\boldsymbol{\\alpha}         &amp; = \\operatorname{LM}(\\tilde{S}_t,\n\\boldsymbol{\\theta})                \\\\\n  \\tilde{\\boldsymbol{\\alpha}} &amp; =\n\\operatorname{m}\\left(\\tilde{S}_t\\right) \\odot \\boldsymbol{\\alpha} \\\\\n  \\tilde{s}_{t+1}             &amp; \\sim\n\\operatorname{Categorical}({\\tilde{\\boldsymbol{\\alpha}}})\n\\end{aligned}\\qquad{(2)}\\]\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>이때 이 마스킹 과정을 runtime에서 해버리면 O(N)의 시간복잡도를 가지게\n된다. 이를 효율적으로 하기 위해 \u003Ca href=\"#ref-willard2023efficient\">[1]\u003C/a>에선\nFSM을 이용한다.\u003C/p>\n\u003Ch3 id=\"iterative-fsm-processing-and-indexing\">Iterative FSM Processing\nand Indexing\u003C/h3>\n\u003Cp>기본골자는 다음과 같다. 어떤 regex로 표현될 수 있는 문자열이 있다면,\n이는 FSM으로 변환이 가능하고, 이 FSM의 각 state별로 다음 transition에\n대해 가능한 token의 확률을 조정하여 저장해놓는다(각각의 vocab에 대한\n마스킹을 미리 해놓는다). 이를 통해 runtime에서는 각 state별로 마스킹된\nlogit을 이용해 샘플링을 하면 되고, hash map 등을 이용하면 평균 O(1)의\n시간복잡도를 가지게 된다. 다음 알고리즘이 이를 표현한 것이다.\u003C/p>\n\u003Cfigure id=\"alg:fsm-sub-sequences\">\n\u003Cimg alt=\"Algorithm 2\" src=\"figures/efficient-guided-generation-for-large-language-models/_13a1__algorithm2.png\" style=\"width:100.0%\"/>\n\u003Cfigcaption>\u003Cp>Algorithm 2: Find sub-sequences of the FSM \u003Cspan class=\"math inline\">\\(M\\)\u003C/span> that accept the string \u003Cspan class=\"math inline\">\\(\\boldsymbol{v}\\)\u003C/span> (Source: Algorithm 3 in\n\u003Ca href=\"#ref-willard2023efficient\">[1]\u003C/a>)\u003C/p>\u003C/figcaption>\n\u003C/figure>\n\u003Ch3 id=\"extensions-to-iterative-parsing\">Extensions to Iterative\nParsing\u003C/h3>\n\u003Cp>이 방법은 PDA를 써서 CFG 등으로 확장이 가능하며 이를 통해 기존의 여러\nparser들과 연계가 가능하다. 기본적인 진행은 이전과 유사하다.\u003C/p>\n\u003Ch3 id=\"limitations\">Limitations\u003C/h3>\n\u003Cp>개인적으로 이 방법이 가지는 한계는 다음과 같다고 생각한다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>결국 vocab size가 크면 쉽게 적용할만하지 않다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>언어모델 외부에선(예를 들어, 출력된 결과만으론) 적용이\n불가능하다(직접 logit을 조정해야함).\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>여전히 원하지 않는 결과를 얻게 될 수도 있다(예를 들어, 어떤\nstring 값을 갖는 JSON을 생성할때 그 string에서 계속(context를 잊어먹어\n다음 단계로 가지않고) 생성이 진행되는 경우, 글자 제한 등으로 이런 경우를\n막으려고 한다면 그 제한안에 우리가 원하는 내용이 들어갈지는 확신할 수\n없다).\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>단순 마스킹은 언어모델이 처음부터 output의 포맷을 고려하여\n진행했을때와 확률자체가 다를 수 있다. 예를 들어 마스킹을 한 경우 첫번째\n인덱스를 답으로 뱉을 수 있지만, 처음부터 어떤 포맷이될지 언어모델이 미리\n알고 진행했다면 두번째 인덱스가 답이 될 수도 있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>하지만 이런 한계에도 불구하고 이 방법을 training이나 fine tuning 때\n이용하여 학습을 진행한다던지, masking을 언어모델 내부적으로 구현하여\ninference때 쓸데 없는 뉴런을 거치지 않게 만들어 cost를 아낀다던지 등의\n부가적인 장점 역시 존재한다.\u003C/p>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>FSM과 memoization같은 방법을 활용해 logit을 masking한다면 평균\nO(1)의 시간복잡도로 guided generation을 할 수 있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>이 방법은 CFG 등으로 확장이 가능하며 기존의 여러 parser들과\n연계가 가능하다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>언어모델 내부적으로 마스킹을 구현한다면 inference때 쓸데 없는\n뉴런을 거치지 않게 만들어 cost를 아끼게 만들 수 있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-willard2023efficient\" role=\"listitem\">[1] \nWillard, Brandon T., and Rémi Louf. 2023. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2307.09702\">Efficient Guided Generation for\nLarge Language Models\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"},"uses":{}}];

					Promise.all([
						import("./_app/immutable/entry/start.1162bf57.js"),
						import("./_app/immutable/entry/app.c39bda2d.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>