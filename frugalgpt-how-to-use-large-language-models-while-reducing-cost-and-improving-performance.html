<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
	<link rel="manifest" href="./favicon/site.webmanifest">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<script>
		MathJax = {
			loader: { load: ['[tex]/textmacros'] },
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				packages: { '[+]': ['textmacros'] },
			},
		};
	</script>
	<script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js">
		</script>
	
		<link href="./_app/immutable/assets/0.08c9bd5d.css" rel="stylesheet">
		<link href="./_app/immutable/assets/6.24feffef.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.1162bf57.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.e108d1fd.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/singletons.caa5cb96.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.c39bda2d.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.38a092e6.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.3e7173ef.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/6.59276d91.js"><title>First Read - FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving
		Performance</title><!-- HEAD_svelte-drgpau_START --><!-- HEAD_svelte-drgpau_END -->
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">  <div class="nav svelte-5baz1e" data-svelte-h="svelte-89nan6"><a href="/">Posts</a></div>  <div class="content svelte-1njf2hb"><div class="title svelte-1njf2hb" data-svelte-h="svelte-fo07eq"><h1 class="svelte-1njf2hb">First Read</h1> <h2 class="svelte-1njf2hb">FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance</h2> <div class="date svelte-1njf2hb">2023-09-10</div></div> <!-- HTML_TAG_START --><html><body><h2 id="overview">Overview</h2>
<p>LLM의 활용도가 높아져감에 따라 LLM API 서비스들이 많이 생겨나고 있다.
이런 LLM API 들은 가격이나 성능이 다양한데, 가장 성능이 높은 LLM API만
사용하면 그 비용이 많이 들게 되고 가격이 저렴한 API를 사용하면 성능이
떨어지게 된다.</p>
<p><a href="#ref-chen2023frugalgpt">[1]</a>에선 성능을
떨어뜨리지 않으면서도 가격은 저렴하도록 LLM API를 사용하는 방법을
제시한다.</p>
<h2 id="scope-and-problem-statement">Scope and Problem Statement</h2>
<h3 id="natural-language-query-answering">Natural language query
answering</h3>
<p>이 논문은 natural language query answering task에 집중하여 논의를
진행하였다. 이 task의 목표는 natural language query distribution <span class="math inline">\(\mathcal{Q}\)</span>에서 샘플링한 query <span class="math inline">\(q\)</span>에 대한 답을 내는것이다. News
classification, reading comprehension, commonsense reasoning 등 다양한
문제들이 QA 문제로 표현될 수 있기 때문에 다양한 상황에 적용될 수
있다.</p>
<h3 id="llm-marketplace">LLM marketplace</h3>
<p>이 논문에선 LLM API를 함수 <span class="math inline">\(f_i(\cdot):
\mathcal{P}\mapsto \mathcal{A}\)</span>로 표현하고, <span class="math inline">\(K\)</span>개의 서로 다른 LLM API들의 집합 <span class="math inline">\(\{f_i(\cdot)\}_{i=1}^{K}\)</span>을 LLM
market이라한다. <span class="math inline">\(\mathcal{P}\)</span>는
prompt space이고, <span class="math inline">\(\mathcal{A}\)</span>는
answer distribution이다. LLM API를 쓰기 위해선 먼저 query <span class="math inline">\(q\)</span>를 prompt <span class="math inline">\(p\)</span>로 변환해야한다는 점을 주의하자. 이
prompt <span class="math inline">\(p\)</span>는 API 함수를 거쳐 answer
<span class="math inline">\(a\)</span>를 생성한다.</p>
<p>LLM API의 cost(비용)는 보통 다음 세가지로 구성돼있다.</p>
<ol>
<li><p>prompt의 길이에 비례하는 비용</p></li>
<li><p>생성된 answer의 길이에 비례하는 비용</p></li>
<li><p>query당 고정 비용</p></li>
</ol>
<p>따라서 cost는 다음과 같이 정의된다. <span><span class="math display">\[c_i(p) \triangleq \tilde{c}_{i,2}\|f_i(p)\|+
\tilde{c}_{i,1} \|p\| + \tilde{c}_{i,0}\qquad{(1)}\]</span></span> 이때,
<span class="math inline">\(\tilde{c}_{i,j},j=0,1,2\)</span>는
constant이다.</p>
<h3 id="problem-statement-budget-aware-llm-api-usage">Problem statement:
budget-aware LLM API usage</h3>
<p>이 논문의 목표는 <em>적절한 방법을 이용해 제한된 budget에서 최대
성능을 내도록 LLM API들을 쓰는것</em>이다. Formal하게는 다음과 같다.
<span><span class="math display">\[\max_{s\in \mathcal{S}}
\mathbb{E}_{(q,a)\in\mathcal{Q}\times \mathcal{A}}[r(a,\hat{a}(s,q))]
  \,\,\text{\textbf{s.t.}}\,\,
  \mathbb{E}_{(q,a)\in\mathcal{Q}\times \mathcal{A}}[c(s,q)]\leq
b\qquad{(2)}\]</span></span> 이때, <span class="math inline">\(s\)</span>는 strategy(API를 쓰는 방법), <span class="math inline">\(a\)</span>는 query <span class="math inline">\(q\)</span>에 대한 correct answer, <span class="math inline">\(\hat{a}(s,q)\)</span>는 strategy <span class="math inline">\(s\)</span>를 사용하여 생성된 answer, <span class="math inline">\(c(s,q)\)</span>는 strategy <span class="math inline">\(s\)</span>를 사용하여 query <span class="math inline">\(q\)</span>를 처리하는데 드는 cost이며, <span class="math inline">\(b\)</span>는 budget이다. 함수 <span class="math inline">\(r(\cdot,\cdot)\)</span>은 generated answer와
correct answer의 유사도를 측정하는 reward function이다.</p>
<h2 id="how-to-use-llms-affordably-and-accurately">How to Use LLMs
Affordably and Accurately</h2>
<figure id="fig:Examples">
<img src="figures/frugalgpt-how-to-use-large-language-models-while-reducing-cost-and-improving-performance/methodexample.jpg" style="width:100.0%"/>
<figcaption>Figure 1: Illustrations of cost-saving strategies. (a)
Prompt selection uses a subset of in-context examples as the prompt to
reduce the size of the prompt. (b) Query concatenation aggregates
multiple queries to share prompts. (c) Completion cache stores and
reuses an LLM API’s response when a similar query is asked. (d) Model
fine-tuning uses expensive LLMs’ responses to fine-tune cheap LLMs. (e)
LLM cascade employs different LLM APIs for different queries. (Source:
Fig. 2 in <a href="#ref-chen2023frugalgpt">[1]</a>)</figcaption>
</figure>
<p><a href="#ref-chen2023frugalgpt">[1]</a>에선 크게
세가지 방법을 제시한다.</p>
<h3 id="prompt-adaptation">Prompt adaptation</h3>
<p>Prompt adaptation은 prompt 길이에 따라 비용이 증가하니 prompt 길이를
줄여 비용을 아끼는 방법이다. 본 논문에선 prompt adaptation을 위해 두가지
방법을 제시한다.</p>
<ol>
<li><p><em>Prompt selection</em>: prompt를 짧게 만들어서 비용을 줄이는
방법이다. Fig. <a data-reference="fig:Examples" data-reference-type="ref" href="#fig:Examples">1</a>(a)에서 보듯이 모든 예제를 prompt로
사용하는것이 아니라 일부 subset만 사용하는 등의 방법으로 비용을 줄인다.
어떤 subset을 선택하는지가 관건이다.</p></li>
<li><p><em>Query concatenation</em>: 여러 query를 하나의 prompt로
사용하는 방법이다. Fig. <a data-reference="fig:Examples" data-reference-type="ref" href="#fig:Examples">1</a>(b)에서
보듯이 같은 prompt를 써서 여러 질문을 하면 prompt 중복에 따라 비용이
들게되니 prompt를 공유하는 query들을 묶어서 하나의 query로 만들어서
사용한다.</p></li>
</ol>
<h3 id="llm-approximation">LLM approximation</h3>
<p>LLM approximation은 LLM의 response를 적절한 비용의 다른 방법을 써서
근사하는 방법이다. 본 논문에선 다음 두가지 방법을 제시한다.</p>
<ol>
<li><p><em>Completion cache</em>: LLM의 response를 저장해두고, 비슷한
query가 들어오면 저장된 response를 사용하는 방법이다. Fig. <a data-reference="fig:Examples" data-reference-type="ref" href="#fig:Examples">1</a>(c)에 이를 적용한 예시가
나와있다.</p></li>
<li><p><em>Model fine-tuning(Distillation)</em>: 비싼 LLM의 response를
사용하여 저렴한 LLM을 fine-tuning하는 방법이다. 이를 통해 저렴한
비용으로 비슷한 성능을 낼 수 있다. Fig. <a data-reference="fig:Examples" data-reference-type="ref" href="#fig:Examples">1</a>(d)에 이를
적용한 예시가 나와있다.</p></li>
</ol>
<h3 id="llm-cascade">LLM cascade</h3>
<p>LLM들이 각각 다른 성능과 가격을 가지고 있으니, 이를 적절히 조합하여
사용하는 방법이다. 이 논문에서 제시된 방법은 LLM을 성능순대로
sequential하게 배치하고, 응답이 적절했으면 그대로 사용하고 그렇지 않다면
다음 LLM으로 진행해나가는 방법이다. Fig. <a data-reference="fig:Examples" data-reference-type="ref" href="#fig:Examples">1</a>(e)에 이를
적용한 예시가 나와있다. Cascade의 핵심요소는 두가지이다.</p>
<ol>
<li><p><em>Generation scoring function</em>: Query와 <span class="math inline">\(q\)</span>와 LLM의 답 <span class="math inline">\(a\)</span>로부터 얼마나 reliable한지 측정하는
함수이다. Formal하게 <span class="math inline">\(g(\cdot,\cdot):\mathcal{Q} \times
\mathcal{A}\mapsto [0,1]\)</span>로 표현된다.</p></li>
<li><p><em>LLM router</em>: LLM router는 LLM market에서 <span class="math inline">\(m\)</span>개의 LLM API를 선택한다.</p></li>
</ol>
<p>Cascade의 동작은 구체적으로 다음과 같다.</p>
<ol>
<li><p><span class="math inline">\(m\)</span>개의 LLM API를 선택한다.
선택된 <span class="math inline">\(m\)</span>개의 LLM의 인덱스를 <span class="math inline">\(\pmb L \in [K]^m\)</span>로 표현하자.</p></li>
<li><p>Query <span class="math inline">\(q\)</span>가 들어오면, <span class="math inline">\(i\)</span>번째 API를 선택하여 query에 대한 답
<span class="math inline">\(f_{L_i}(q)\)</span>을 생성한다.</p></li>
<li><p><span class="math inline">\(g(q,f_{L_i}(q))\)</span>를 계산하여
<span class="math inline">\(\pmb \tau_i\)</span>보다 높으면 그대로
사용하고, 그렇지 않으면 다음 API를 선택한다.</p></li>
</ol>
<p>Cascade는 optimization problem으로 표현될 수 있는데, 이 optimization
problem을 그대로 풀기엔 계산량이 너무 많으므로 small answer
disagreement를 가지는 LLM들은 무시하고, objective는 작은 sample을 돌려본
결과로 근사하여 풀어낸다.</p>
<h3 id="compositions">Compositions</h3>
<p>위의 방법들을 조합하여 사용하는것도 cost reduction에 도움이 된다.</p>
<h2 id="llm-cascade-reduces-cost-and-improves-accuracy">LLM Cascade
Reduces Cost and Improves Accuracy</h2>
<figure id="fig:Tradeoffs">
<img src="figures/frugalgpt-how-to-use-large-language-models-while-reducing-cost-and-improving-performance/tradeoffs_full.jpg" style="width:90.0%"/>
<figcaption>Figure 2: Accuracy and cost tradeoffs achieved by FrugalGPT.
Overall, FrugalGPT often achieves the same performance of the best
individual LLM API (e.g., GPT-4) with orders of magnitudes smaller cost.
When incurring the same cost, FrugalGPT can improves the accuracy by up
to 5%. Examples of LLM cascade for each dataset are shown on the right.
(Source: Fig. 5 in <a href="#ref-chen2023frugalgpt">[1]</a>)</figcaption>
</figure>
<p>저자들이 LLM cascade의 방법으로 선보인 FrugalGPT의 주요 실험 결과는
Fig. <a data-reference="fig:Tradeoffs" data-reference-type="ref" href="#fig:Tradeoffs">2</a>에 나와있다.</p>
<p>실험에서 싼 LLM이라도 비싼 LLM보다 성능이 높을 수 있음을 알 수 있다.
저자들은 이를 각 dataset에 대해 측정해 얼마나 performance를 더 높일 수
있는지(상한)를 계산하고 이를 <em>MPI</em>(maxium performance
improvement)라고 정의하였다.</p>
<p>또 Fig. <a data-reference="fig:Tradeoffs" data-reference-type="ref" href="#fig:Tradeoffs">2</a>에서 LLM API의 cost 순위는 dataset에
따라 가변적임을 주목하자. 이는 LLM API가 각각 다른 pricing 정책을 가지고
있기 때문으로 보인다.</p>
<h2 id="limitations">Limitations</h2>
<p>저자들이 밝힌 한계점은 다음과 같다.</p>
<ol>
<li><p>LLM cascade를 학습하기 위해 labeled example이 필요하다.</p></li>
<li><p>학습의 training example은 test example과 같은 distribution에서
나와야한다. 즉, 실제로 마주할 문제의 distribution과 비슷한
distribution에서 학습을 해야한다.</p></li>
<li><p>LLM cascade를 학습하는데에도 비용이 든다.</p></li>
</ol>
<h2 id="quick-recap">Quick Recap</h2>
<ol>
<li><p>LLM API를 적절히 사용하면 성능을 높이면서도 비용을 줄일 수
있다.</p></li>
<li><p>그 방법으로는 prompt adaptation, LLM approximation, LLM cascade가
있다.</p></li>
<li><p>LLM cascade는 LLM API를 성능순대로 sequential하게 배치하고,
응답이 적절했으면 그대로 사용하고 그렇지 않다면 다음 LLM으로
진행해나가는 방법이다.</p></li>
<li><p>LLM cascade는 비용을 크게 줄이면서도 성능은 최고 성능의 단일
API와 비슷하거나 더 높도록 할 수 있다.</p></li>
</ol>
<div class="references csl-bib-body hanging-indent" id="refs" role="list" style="margin-bottom: 2rem"><h2 style="margin-top: 4rem">References</h2>
<div class="csl-entry" id="ref-chen2023frugalgpt" role="listitem">[1] 
Chen, Lingjiao, Matei Zaharia, and James Zou. 2023. <span>“<a href="https://arxiv.org/abs/2305.05176">FrugalGPT: How to Use Large
Language Models While Reducing Cost and Improving
Performance</a>.”</span>
</div>
</div>
</body></html><!-- HTML_TAG_END --> </div> 
			
			<script>
				{
					__sveltekit_rqxlb1 = {
						base: new URL(".", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{content:"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>LLM의 활용도가 높아져감에 따라 LLM API 서비스들이 많이 생겨나고 있다.\n이런 LLM API 들은 가격이나 성능이 다양한데, 가장 성능이 높은 LLM API만\n사용하면 그 비용이 많이 들게 되고 가격이 저렴한 API를 사용하면 성능이\n떨어지게 된다.\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-chen2023frugalgpt\">[1]\u003C/a>에선 성능을\n떨어뜨리지 않으면서도 가격은 저렴하도록 LLM API를 사용하는 방법을\n제시한다.\u003C/p>\n\u003Ch2 id=\"scope-and-problem-statement\">Scope and Problem Statement\u003C/h2>\n\u003Ch3 id=\"natural-language-query-answering\">Natural language query\nanswering\u003C/h3>\n\u003Cp>이 논문은 natural language query answering task에 집중하여 논의를\n진행하였다. 이 task의 목표는 natural language query distribution \u003Cspan class=\"math inline\">\\(\\mathcal{Q}\\)\u003C/span>에서 샘플링한 query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>에 대한 답을 내는것이다. News\nclassification, reading comprehension, commonsense reasoning 등 다양한\n문제들이 QA 문제로 표현될 수 있기 때문에 다양한 상황에 적용될 수\n있다.\u003C/p>\n\u003Ch3 id=\"llm-marketplace\">LLM marketplace\u003C/h3>\n\u003Cp>이 논문에선 LLM API를 함수 \u003Cspan class=\"math inline\">\\(f_i(\\cdot):\n\\mathcal{P}\\mapsto \\mathcal{A}\\)\u003C/span>로 표현하고, \u003Cspan class=\"math inline\">\\(K\\)\u003C/span>개의 서로 다른 LLM API들의 집합 \u003Cspan class=\"math inline\">\\(\\{f_i(\\cdot)\\}_{i=1}^{K}\\)\u003C/span>을 LLM\nmarket이라한다. \u003Cspan class=\"math inline\">\\(\\mathcal{P}\\)\u003C/span>는\nprompt space이고, \u003Cspan class=\"math inline\">\\(\\mathcal{A}\\)\u003C/span>는\nanswer distribution이다. LLM API를 쓰기 위해선 먼저 query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>를 prompt \u003Cspan class=\"math inline\">\\(p\\)\u003C/span>로 변환해야한다는 점을 주의하자. 이\nprompt \u003Cspan class=\"math inline\">\\(p\\)\u003C/span>는 API 함수를 거쳐 answer\n\u003Cspan class=\"math inline\">\\(a\\)\u003C/span>를 생성한다.\u003C/p>\n\u003Cp>LLM API의 cost(비용)는 보통 다음 세가지로 구성돼있다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>prompt의 길이에 비례하는 비용\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>생성된 answer의 길이에 비례하는 비용\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>query당 고정 비용\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>따라서 cost는 다음과 같이 정의된다. \u003Cspan>\u003Cspan class=\"math display\">\\[c_i(p) \\triangleq \\tilde{c}_{i,2}\\|f_i(p)\\|+\n\\tilde{c}_{i,1} \\|p\\| + \\tilde{c}_{i,0}\\qquad{(1)}\\]\u003C/span>\u003C/span> 이때,\n\u003Cspan class=\"math inline\">\\(\\tilde{c}_{i,j},j=0,1,2\\)\u003C/span>는\nconstant이다.\u003C/p>\n\u003Ch3 id=\"problem-statement-budget-aware-llm-api-usage\">Problem statement:\nbudget-aware LLM API usage\u003C/h3>\n\u003Cp>이 논문의 목표는 \u003Cem>적절한 방법을 이용해 제한된 budget에서 최대\n성능을 내도록 LLM API들을 쓰는것\u003C/em>이다. Formal하게는 다음과 같다.\n\u003Cspan>\u003Cspan class=\"math display\">\\[\\max_{s\\in \\mathcal{S}}\n\\mathbb{E}_{(q,a)\\in\\mathcal{Q}\\times \\mathcal{A}}[r(a,\\hat{a}(s,q))]\n  \\,\\,\\text{\\textbf{s.t.}}\\,\\,\n  \\mathbb{E}_{(q,a)\\in\\mathcal{Q}\\times \\mathcal{A}}[c(s,q)]\\leq\nb\\qquad{(2)}\\]\u003C/span>\u003C/span> 이때, \u003Cspan class=\"math inline\">\\(s\\)\u003C/span>는 strategy(API를 쓰는 방법), \u003Cspan class=\"math inline\">\\(a\\)\u003C/span>는 query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>에 대한 correct answer, \u003Cspan class=\"math inline\">\\(\\hat{a}(s,q)\\)\u003C/span>는 strategy \u003Cspan class=\"math inline\">\\(s\\)\u003C/span>를 사용하여 생성된 answer, \u003Cspan class=\"math inline\">\\(c(s,q)\\)\u003C/span>는 strategy \u003Cspan class=\"math inline\">\\(s\\)\u003C/span>를 사용하여 query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>를 처리하는데 드는 cost이며, \u003Cspan class=\"math inline\">\\(b\\)\u003C/span>는 budget이다. 함수 \u003Cspan class=\"math inline\">\\(r(\\cdot,\\cdot)\\)\u003C/span>은 generated answer와\ncorrect answer의 유사도를 측정하는 reward function이다.\u003C/p>\n\u003Ch2 id=\"how-to-use-llms-affordably-and-accurately\">How to Use LLMs\nAffordably and Accurately\u003C/h2>\n\u003Cfigure id=\"fig:Examples\">\n\u003Cimg src=\"figures/frugalgpt-how-to-use-large-language-models-while-reducing-cost-and-improving-performance/methodexample.jpg\" style=\"width:100.0%\"/>\n\u003Cfigcaption>Figure 1: Illustrations of cost-saving strategies. (a)\nPrompt selection uses a subset of in-context examples as the prompt to\nreduce the size of the prompt. (b) Query concatenation aggregates\nmultiple queries to share prompts. (c) Completion cache stores and\nreuses an LLM API’s response when a similar query is asked. (d) Model\nfine-tuning uses expensive LLMs’ responses to fine-tune cheap LLMs. (e)\nLLM cascade employs different LLM APIs for different queries. (Source:\nFig. 2 in \u003Ca href=\"#ref-chen2023frugalgpt\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>\u003Ca href=\"#ref-chen2023frugalgpt\">[1]\u003C/a>에선 크게\n세가지 방법을 제시한다.\u003C/p>\n\u003Ch3 id=\"prompt-adaptation\">Prompt adaptation\u003C/h3>\n\u003Cp>Prompt adaptation은 prompt 길이에 따라 비용이 증가하니 prompt 길이를\n줄여 비용을 아끼는 방법이다. 본 논문에선 prompt adaptation을 위해 두가지\n방법을 제시한다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cem>Prompt selection\u003C/em>: prompt를 짧게 만들어서 비용을 줄이는\n방법이다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(a)에서 보듯이 모든 예제를 prompt로\n사용하는것이 아니라 일부 subset만 사용하는 등의 방법으로 비용을 줄인다.\n어떤 subset을 선택하는지가 관건이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cem>Query concatenation\u003C/em>: 여러 query를 하나의 prompt로\n사용하는 방법이다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(b)에서\n보듯이 같은 prompt를 써서 여러 질문을 하면 prompt 중복에 따라 비용이\n들게되니 prompt를 공유하는 query들을 묶어서 하나의 query로 만들어서\n사용한다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"llm-approximation\">LLM approximation\u003C/h3>\n\u003Cp>LLM approximation은 LLM의 response를 적절한 비용의 다른 방법을 써서\n근사하는 방법이다. 본 논문에선 다음 두가지 방법을 제시한다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cem>Completion cache\u003C/em>: LLM의 response를 저장해두고, 비슷한\nquery가 들어오면 저장된 response를 사용하는 방법이다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(c)에 이를 적용한 예시가\n나와있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cem>Model fine-tuning(Distillation)\u003C/em>: 비싼 LLM의 response를\n사용하여 저렴한 LLM을 fine-tuning하는 방법이다. 이를 통해 저렴한\n비용으로 비슷한 성능을 낼 수 있다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(d)에 이를\n적용한 예시가 나와있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"llm-cascade\">LLM cascade\u003C/h3>\n\u003Cp>LLM들이 각각 다른 성능과 가격을 가지고 있으니, 이를 적절히 조합하여\n사용하는 방법이다. 이 논문에서 제시된 방법은 LLM을 성능순대로\nsequential하게 배치하고, 응답이 적절했으면 그대로 사용하고 그렇지 않다면\n다음 LLM으로 진행해나가는 방법이다. Fig. \u003Ca data-reference=\"fig:Examples\" data-reference-type=\"ref\" href=\"#fig:Examples\">1\u003C/a>(e)에 이를\n적용한 예시가 나와있다. Cascade의 핵심요소는 두가지이다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cem>Generation scoring function\u003C/em>: Query와 \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>와 LLM의 답 \u003Cspan class=\"math inline\">\\(a\\)\u003C/span>로부터 얼마나 reliable한지 측정하는\n함수이다. Formal하게 \u003Cspan class=\"math inline\">\\(g(\\cdot,\\cdot):\\mathcal{Q} \\times\n\\mathcal{A}\\mapsto [0,1]\\)\u003C/span>로 표현된다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cem>LLM router\u003C/em>: LLM router는 LLM market에서 \u003Cspan class=\"math inline\">\\(m\\)\u003C/span>개의 LLM API를 선택한다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>Cascade의 동작은 구체적으로 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cspan class=\"math inline\">\\(m\\)\u003C/span>개의 LLM API를 선택한다.\n선택된 \u003Cspan class=\"math inline\">\\(m\\)\u003C/span>개의 LLM의 인덱스를 \u003Cspan class=\"math inline\">\\(\\pmb L \\in [K]^m\\)\u003C/span>로 표현하자.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Query \u003Cspan class=\"math inline\">\\(q\\)\u003C/span>가 들어오면, \u003Cspan class=\"math inline\">\\(i\\)\u003C/span>번째 API를 선택하여 query에 대한 답\n\u003Cspan class=\"math inline\">\\(f_{L_i}(q)\\)\u003C/span>을 생성한다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cspan class=\"math inline\">\\(g(q,f_{L_i}(q))\\)\u003C/span>를 계산하여\n\u003Cspan class=\"math inline\">\\(\\pmb \\tau_i\\)\u003C/span>보다 높으면 그대로\n사용하고, 그렇지 않으면 다음 API를 선택한다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>Cascade는 optimization problem으로 표현될 수 있는데, 이 optimization\nproblem을 그대로 풀기엔 계산량이 너무 많으므로 small answer\ndisagreement를 가지는 LLM들은 무시하고, objective는 작은 sample을 돌려본\n결과로 근사하여 풀어낸다.\u003C/p>\n\u003Ch3 id=\"compositions\">Compositions\u003C/h3>\n\u003Cp>위의 방법들을 조합하여 사용하는것도 cost reduction에 도움이 된다.\u003C/p>\n\u003Ch2 id=\"llm-cascade-reduces-cost-and-improves-accuracy\">LLM Cascade\nReduces Cost and Improves Accuracy\u003C/h2>\n\u003Cfigure id=\"fig:Tradeoffs\">\n\u003Cimg src=\"figures/frugalgpt-how-to-use-large-language-models-while-reducing-cost-and-improving-performance/tradeoffs_full.jpg\" style=\"width:90.0%\"/>\n\u003Cfigcaption>Figure 2: Accuracy and cost tradeoffs achieved by FrugalGPT.\nOverall, FrugalGPT often achieves the same performance of the best\nindividual LLM API (e.g., GPT-4) with orders of magnitudes smaller cost.\nWhen incurring the same cost, FrugalGPT can improves the accuracy by up\nto 5%. Examples of LLM cascade for each dataset are shown on the right.\n(Source: Fig. 5 in \u003Ca href=\"#ref-chen2023frugalgpt\">[1]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>저자들이 LLM cascade의 방법으로 선보인 FrugalGPT의 주요 실험 결과는\nFig. \u003Ca data-reference=\"fig:Tradeoffs\" data-reference-type=\"ref\" href=\"#fig:Tradeoffs\">2\u003C/a>에 나와있다.\u003C/p>\n\u003Cp>실험에서 싼 LLM이라도 비싼 LLM보다 성능이 높을 수 있음을 알 수 있다.\n저자들은 이를 각 dataset에 대해 측정해 얼마나 performance를 더 높일 수\n있는지(상한)를 계산하고 이를 \u003Cem>MPI\u003C/em>(maxium performance\nimprovement)라고 정의하였다.\u003C/p>\n\u003Cp>또 Fig. \u003Ca data-reference=\"fig:Tradeoffs\" data-reference-type=\"ref\" href=\"#fig:Tradeoffs\">2\u003C/a>에서 LLM API의 cost 순위는 dataset에\n따라 가변적임을 주목하자. 이는 LLM API가 각각 다른 pricing 정책을 가지고\n있기 때문으로 보인다.\u003C/p>\n\u003Ch2 id=\"limitations\">Limitations\u003C/h2>\n\u003Cp>저자들이 밝힌 한계점은 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>LLM cascade를 학습하기 위해 labeled example이 필요하다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>학습의 training example은 test example과 같은 distribution에서\n나와야한다. 즉, 실제로 마주할 문제의 distribution과 비슷한\ndistribution에서 학습을 해야한다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM cascade를 학습하는데에도 비용이 든다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>LLM API를 적절히 사용하면 성능을 높이면서도 비용을 줄일 수\n있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>그 방법으로는 prompt adaptation, LLM approximation, LLM cascade가\n있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM cascade는 LLM API를 성능순대로 sequential하게 배치하고,\n응답이 적절했으면 그대로 사용하고 그렇지 않다면 다음 LLM으로\n진행해나가는 방법이다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>LLM cascade는 비용을 크게 줄이면서도 성능은 최고 성능의 단일\nAPI와 비슷하거나 더 높도록 할 수 있다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-chen2023frugalgpt\" role=\"listitem\">[1] \nChen, Lingjiao, Matei Zaharia, and James Zou. 2023. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2305.05176\">FrugalGPT: How to Use Large\nLanguage Models While Reducing Cost and Improving\nPerformance\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"},"uses":{}}];

					Promise.all([
						import("./_app/immutable/entry/start.1162bf57.js"),
						import("./_app/immutable/entry/app.c39bda2d.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 6],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>