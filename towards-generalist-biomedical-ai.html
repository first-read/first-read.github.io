<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
	<link rel="manifest" href="./favicon/site.webmanifest">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<script>
		MathJax = {
			loader: { load: ['[tex]/textmacros'] },
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				packages: { '[+]': ['textmacros'] },
			},
		};
	</script>
	<script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js">
		</script>
	
		<link href="./_app/immutable/assets/0.08c9bd5d.css" rel="stylesheet">
		<link href="./_app/immutable/assets/10.24feffef.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.f49ee20b.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.e108d1fd.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/singletons.55e0fd2e.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.6f9a8e8a.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.38a092e6.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.f6ebacf9.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/10.d23948dd.js"><title>First Read - Towards Generalist Biomedical AI</title><!-- HEAD_svelte-qr74ha_START --><!-- HEAD_svelte-qr74ha_END -->
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">  <div class="nav svelte-5baz1e" data-svelte-h="svelte-89nan6"><a href="/">Posts</a></div>  <div class="content svelte-1njf2hb"><div class="title svelte-1njf2hb" data-svelte-h="svelte-1mtzjjj"><h1 class="svelte-1njf2hb">First Read</h1> <h2 class="svelte-1njf2hb">Towards Generalist Biomedical AI</h2> <div class="date svelte-1njf2hb">2023-08-27</div></div> <!-- HTML_TAG_START --><html><body><h2 id="overview">Overview</h2>
<p>의료 데이터는 이미지, 텍스트, 수치 등 다양한 형태로 존재하며, 이를
통해 진단, 치료, 예방 등 다양한 목적을 달성하고자 한다. 즉, 의료분야는
본질적으로 multimodal인 분야이다. 그러나, 지금까지의 biomedical AI는
대부분 unimodal로 여러 의료데이터 정보의 통합에 한계가 있었다.</p>
<p><a href="#ref-tu2023generalist">[2]</a>에선 이를
해결하기 위해</p>
<ol>
<li><p>Multimodal medical benchmark를 위한 새 벤치마크
<strong>MultiMedBench</strong>를 curate하고,</p></li>
<li><p>Multimodal medical data의 여러 downstream task를 풀기 위한
<em>generalist</em>(동일한 weight으로 여러 task를 풀 수 있음) biomedical
AI 모델, <strong>Med-PaLM M</strong>을 제시한다.</p></li>
</ol>
<h2 id="multimedbench-a-benchmark-for-generalist-biomedical-ai">MultiMedBench:
A Benchmark for Generalist Biomedical AI</h2>
<figure id="tab:multimedbench-overview">
<img src="figures/towards-generalist-biomedical-ai/_13a1__table1.png" style="width:100%;"/>
<figcaption><p>Table 1: <strong>MultiMedBench overview.</strong> Summary of
MultiMedBench, the benchmark we introduce for the development and
evaluation of Med-PaLM M. MultiMedBench consists of 14 individual tasks
across 5 task types and 12 datasets spanning 7 biomedical data
modalities. In total, the benchmark contains over 1 million samples.
(Source: Table 1 in <a href="#ref-tu2023generalist">[2]</a>)</p></figcaption>
</figure>
<p>MultiMedBench는 generalist biomedical AI를 위해 curated된 multi-task,
multimodal benchmark이다. 기존의 12개의 비식별화된 오픈소스 데이터셋을
통합해 14개의 individual task를 가지고 있다. Output format은
classification을 포함한 모든 task에서 open-ended generation으로
동일하다. 대략적인 구조는 Table <a data-reference="tab:multimedbench-overview" data-reference-type="ref" href="#tab:multimedbench-overview">1</a>과
같다.</p>
<h2 id="med-palm-m-a-proof-of-concept-for-generalist-biomedical-ai">Med-PaLM
M: A Proof of Concept for Generalist Biomedical AI</h2>
<figure id="fig:task_prompts">
<img src="figures/towards-generalist-biomedical-ai/task_prompts.jpg" style="width:100.0%"/>
<figcaption>Figure 1: <strong>Illustration of instruction task prompting
with one-shot exemplar.</strong> (top) shows the task prompt for the
chest X-ray report generation task. It consists of task-specific
instructions, a text-only “one-shot exemplar” (omitting the
corresponding image but preserving the target answer), and the actual
question. The X-ray image is embedded and interleaved with textual
context including view orientation and reason for the study in addition
to the question. (bottom) shows the task prompt for the dermatology
classification task. The blue &lt;img&gt; denotes the position in the
prompt where the image tokens are embedded. (Source: Fig. 2 in <a href="#ref-tu2023generalist">[2]</a>)</figcaption>
</figure>
<p>Med-PaLM M은 PaLM-E를 기반으로 fine-tuning한 모델이다. <a href="#ref-tu2023generalist">[2]</a>에선 scale에
따라 총 3개의 PaLM-E(각각 PaLM-E 12B, PaLM-E 84B, PaLM-E 562B)를
기반으로 실험을 진행하였다.</p>
<p>학습은 Instruction tuning을 통해 multiple task를 가능하게 만들었고,
exemplar를 제공해 모델이 instruction을 더 잘 따르게 하였다. 대략적으로
task prompt는 Fig. <a data-reference="fig:task_prompts" data-reference-type="ref" href="#fig:task_prompts">1</a>처럼 instruction, exemplar,
context information, question식으로 주어진다. 이때 exemplar에서 이미지는
단순 텍스트 토큰으로 대체된다.</p>
<p>학습은 full fine-tuning으로 진행하였다.</p>
<h2 id="evaluation">Evaluation</h2>
<p>Evaluation은 generalist 모델의 성능 파악, multimodal에서 오는 성능
향상 파악, generated report의 평가를 목적으로 진행되었다. 수행한
evaluation은 크게 3가지로 나눌 수 있다.</p>
<ol>
<li><p><strong>Evaluation on MultiMedBench</strong>: MultiMedBench의
이전 SoTA모델들과의 성능을 비교하여 평가하였다.</p></li>
<li><p><strong>Evaluation of language enabled zero-shot
generalization</strong>: 모델의 zero-shot generalization 능력을
평가하기위해 unseen medical concept에 대한 평가를 진행하였다.</p></li>
<li><p><strong>Clinician evaluation of radiology report
generation</strong>: 모델이 생성한 report의 성능 평가를 위해 실제
radiologist들이 평가를 하였다. 여러 리포트를 주고 랭킹을 매기는 방식,
각각의 리포트에 rating을 매기는 방식, 총 두가지 방식으로
진행하였다.</p></li>
</ol>
<h2 id="results">Results</h2>
<h3 id="med-palm-m-performs-near-or-exceeding-sota-on-all-multimedbench-tasks">Med-PaLM
M performs near or exceeding SOTA on all MultiMedBench tasks</h3>
<p>Med-PaLM M은 다음 두가지 baseline과 비교하였다.</p>
<ol>
<li><p>각 MultiMedBench tasks에 대한 이전 SOTA 모델들</p></li>
<li><p>Biomedical domain에 대해 아무런 fine-tuning을 하지 않은 PaLM-E
84B</p></li>
</ol>
<p>주요 결과는 다음과 같다.</p>
<ol>
<li><p>Med-PaLM M의 best result(3개 scale중 가장 나은 결과)는 12개의
task 중 5개에서 SOTA를 넘어섰고, 그 외 task도 SOTA와 비슷한 성능을
보였다.</p></li>
<li><p>Baseline PaLM-E 모델과 비교했을때 큰 폭으로 성능이 향상된걸 볼 수
있었다.</p></li>
<li><p>Language reasoning task(Medical question answering 등)는 scale이
커짐에 따라 성능이 향상되었다.</p></li>
<li><p>Multimodal task에선 vision encoder의 성능에 맞춰 병목현상이
발생하였다.</p></li>
</ol>
<h3 id="med-palm-m-demonstrates-zero-shot-generalization-to-novel-medical-tasks-and-concepts">Med-PaLM
M demonstrates zero-shot generalization to novel medical tasks and
concepts</h3>
<p>Zero-shot generalization 능력을 보기 위해 우선 unseen medical
concept에 대한 평가를 진행하였다. Montgomery County dataset의 chest
X-ray 이미지를 보고 결핵인지 detect 하는 task로, Table <a data-reference="tab:results-tb-classification" data-reference-type="ref" href="#tab:results-tb-classification">2</a>에서
보듯이 기존의 SOTA<a href="#ref-oloko2021ensemble">[1]</a>와 비견되는
성능을 보여주었다.</p>
<figure id="tab:results-tb-classification">
<img src="figures/towards-generalist-biomedical-ai/_13a1__table2.png" style="width:100%;"/>
<figcaption><p>Table 2: <strong>Zero-shot classification performance of Med-PaLM M
on the tuberculosis (TB) detection task.</strong> Med-PaLM M performs
competitively to the SOTA model <a href="#ref-oloko2021ensemble">[1]</a> finetuned on
the Montgomery County TB dataset using model ensemble. Notably, Med-PaLM
M achieves this result with a simple task prompt consisting of a single
text-only exemplar (without task-specific image and hence zero-shot), in
contrast to the specialist model that requires training on all the
samples in the dataset. (Source: Table 4 in <a href="#ref-tu2023generalist">[2]</a>)</p></figcaption>
</figure>
<p>기타 generalization에 대한 평가로 multimodal medical reasoning 등에
대한 실험이 수행되었고, 모두 좋은 결과를 보여주었다.</p>
<h3 id="med-palm-m-performs-encouragingly-on-radiology-report-generation-across-model-scales">Med-PaLM
M performs encouragingly on radiology report generation across model
scales</h3>
<p>Radiologist evaluation 결과는 다음과 같다.</p>
<ol>
<li><p>Side-by-side comparison: 대체로 radiologist-provided reference
report가 가장 좋은결과를, 그다음이 84B scale이 좋은 결과를
보였다.</p></li>
<li><p>Independent evaluation: 대체로 84B scale이 가장 좋은 결과를
보였다.</p></li>
</ol>
<h2 id="quick-recap">Quick Recap</h2>
<ol>
<li><p>MultiMedBench: generalist biomedical AI를 위해 curated된
multimodal benchmark.</p></li>
<li><p>Med-PaLM M: PaLM-E를 기반으로 fine-tuning한 generalist biomedical
AI 모델.</p></li>
<li><p>Generalist AI는 benchmark가 부족해 만들기 힘든면이 있다.</p></li>
<li><p>biomedical data같이 domain-specific한 데이터에는 fine-tuning이
성능을 크게 좌우한다.</p></li>
<li><p>Multimodal AI 모델은 성능이 부족한 쪽에서 병목현상이 일어날 수
있다. 이는 다른 쪽의 성능을 끌어올려도 해결되지 않는다. 예를 들어,
vision쪽에서 병목이 생긴다면 language component만 향상시킨다고 해서
해결되지 않는다.</p></li>
</ol>
<div class="references csl-bib-body hanging-indent" id="refs" role="list" style="margin-bottom: 2rem"><h2 style="margin-top: 4rem">References</h2>
<div class="csl-entry" id="ref-oloko2021ensemble" role="listitem">[1] 
Oloko-Oba, Mustapha, Serestina Viriri, et al. 2021. <span>“<a href="https://www.hindawi.com/journals/cin/2021/9790894/">Ensemble of
EfficientNets for the Diagnosis of Tuberculosis</a>.”</span>
</div>
<div class="csl-entry" id="ref-tu2023generalist" role="listitem">[2] 
Tu, Tao, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin,
Pi-Chuan Chang, Andrew Carroll, et al. 2023. <span>“<a href="https://arxiv.org/abs/2307.14334">Towards Generalist Biomedical
AI</a>.”</span>
</div>
</div>
</body></html><!-- HTML_TAG_END --> </div> 
			
			<script>
				{
					__sveltekit_1t3kc7r = {
						base: new URL(".", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{content:"\u003Chtml>\u003Cbody>\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>의료 데이터는 이미지, 텍스트, 수치 등 다양한 형태로 존재하며, 이를\n통해 진단, 치료, 예방 등 다양한 목적을 달성하고자 한다. 즉, 의료분야는\n본질적으로 multimodal인 분야이다. 그러나, 지금까지의 biomedical AI는\n대부분 unimodal로 여러 의료데이터 정보의 통합에 한계가 있었다.\u003C/p>\n\u003Cp>\u003Ca href=\"#ref-tu2023generalist\">[2]\u003C/a>에선 이를\n해결하기 위해\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>Multimodal medical benchmark를 위한 새 벤치마크\n\u003Cstrong>MultiMedBench\u003C/strong>를 curate하고,\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Multimodal medical data의 여러 downstream task를 풀기 위한\n\u003Cem>generalist\u003C/em>(동일한 weight으로 여러 task를 풀 수 있음) biomedical\nAI 모델, \u003Cstrong>Med-PaLM M\u003C/strong>을 제시한다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"multimedbench-a-benchmark-for-generalist-biomedical-ai\">MultiMedBench:\nA Benchmark for Generalist Biomedical AI\u003C/h2>\n\u003Cfigure id=\"tab:multimedbench-overview\">\n\u003Cimg src=\"figures/towards-generalist-biomedical-ai/_13a1__table1.png\" style=\"width:100%;\"/>\n\u003Cfigcaption>\u003Cp>Table 1: \u003Cstrong>MultiMedBench overview.\u003C/strong> Summary of\nMultiMedBench, the benchmark we introduce for the development and\nevaluation of Med-PaLM M. MultiMedBench consists of 14 individual tasks\nacross 5 task types and 12 datasets spanning 7 biomedical data\nmodalities. In total, the benchmark contains over 1 million samples.\n(Source: Table 1 in \u003Ca href=\"#ref-tu2023generalist\">[2]\u003C/a>)\u003C/p>\u003C/figcaption>\n\u003C/figure>\n\u003Cp>MultiMedBench는 generalist biomedical AI를 위해 curated된 multi-task,\nmultimodal benchmark이다. 기존의 12개의 비식별화된 오픈소스 데이터셋을\n통합해 14개의 individual task를 가지고 있다. Output format은\nclassification을 포함한 모든 task에서 open-ended generation으로\n동일하다. 대략적인 구조는 Table \u003Ca data-reference=\"tab:multimedbench-overview\" data-reference-type=\"ref\" href=\"#tab:multimedbench-overview\">1\u003C/a>과\n같다.\u003C/p>\n\u003Ch2 id=\"med-palm-m-a-proof-of-concept-for-generalist-biomedical-ai\">Med-PaLM\nM: A Proof of Concept for Generalist Biomedical AI\u003C/h2>\n\u003Cfigure id=\"fig:task_prompts\">\n\u003Cimg src=\"figures/towards-generalist-biomedical-ai/task_prompts.jpg\" style=\"width:100.0%\"/>\n\u003Cfigcaption>Figure 1: \u003Cstrong>Illustration of instruction task prompting\nwith one-shot exemplar.\u003C/strong> (top) shows the task prompt for the\nchest X-ray report generation task. It consists of task-specific\ninstructions, a text-only “one-shot exemplar” (omitting the\ncorresponding image but preserving the target answer), and the actual\nquestion. The X-ray image is embedded and interleaved with textual\ncontext including view orientation and reason for the study in addition\nto the question. (bottom) shows the task prompt for the dermatology\nclassification task. The blue &lt;img&gt; denotes the position in the\nprompt where the image tokens are embedded. (Source: Fig. 2 in \u003Ca href=\"#ref-tu2023generalist\">[2]\u003C/a>)\u003C/figcaption>\n\u003C/figure>\n\u003Cp>Med-PaLM M은 PaLM-E를 기반으로 fine-tuning한 모델이다. \u003Ca href=\"#ref-tu2023generalist\">[2]\u003C/a>에선 scale에\n따라 총 3개의 PaLM-E(각각 PaLM-E 12B, PaLM-E 84B, PaLM-E 562B)를\n기반으로 실험을 진행하였다.\u003C/p>\n\u003Cp>학습은 Instruction tuning을 통해 multiple task를 가능하게 만들었고,\nexemplar를 제공해 모델이 instruction을 더 잘 따르게 하였다. 대략적으로\ntask prompt는 Fig. \u003Ca data-reference=\"fig:task_prompts\" data-reference-type=\"ref\" href=\"#fig:task_prompts\">1\u003C/a>처럼 instruction, exemplar,\ncontext information, question식으로 주어진다. 이때 exemplar에서 이미지는\n단순 텍스트 토큰으로 대체된다.\u003C/p>\n\u003Cp>학습은 full fine-tuning으로 진행하였다.\u003C/p>\n\u003Ch2 id=\"evaluation\">Evaluation\u003C/h2>\n\u003Cp>Evaluation은 generalist 모델의 성능 파악, multimodal에서 오는 성능\n향상 파악, generated report의 평가를 목적으로 진행되었다. 수행한\nevaluation은 크게 3가지로 나눌 수 있다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>\u003Cstrong>Evaluation on MultiMedBench\u003C/strong>: MultiMedBench의\n이전 SoTA모델들과의 성능을 비교하여 평가하였다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Evaluation of language enabled zero-shot\ngeneralization\u003C/strong>: 모델의 zero-shot generalization 능력을\n평가하기위해 unseen medical concept에 대한 평가를 진행하였다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>\u003Cstrong>Clinician evaluation of radiology report\ngeneration\u003C/strong>: 모델이 생성한 report의 성능 평가를 위해 실제\nradiologist들이 평가를 하였다. 여러 리포트를 주고 랭킹을 매기는 방식,\n각각의 리포트에 rating을 매기는 방식, 총 두가지 방식으로\n진행하였다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"results\">Results\u003C/h2>\n\u003Ch3 id=\"med-palm-m-performs-near-or-exceeding-sota-on-all-multimedbench-tasks\">Med-PaLM\nM performs near or exceeding SOTA on all MultiMedBench tasks\u003C/h3>\n\u003Cp>Med-PaLM M은 다음 두가지 baseline과 비교하였다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>각 MultiMedBench tasks에 대한 이전 SOTA 모델들\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Biomedical domain에 대해 아무런 fine-tuning을 하지 않은 PaLM-E\n84B\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>주요 결과는 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>Med-PaLM M의 best result(3개 scale중 가장 나은 결과)는 12개의\ntask 중 5개에서 SOTA를 넘어섰고, 그 외 task도 SOTA와 비슷한 성능을\n보였다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Baseline PaLM-E 모델과 비교했을때 큰 폭으로 성능이 향상된걸 볼 수\n있었다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Language reasoning task(Medical question answering 등)는 scale이\n커짐에 따라 성능이 향상되었다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Multimodal task에선 vision encoder의 성능에 맞춰 병목현상이\n발생하였다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"med-palm-m-demonstrates-zero-shot-generalization-to-novel-medical-tasks-and-concepts\">Med-PaLM\nM demonstrates zero-shot generalization to novel medical tasks and\nconcepts\u003C/h3>\n\u003Cp>Zero-shot generalization 능력을 보기 위해 우선 unseen medical\nconcept에 대한 평가를 진행하였다. Montgomery County dataset의 chest\nX-ray 이미지를 보고 결핵인지 detect 하는 task로, Table \u003Ca data-reference=\"tab:results-tb-classification\" data-reference-type=\"ref\" href=\"#tab:results-tb-classification\">2\u003C/a>에서\n보듯이 기존의 SOTA\u003Ca href=\"#ref-oloko2021ensemble\">[1]\u003C/a>와 비견되는\n성능을 보여주었다.\u003C/p>\n\u003Cfigure id=\"tab:results-tb-classification\">\n\u003Cimg src=\"figures/towards-generalist-biomedical-ai/_13a1__table2.png\" style=\"width:100%;\"/>\n\u003Cfigcaption>\u003Cp>Table 2: \u003Cstrong>Zero-shot classification performance of Med-PaLM M\non the tuberculosis (TB) detection task.\u003C/strong> Med-PaLM M performs\ncompetitively to the SOTA model \u003Ca href=\"#ref-oloko2021ensemble\">[1]\u003C/a> finetuned on\nthe Montgomery County TB dataset using model ensemble. Notably, Med-PaLM\nM achieves this result with a simple task prompt consisting of a single\ntext-only exemplar (without task-specific image and hence zero-shot), in\ncontrast to the specialist model that requires training on all the\nsamples in the dataset. (Source: Table 4 in \u003Ca href=\"#ref-tu2023generalist\">[2]\u003C/a>)\u003C/p>\u003C/figcaption>\n\u003C/figure>\n\u003Cp>기타 generalization에 대한 평가로 multimodal medical reasoning 등에\n대한 실험이 수행되었고, 모두 좋은 결과를 보여주었다.\u003C/p>\n\u003Ch3 id=\"med-palm-m-performs-encouragingly-on-radiology-report-generation-across-model-scales\">Med-PaLM\nM performs encouragingly on radiology report generation across model\nscales\u003C/h3>\n\u003Cp>Radiologist evaluation 결과는 다음과 같다.\u003C/p>\n\u003Col>\n\u003Cli>\u003Cp>Side-by-side comparison: 대체로 radiologist-provided reference\nreport가 가장 좋은결과를, 그다음이 84B scale이 좋은 결과를\n보였다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Independent evaluation: 대체로 84B scale이 가장 좋은 결과를\n보였다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"quick-recap\">Quick Recap\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cp>MultiMedBench: generalist biomedical AI를 위해 curated된\nmultimodal benchmark.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Med-PaLM M: PaLM-E를 기반으로 fine-tuning한 generalist biomedical\nAI 모델.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Generalist AI는 benchmark가 부족해 만들기 힘든면이 있다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>biomedical data같이 domain-specific한 데이터에는 fine-tuning이\n성능을 크게 좌우한다.\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Multimodal AI 모델은 성능이 부족한 쪽에서 병목현상이 일어날 수\n있다. 이는 다른 쪽의 성능을 끌어올려도 해결되지 않는다. 예를 들어,\nvision쪽에서 병목이 생긴다면 language component만 향상시킨다고 해서\n해결되지 않는다.\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"references csl-bib-body hanging-indent\" id=\"refs\" role=\"list\" style=\"margin-bottom: 2rem\">\u003Ch2 style=\"margin-top: 4rem\">References\u003C/h2>\n\u003Cdiv class=\"csl-entry\" id=\"ref-oloko2021ensemble\" role=\"listitem\">[1] \nOloko-Oba, Mustapha, Serestina Viriri, et al. 2021. \u003Cspan>“\u003Ca href=\"https://www.hindawi.com/journals/cin/2021/9790894/\">Ensemble of\nEfficientNets for the Diagnosis of Tuberculosis\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003Cdiv class=\"csl-entry\" id=\"ref-tu2023generalist\" role=\"listitem\">[2] \nTu, Tao, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin,\nPi-Chuan Chang, Andrew Carroll, et al. 2023. \u003Cspan>“\u003Ca href=\"https://arxiv.org/abs/2307.14334\">Towards Generalist Biomedical\nAI\u003C/a>.”\u003C/span>\n\u003C/div>\n\u003C/div>\n\u003C/body>\u003C/html>"},"uses":{}}];

					Promise.all([
						import("./_app/immutable/entry/start.f49ee20b.js"),
						import("./_app/immutable/entry/app.6f9a8e8a.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 10],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>